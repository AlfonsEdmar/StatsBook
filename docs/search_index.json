[["index.html", "Not yet decided 1 Table of contents", " Not yet decided B. Alfons Edmar 2022-05-06 1 Table of contents Introduction Foreword Some useful resources What I expect you to know Descriptive statistics Basic null hypothesis testing Statistical phenomena (regression to the mean, CLT, LOLN) A brief introduction to R programming (with links) Bayesian and Frequentist statistics Likelihood functions Density distributes All hail the almighty Gauss Non-parametric statistics Asking all the question, making statements, and assuming The old guard Bootstrapping Correlation and Covariation in R Welcome to the matrix Regression analysis Univariate regression Multiple regression The general linear model (ANOVA) The generalized linear model(logit) Cross validation LASSO (least absolute shrinkage and selection operator) Ridge regression Fully Bayesian regression Poststratification Path analysis Multiple regression(again) Mediation analysis Exploratory factor analysis (EFA) Confirmatory factor analysis (CFA) Multi-level modelling (MLM) Structural equation modelling (SEM) Meta-analysis "],["introduction.html", "2 Introduction 2.1 Foreword 2.2 Some useful resources", " 2 Introduction 2.1 Foreword I thought It would be fun to create a book covering the things that I have learnt this past year and a half while creating a (hopefully) useful tool for future students. I also really enjoy opensource projects such as R and really think that it is a brilliant workspace. It is not only flexible but creates a very structured environment where all your decisions throughout the analysis is easily documented and shared. In short, I like R and RStudio. The problem is that R is hard, the learning curve is slow, and if you are a psychologist, the opportunities to learn programming are sparse unless you seek them out for yourself. With this book I hope to create an accessible guide to how you can use R for your studies and hopefully your future work. Now is a good time to point out that I am not a statistician, as of writing this I have not even graduated from the master´s program. Thus, this book should not be treated as an authoritative voice on how you should do statistics or interpret your results. The aim here is simple, it is to guide you through some of the statistical analyses that you will encounter and introduce the R programming language in the meantime. There will of course be some explanations of the tools that we use in terms of their statistical properties and interpretations of the results from our analyses, but I cannot stress enough that this is not a book about statistics, it´s a book about doing statistics in R. It´s designed to be brief and easily skimmed though in the sense that you wont have to read so much of my blathering to understand the interesting bits - the code. The general layout of the applied chapters will be analyses of the working examples in Discovering statistics by Andy Field (when applicable) and some workshop exercises. I will also end each applied chapter by finding some real data from an article and working through their analysis. In sum, this is a compilation of the topics covered in the master´s programme in psychological science at the university of Gothenburg in 2021/22 with some additional chapters by yours truly on some of the things that I think are useful and important. It is a supplementary text that simply translates the analyses described in the literature into R-code. It is for students who want to learn R but dont know where to start. It can be hard to learn things on your own without any real incentives, and I hope that this book can coax some people into R through it´s accessibility and relevance to what you will be doing in class anyways. Good luck and happy reading/coding. P.s, If you are a statistician, stop reading here, only pain will follow. 2.2 Some useful resources "],["welcome-to-r.html", "3 Welcome to R 3.1 The R environment 3.2 Basic programming 3.3 Organisation 3.4 Rmarkdowm", " 3 Welcome to R 3.1 The R environment 3.2 Basic programming 3.3 Organisation 3.4 Rmarkdowm "],["what-i-expect-you-to-know.html", "4 What I expect you to know 4.1 Descriptive statistics 4.2 Basic null hypothesis testing 4.3 Statistical phenomena", " 4 What I expect you to know 4.1 Descriptive statistics 4.2 Basic null hypothesis testing 4.3 Statistical phenomena "],["useful-packages-and-functions-that-helped-me-learn.html", "5 Useful packages and functions that helped me learn 5.1 Tidyverse 5.2 Distribution functions - rnorm, dbeta, rbinom etc 5.3 Faux 5.4 lm 5.5 Lavaan 5.6 semPlot 5.7 ?", " 5 Useful packages and functions that helped me learn This is a bit of a strange chapter, I am not entirely sure why I included it, but I think it might be a good idea to have a few words on the packages that we will be using. This Chapter could be returned two If you get stuck with some code and for some really strange reason can´t find a better place to search for answers than here. In sum, there are some useful packages and function in R, and using them might be beneficial for both learning and doing you statistics. 5.1 Tidyverse Like the name implies, tidyverse is an entire universe of connected packages designed to make your coding comfortable and aesthetically pleasing. I neither have the expertise nor the time to go through even the basics of all the tidyverse packages but we will be using them frequently, and when it happens there will be some notation. In the meantime, the first chapter of R for data science is an absolute gem if you are serious about learning R. Very nice introduction, assuming no previous coding experience (and it´s free) I cannot recommend it enough (hence the repeat) https://r4ds.had.co.nz/ 5.2 Distribution functions - rnorm, dbeta, rbinom etc These functions were probably what got me to actually think I understood some things about statistics, which is quite impressive for me. They are essentially functions that describe the shape of certain distributions and gives you the ability to generate random variables from those distributions. The possibilities of this is really endless but I will show some examples of how to generate data below. #Since the data is &quot;random&quot; it will be impossible to reproduce if you re-run the code. This is why we usually set a seed. This makes it so the random data can be reproduced. We do this through the set.seed function before we generate random numbers. Fun fact, the numbers are not truly random since they can be reproduced exactly, they are pseudo-random. #Random data following the normal distribution set.seed(5395) #setting the seed, this can by any combination of integers. norm_data &lt;- rnorm( n = 500 #the number of observations we want , mean = 4 #the mean of the numbers , sd = 1 #the standard deviation of the data ) #Prof of normality through histogram hist(norm_data) #Voila. #We can do the same with the t-distribution set.seed(453) t_data &lt;- rt( n = 50 #number of observations , df= 6 #degrees of freedom ) hist(t_data) #the dame things can be done to the F distribution using rf() #If we want binary outcomes we can use the binomial distribution with r(binom) #lets say we want to simulate 10 tosses of a fair coin for example. set.seed(5325) coin_data &lt;- rbinom( n = 10 #number of observations/tosses , size = 1 #number of trials per observation(one this time) , prob = .5 #the probability of a successful outcome (50/50) ) hist(coin_data) #Cool, If we say 1 is tails we have 6 tails and 4 heads. These might seem like very trivial things, but by manipulating the data and doing analyses on data sets where you actually KNOW what the true population parameters are, can be very useful and enlightening. Simulation is the best kind of preparation; it also highlights very well what you expect from the data. The next package I can recommend handles data generation quite smoothly. 5.3 Faux One of the most powerful properties of programming languages is their ability to generate pseudo random variables. We have seen that we can easily generate independent random numbers following some distribution, but what if we want to simulate relationships between variables? enter faux. There are probably many other packages doing the same things, but this is the one I use, and it has served me quite well.  let´s generate the data that we will work with through this chapter. library(&#39;faux&#39;) set.seed(324) #the seed returns! data &lt;- rnorm_multi(n = 500 #the number of observations we want , vars = 3 #number of variables we want , mu = c(3,5,2)#the three means for our variables. , sd = c(1.3, 2, .6) #the three SDs , r = c(1, .4, .7, #this is where it gets tricky. r is .4, 1, .4, #the correlation matrix. If we have .7, .4, 1) #many variables it can be tedious , varnames = c(&#39;y&#39;, &#39;x&#39;, &#39;z&#39;)) #the names of our variables #lets look at the correlations and the mean of our data cor(data) #correlates all variables in the data set with each other ## y x z ## y 1.0000000 0.3674651 0.6744010 ## x 0.3674651 1.0000000 0.4105545 ## z 0.6744010 0.4105545 1.0000000 summary(data) #summarises the content in the data ## y x z ## Min. :-0.3747 Min. :-0.8381 Min. :0.2554 ## 1st Qu.: 1.9188 1st Qu.: 3.6476 1st Qu.:1.5958 ## Median : 2.9566 Median : 4.9841 Median :2.0288 ## Mean : 2.9204 Mean : 5.0046 Mean :2.0239 ## 3rd Qu.: 3.7996 3rd Qu.: 6.3985 3rd Qu.:2.4164 ## Max. : 6.5359 Max. :12.3798 Max. :3.8473 #Notice how the values are similar, but not exactly the same, why is that? it is because of randomness. We can remedy this if you want to have full control over the parameter estimates in your data. We do this adding the empirical = true to the data generation formula like so: data.2 &lt;- rnorm_multi(n = 500 , vars = 3 , mu = c(3,5,2) , sd = c(1.3, 2, .6) , r = c(1, .4, .7, .4, 1, .4, .7, .4, 1) , varnames = c(&#39;y&#39;, &#39;x&#39;, &#39;z&#39;) , empirical = TRUE) #do we want empirical estimates? #This is now empirical data and not data sample from a population cor(data.2) ## y x z ## y 1.0 0.4 0.7 ## x 0.4 1.0 0.4 ## z 0.7 0.4 1.0 summary(data.2) ## y x z ## Min. :-0.8063 Min. :-0.8758 Min. :0.293 ## 1st Qu.: 2.1405 1st Qu.: 3.7060 1st Qu.:1.590 ## Median : 2.9691 Median : 4.9931 Median :2.015 ## Mean : 3.0000 Mean : 5.0000 Mean :2.000 ## 3rd Qu.: 3.9539 3rd Qu.: 6.2568 3rd Qu.:2.398 ## Max. : 6.5047 Max. :11.9103 Max. :3.605 #And there it is, identical to how we specified it in the rnorm_multi function. Now that we have some data, let´s try to fit it to a model 5.4 lm As we will a little bit more in-depth below, R generally uses strings from specifying models. That is, if we want to regress Y on x we specify that as Y~X. You will be doing a lot of that little squiggly(~), or, as some call it tilde(it´s proper name). Most if not all simple statistics can be done using the linear model function lm(). There are many functions for various types of t-test and chi2 and others but you will get really far with just the lm(), at the end of the day most tests all fall under the general linear model. Let´s fit a linear model with the data we just generated, we can predict y with x and z. fit.lm &lt;- lm( formula = y ~ x + z #the formula of the regression , data = data #the data we want to use ) #When we have fitted a model, we need to summarise the output. There is on specifically for linear models that we can use. summary.lm(object = fit.lm) ## ## Call: ## lm(formula = y ~ x + z, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7678 -0.5999 -0.0311 0.6150 2.9941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.25511 0.16116 -1.583 0.1141 ## x 0.07015 0.02318 3.027 0.0026 ** ## z 1.39556 0.07977 17.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9565 on 497 degrees of freedom ## Multiple R-squared: 0.4647, Adjusted R-squared: 0.4625 ## F-statistic: 215.7 on 2 and 497 DF, p-value: &lt; 2.2e-16 Cool, this is a run of the mill multiple regression. But remember that you cannot only fit our model, you must also summarise it, be that through summary.lm() or by any other means. We´ll get more in to lm() in the regression chapters. Let´s move on to a real big boy, lavaan. 5.5 Lavaan Lavaan, or, latent variable analysis, is an R package that we will use quite a bit. As the name implies it´s main function is to do latent variable analysis. If you do not know what latent variable analysis is, don´t worry, you soon will. The main thing that can be a bit tricky with lavaan is that it uses different kind of model specification than we usually do in simple analyses such as linear models. Contrary to those models, lavaan works in three steps. - first: you specify your model - second: you fit your model - third: you summarise the output of the model fit. If we compare this to the previous two step process of fitting an lm, it is the separation of model specification and model fitting that can take you for a loop. Lavaan models are specified as strings, that is, characters. These characters are then applied to a fitting function. To make things clearer, let´s play around with it a bit. We preciously specified the model of y ~ x + z. This is a character string that communicates that y is regressed on x + z. Lavaan works the same way, so if we wanted to fit a regression in lavaan we can use that exact string, the difference is that we specify the modal as a separate object from the fit. library(lavaan) #Step 1: specify the model model &lt;- (&#39;y ~ x + z&#39;) #Step 2: fit the specifid model with the sem function fit &lt;- sem(model = model, data = data) #Step 3: summarise the output summary(fit) ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 0.070 0.023 3.036 0.002 ## z 1.396 0.080 17.547 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.909 0.058 15.811 0.000 Now, if we look at the regression section in the bottom, we can see that the regression output is exactly the same as we would get if we did a normal lm regression. Let´s print out that regression we fitted earlier. summary.lm(fit.lm) ## ## Call: ## lm(formula = y ~ x + z, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7678 -0.5999 -0.0311 0.6150 2.9941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.25511 0.16116 -1.583 0.1141 ## x 0.07015 0.02318 3.027 0.0026 ** ## z 1.39556 0.07977 17.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9565 on 497 degrees of freedom ## Multiple R-squared: 0.4647, Adjusted R-squared: 0.4625 ## F-statistic: 215.7 on 2 and 497 DF, p-value: &lt; 2.2e-16 And there you go, it´s very much the same. Now, why would you want to do all this other stuff just to fit a regression? well, you wouldnt. But this is not the point of lavaan, lavaan can specify very complex models ranging from latent variable models to multilevel models, and all this is done though the mode character string, also knowns as the lavaan syntax. Below I will give some examples of what type of models you can fit with lavaan. I will plot them out using a package called semPlot - but we will get to that in due time. library(semPlot) #A simple correlation model cor_model &lt;- (&#39; #Correlations are described by &quot;~~&quot; y ~~ x&#39; ) semPaths(sem(cor_model)) #A simple regression model reg_model &lt;- (&#39; #Regressions are described by &quot;~&quot; y ~ x&#39;) semPaths(sem(reg_model), rotation = 2) #A simple mediation model med_model &lt;- (&#39; y ~ x + z z ~ x&#39; ) semPaths(sem(med_model)) #a simple latent variable model sem_model &lt;- (&#39; #Latent variable as describe by &quot;=~&quot; y =~ a + b + c + d x =~ c + d + e + f &#39;) semPaths(sem(sem_model)) This is just a little taste of what lavaan can do. If your data can handle it, you can make incredibly interesting models. We will use lavaan for path analysis, mediation analysis, SEM and multi-level SEM. If you want to go Bayesian, there is even a Bayesian alternative under the name blavaan. Again, this is barely scratching the surface, and if you want to get deeper insight into the workings of lavaan i can recommend going straight to the source: https://lavaan.ugent.be/tutorial/index.html 5.6 semPlot The semPlot package is used to visual/plot SEMs, CFAs and path analyses. We have already used semPlot to graph the sem models above. Now, graphing sems can be quite annoying in the beginning. The documentation is very very extensive, but not super easy to understand (IMO) and before you have a basic understanding of the semPaths function reading the documentation might feel overwhelming. I would recommend going to this very concise video by the author of the package for a nice little overview: https://www.youtube.com/watch?v=rUUzF1_yaXg&amp;ab_channel=SachaEpskamp 5.7 ? Last, but certainly not least, the question mark. This might be the most useful little trick R has to offer. All packages/functions have built in documentation that explains it´s uses and intricacies. Most even have examples of how to use the function. I cannot stress enough how useful this is, not only is it good for solving problems in your code quickly, but all this documentation is a wealth of knowledge. Though brief, most if not all documentation has useful references and information that can broaden your knowledge of what you are actually doing. So how does it work? you simply type ? before a function. Let´s say we want to know more about how to fit lavaan models with the sem function, then we simple write: ?sem #Maybe we have issues generating correct correlations and have to revisit how to use rnorm_multi ?rnorm_multi #Perhaps just the rnorm? ?rnorm Make use of this tool! it can help you a lot. Also, I don´t like leaving a mess. So now that we are done, we should clean up. The fastest way of removing things in R is with the rm() function. This removes singular things from the environment. If we want to remove the coin_data we can simply write: rm(coin_data) And it´s gone. Now you will be glad that you have all your things documented neatly, that way you can load and remove things at the press of a button without having to worry about losing things. But if we want to clean everything out (as I often want) we can write the line: rm(list = ls()) To be completely honest I not know exactly what this line means, but it does a great job cleaning. "],["bayesian-and-frequentists-statistics.html", "6 Bayesian and Frequentists statistics 6.1 Likelihood functions 6.2 Density distributes 6.3 All hail the almighty Gauss", " 6 Bayesian and Frequentists statistics 6.1 Likelihood functions 6.2 Density distributes 6.3 All hail the almighty Gauss "],["non-parametric-statistics.html", "7 Non-parametric statistics 7.1 Asking all the question, making statements, and assuming 7.2 The old guard 7.3 Bootstrapping", " 7 Non-parametric statistics 7.1 Asking all the question, making statements, and assuming 7.2 The old guard 7.3 Bootstrapping "],["correlation-and-covariation-in-r.html", "8 Correlation and Covariation in R 8.1 Welcome to the matrix", " 8 Correlation and Covariation in R 8.1 Welcome to the matrix "],["regression-analysis.html", "9 Regression analysis 9.1 Univariate regression 9.2 Multiple regression 9.3 The general linear model (ANOVA) 9.4 The generalized linear model(logit) 9.5 Cross validation 9.6 LASSO (least absolute shrinkage and selection operator) 9.7 Ridge regression 9.8 Fully Bayesian regression 9.9 Poststratification", " 9 Regression analysis 9.1 Univariate regression 9.2 Multiple regression 9.3 The general linear model (ANOVA) 9.4 The generalized linear model(logit) 9.5 Cross validation 9.6 LASSO (least absolute shrinkage and selection operator) 9.7 Ridge regression 9.8 Fully Bayesian regression 9.9 Poststratification "],["path-analysis.html", "10 Path analysis 10.1 Streiner(2005) 10.2 workshop analysis 10.3 Multiple regression(again) 10.4 Mediation analysis", " 10 Path analysis 10.1 Streiner(2005) Path analysis is not really a type of analysis like OLS regression or logistic regression but rather a mode for communicating and fitting more complex models. The path in path analysis refers to the specification of relationships between variables. What the path describe can vary. For example, a regression analysis where Y is regressed on X we have a path between X and Y that describes a causal effect of X on Y. Note that it is us as researchers that make the call that the relationship is causal, just because X and Y correlate does not mean that either variable causes the other. If you recall the lavaan notations from way back in chapter 4 you should recognise the difference between fitting models in a path context versus a regression context. To monkey what all the smart people say, OLS/ML regression is only a special case of a path analysis/SEM. Hopefully the distinction between path analysis and regression will become a bit fuzzy by the end of this chapter, they are very much the same. One might intuit them as slightly different tools for very similar jobs. In my opinion, the strength of path analysis lies in the visualisation opportunities combined with the ability to fit complex models, restrict those models and lastly compare the fit of those models. The little less cool thing with complex path analysis is that it is, wellcomplex. Another weakness of path analysis compared to normal regression is that it is very easy to fit nested regression models and there is a very natural progression of model building in the multiple regression context - as we saw in the previous chapter. Before getting in to live datasets, we can simulate some data. For this lecture I/we(class of 2021) were assigned an article by Streiner, it is a pretty nice article IMO so even if you havent read it I can recommend it. In the article Streiner analyses data from an unknown disorder called photonumerophobia, he describes it as the fear, that our fears of numbers will come to light. He then defines three predictors for this disorder, namely: HSM &lt;- high school math grade ANX &lt;- overall anxiety TAX &lt;- the difference between predicted tax and actual tax (weird I know, but lets run with it). Note that Streiner does not supply us with any data, he does however give as ample descriptives, enough to simulate the data quite closely, so let´s do that. All numbers are taken from table 1 in Streiner (2005): https://journals.sagepub.com/doi/pdf/10.1177/070674370505000207 #This is a perfect time for some faux. A nice little function is the cormat_from_triangle calls. It is quite simple, if you write in the top right triangle of correlations the function fills out the missing bits in the matrix. Now you dont have to write so much - thanks computer. library(faux) cors &lt;- cormat_from_triangle(c(.509 , -.366 ,.346, -.264 ,.338, .260)) #Now that we have the relationship between the data we can define the rest of the variables with the other descriptives given. set.seed(4543) #setting seed is always good data &lt;- rnorm_multi( n = 200 #let&#39;s take 200 observations , vars = 4 , mu = c(26.79, 20.33, 74.69, 1983.23) , sd = c(7.33 , 5.17 , 5.37 , 525.49) , r = cors #this is our previously defined cor matrix , varnames = c(&#39;pnp&#39;, &#39;anx&#39;, &#39;hsm&#39;, &#39;tax&#39;) , empirical = T)#we want to perfectly reproduce the data #Let´s look at the correlation matrix for the data we just generated cor(data) ## pnp anx hsm tax ## pnp 1.000 0.509 -0.366 0.346 ## anx 0.509 1.000 -0.264 0.338 ## hsm -0.366 -0.264 1.000 0.260 ## tax 0.346 0.338 0.260 1.000 #It´s a thing of beauty is it not? What about the descriptives? pastecs::stat.desc(data, desc = T ,basic = F) ## pnp anx hsm tax ## median 26.4386929 20.0561011 74.67091194 1.954408e+03 ## mean 26.7900000 20.3300000 74.69000000 1.983230e+03 ## SE.mean 0.5183093 0.3655742 0.37971634 3.715775e+01 ## CI.mean.0.95 1.0220834 0.7208964 0.74878412 7.327348e+01 ## var 53.7289000 26.7289000 28.83690000 2.761397e+05 ## std.dev 7.3300000 5.1700000 5.37000000 5.254900e+02 ## coef.var 0.2736096 0.2543040 0.07189717 2.649667e-01 #Lovely. Now we have data that very much resembles that from the article we can fit the path models he describes. Let´s start with table 4. pnp is regressed on all variables and the predictors are allowed to correlate. We will use the package lavaan for this so if you need a refresher go back to chapter 4 and check out the links provided there. #Loading lavaan and semPlot library(lavaan) library(semPlot) #Specifying our model with the lavaan syntax model.1 &lt;- (&#39; #regressions pnp ~ tax + hsm + anx #correlations anx ~~ hsm anx ~~ tax tax ~~ hsm &#39;) #Fitting our model using the sem function fit.1 &lt;- sem(model = model.1, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #The warning here is due to the fact that the tax variable is much larger than the others. Let´s plot the model. semPaths( object = fit.1 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , curvePivot = TRUE , edge.label.cex = 1.5 , rotation = 2 , sizeMan = 10) #And there it is, pretty as a picture. Now, Streiner actually specifies two other models, those seen in figure 5. Lets create and plot those two. After that we can check how they fit. #lets start with model a model.a &lt;- (&#39; #regressions pnp ~ tax tax ~ hsm hsm ~ anx &#39;) #Fitting the model fit.a &lt;- sem(model = model.a, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #Ploting the model semPaths( object = fit.a , what = &#39;est, std&#39; , layout = &#39;spring&#39; , style = &#39;lisrel&#39; , curvePivot = T , edge.label.cex = 1.5 , rotation = 1 , residuals = T , sizeMan = 10) #The control you have over the layout of these plots are vast, but the require tinkering that is not often worth it. This looks good enough. Let´s do the next model. model.b &lt;- (&#39; #regressions pnp ~ tax + hsm tax ~ anx hsm ~ anx &#39;) #Fitting the model fit.b &lt;- sem(model = model.b, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #Ploting the model semPaths( object = fit.b , what = &#39;est, std&#39; , layout = &#39;tree2&#39; , style = &#39;lisrel&#39; , curvePivot = T , edge.label.cex = 1.5 , rotation = 2 , residuals = T , sizeMan = 10) Cool, this is the exact models he fits. Now, are these models any good? we can explore that question by summarising the fits (fit.1, fit.a and fit.b), but first, how many parameters can we estimate? This is one of the trickier parts of path analysis and SEM. We cannot simply think about the number of variables we have; we must think about our data in terms of a variance/co-variance matrix. For each variable we have one variance, in this case 4, each variable pair has a covariation, thus we have ([k^2  k] / 2) covariances where k is the number of variables we have. Let´s calculate how many pieces of information we have k &lt;- 4 co &lt;- ((k^2-k)/2) #We have 4 variances and 6 covariances, this totals out at 10. This means that we can make a total 10 parameter estimates before we exhaust our degrees of freedom. Read the article more closely for a better understanding of degrees of freedom, they are fascinating. So, how many parameters are we estimating? in the first model(fit.1) we are actually estimating 10 parameters (three regressions, tree covariations, and 4 variances). This means that we have 0 degrees of freedom and a perfect fit. In second model(fit.a) we estimate 6 parameters(three regressions and tree variances) and in the last model(fit.b) we estimate 7 parameters(4 regressions and 3 variances). This can be tricky to wrap your mind around in the beginning but once you play around with it a bit it becomes more straight forward. Since we have two overidentified models we can compare the fit of them(the identified model fits perfectly), we can do this with the anova function. anova(fit.a, fit.b) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.b 2 5565.9 5589.0 52.613 ## fit.a 3 5632.6 5652.4 121.295 68.681 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #note that this if not an F test but a chi-squared test(LRT) The significance of the test indicates that model.b fits significantly better than model.a. However, the information difference in terms of AIC and BIC is very small. But this is an issue that doesnt really apply itself well to entirely imaginary data. We don´t really have a good frame of reference in terms of theory. 10.2 workshop analysis Now that we have gotten familiar with path analysis, we can try out another example. The data is still very much simulated, but this time we have to deal with some actual data points, not just generation by ourselves. The following analysis is based of the workshop on path analysis at GU. Hopefully i can take the data from the workshop without any issues. Let´s load the data (it should be available on the repository, hopefully.) library(haven) data &lt;- read_sav(&quot;data path analysis spring 22.sav&quot;) #Note. this is my path to the data and to access the file you need to import it yourself These are the main packages we will be using. We have the usual suspects + MVN for analysis of multivariate normality. library(tidyverse) ## -- Attaching packages ------------------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## -- Conflicts ---------------------------------------------------- tidyverse_conflicts() -- ## x purrr::%||%() masks faux::%||%() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(lavaan) library(semPlot) library(MVN) Let´s take a peek at what we are working with. summary(data) ## stress satisfaction turnover_intent demands support ## Min. :0.600 Min. :0.000 Min. :0.000 Min. :1.000 Min. :1.000 ## 1st Qu.:1.500 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:2.000 1st Qu.:2.333 ## Median :2.000 Median :2.000 Median :1.500 Median :2.000 Median :2.667 ## Mean :1.993 Mean :2.578 Mean :1.896 Mean :2.363 Mean :2.732 ## 3rd Qu.:2.333 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:3.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 All variables are numeric and seems to range between 0 and 5. Note that some variables are ordinal, that is, they are whole numbers that are not continuous, that is, the distance between 1 and 2 cannot be assumed to be the same as between 3 and 4. If we want to assume this, the data needs to normally distributed. So let´s take a look at the distributions. Note however that this is NOT the normality assumption of the linear regression but rather an assumption of using ordinal data as if they were continuous. ggplot(data, aes(stress))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #It has a normal look, and it seems to be continuous measure, this is good. ggplot(data, aes(satisfaction))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Again, quite normal looking - this is ordinal but could be treated as if continuous with some mental gymnastics ggplot(data, aes(turnover_intent))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Interesting, perhaps its measured in half steps? but that begs the question why none of the higher valus such as 4.5 exists, it seems to stop after 2. Strange. ggplot(data, aes(demands))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Like satisfaction, looks pretty good. ggplot(data, aes(support))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Now this is a strange looking distribution...it looks to be a continuous measure and the density of the curve should be quite normal. #However, we need to check the normality of our predicted variables when we have them. Before that we can check the normality tests. One common measure is mardia, so let´s use that. We can also look at some of the qq-plots and the outlier measures. Let´s save the scores in a list called normality_diagnostics normality_diagnostics &lt;- mvn(data , mvnTest = &#39;mardia&#39; #takes maridas test , multivariatePlot = &#39;qq&#39; #normal chi2 qq-plot , univariateTest = &#39;AD&#39; #anderson-darling test for univariate normality , showOutliers = TRUE , showNewData = TRUE #stores a tibble without the outliers , multivariateOutlierMethod = &#39;adj&#39;) #takes the adjusted mahalanobis distance #Checking the normality descriptives normality_diagnostics$Descriptives ## n Mean Std.Dev Median Min Max 25th 75th Skew ## stress 322 1.992847 0.7447653 2.000000 0.6 5 1.500000 2.3325 1.5307216 ## satisfaction 322 2.577640 1.0715218 2.000000 0.0 5 2.000000 3.0000 0.6089998 ## turnover_intent 322 1.895963 1.2288900 1.500000 0.0 5 1.000000 3.0000 0.7192168 ## demands 322 2.363354 0.9145545 2.000000 1.0 5 2.000000 3.0000 0.4177021 ## support 322 2.731884 0.7106893 2.666667 1.0 5 2.333333 3.0000 0.2551844 ## Kurtosis ## stress 4.593582827 ## satisfaction 0.130539733 ## turnover_intent -0.169774465 ## demands -0.002784417 ## support 0.298869065 #Checking univariate normality normality_diagnostics$univariateNormality ## Test Variable Statistic p value Normality ## 1 Anderson-Darling stress 8.7723 &lt;0.001 NO ## 2 Anderson-Darling satisfaction 17.5045 &lt;0.001 NO ## 3 Anderson-Darling turnover_intent 9.3350 &lt;0.001 NO ## 4 Anderson-Darling demands 16.5444 &lt;0.001 NO ## 5 Anderson-Darling support 3.2055 &lt;0.001 NO #Checking multivariate normality normality_diagnostics$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 460.776024749078 1.05636187815355e-75 NO ## 2 Mardia Kurtosis 8.96312473441598 0 NO ## 3 MVN &lt;NA&gt; &lt;NA&gt; NO So, nothing is normal and we have quite a few outliers according to Mahalanobis. If we want an outlier free data set the mvn function saves it for us. Let´s try to extract the clean data from the normality diagnostics. clean_data &lt;- normality_diagnostics$newData Here we have the data without the 27 outliers indicated by the normality diagnostics. I´m not a fan of this type of diagnostics, but we can use the raw data as a type of sensitivity analysis later on, for now lets move on using the clean data. Since we don´t really know what we are doing it´s always good to look at the correlations to inform us about what connections lie within our sample. So let´s do that and then try to find a good model for our data. #This gives a normal correlation matrix cor(clean_data) ## stress satisfaction turnover_intent demands support ## stress 1.00000000 -0.4892677 0.22641261 -0.04095583 -0.3617246 ## satisfaction -0.48926773 1.0000000 -0.59110506 0.12206989 0.3145087 ## turnover_intent 0.22641261 -0.5911051 1.00000000 0.03633829 -0.2111387 ## demands -0.04095583 0.1220699 0.03633829 1.00000000 0.1464965 ## support -0.36172462 0.3145087 -0.21113872 0.14649650 1.0000000 #we can however plot the data out using the corplot function from the psych package without loading it by using the :: command like so: psych::corPlot(cor(clean_data)) #Note. the omitted variable in the bottom is turnover_intent Stress, support and satisfaction seems to be correlated just fine. Demands seem to be a quite superfluous variable having low correlations to all other variables. So let´s build a model where stress and support are allowed to correlate and that both have a causal impact on the satisfaction you have in the workplace. Let´s also say that support and satisfaction have a causal impact on your turnover intent. Let´s specify this model using the lavaan syntax. model.1 &lt;- (&#39; #regressions satisfaction ~ stress + support turnover_intent ~ satisfaction turnover_intent ~ support #covariates stress ~~ support &#39;) Now that we have our model - specified in text - we can fit that model using lavaans sem function just like we did before. fit.1 &lt;- sem( model = model.1 #the model we want to fit , data = clean_data #the data we want to fit the model on , estimator = &#39;MLM&#39; #the estimator we use (robust maximum likelihood) ) Now that we have our fit, we can summarise the results summary(fit.1 #the fit we want to summarize , standardize = TRUE #we want the standardised estimates , fit.measures = TRUE #we also want the fit measures(CFI, TLI) , rsquare = TRUE #if we want to r2 for the model ) ## lavaan 0.6-9 ended normally after 18 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 295 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 3.014 2.867 ## Degrees of freedom 1 1 ## P-value (Chi-square) 0.083 0.090 ## Scaling correction factor 1.051 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 260.797 208.698 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.250 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.992 0.991 ## Tucker-Lewis Index (TLI) 0.953 0.945 ## ## Robust Comparative Fit Index (CFI) 0.992 ## Robust Tucker-Lewis Index (TLI) 0.954 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1420.264 -1420.264 ## Loglikelihood unrestricted model (H1) -1418.757 -1418.757 ## ## Akaike (AIC) 2858.528 2858.528 ## Bayesian (BIC) 2891.711 2891.711 ## Sample-size adjusted Bayesian (BIC) 2863.169 2863.169 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.083 0.080 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.197 0.191 ## P-value RMSEA &lt;= 0.05 0.195 0.207 ## ## Robust RMSEA 0.082 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.199 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.022 0.022 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## stress -0.633 0.107 -5.902 0.000 -0.633 -0.432 ## support 0.252 0.087 2.909 0.004 0.252 0.158 ## turnover_intent ~ ## satisfaction -0.672 0.051 -13.158 0.000 -0.672 -0.582 ## support -0.051 0.084 -0.610 0.542 -0.051 -0.028 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~~ ## support -0.178 0.044 -4.093 0.000 -0.178 -0.362 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .satisfaction 0.850 0.066 12.878 0.000 0.850 0.739 ## .turnover_intnt 0.994 0.074 13.440 0.000 0.994 0.650 ## stress 0.536 0.081 6.649 0.000 0.536 1.000 ## support 0.454 0.043 10.580 0.000 0.454 1.000 ## ## R-Square: ## Estimate ## satisfaction 0.261 ## turnover_intnt 0.350 So our model fits remarkably well. Not even the chi2 is significant. Also, not how much better our model is than the baseline model. One bad thing is the fact that we are not very courageous, we only have one degree of freedom. Let´s plot this model out using semPlot and then try improving our fit even more. #This is a more detailed notation of the semPlot alternatives semPaths( object = fit.1 #the fit we want to plot(our model) , what = &#39;est, std&#39; #what we want to plot(the z-coefficients) , style = &#39;lisrel&#39; #the style of the plot , curvePivot = TRUE #manipulates how the lines look , esize = 4 #the size of our lines , nCharNodes = 0 #takes all the characters in our manifest vars , edge.label.cex = 1.5#how big the estimates are(indicated in &quot;what =&quot;) , residuals = FALSE #removes the residual loops , sizeMan = 15 #indicates the width of manifest variables , rotation = 2 #how we want to rotate the plot(wich direction) , theme = &#39;Borkulo&#39;) #the theme we want to use, i like this one #Looks pretty good right? but notice the very weak trace between support and turnover intent and satisfaction! Let´s interpret this. We can see that the correlation between support and stress is -.36 (as we saw in the correlation plot). We can also see that for every unit increase in stress satisfaction decreased with .43 standard deviations. We also see that for every unit increase in satisfaction we see a decrease in turnover_intent at .58 standard deviations. All these estimates, together with the unstandardized counterparts are available in the summary above. In the summary we also see an r2 of .26 in satisfaction and .35 in turnover intent, meaning that we can explain roughly 26% of the variation in satisfaction through stress and support and .35% of the variation in turnover intent through support and satisfaction. Pretty neat right? So where do we go from here? we have a model and its fits the data very well. We want to be brave scientists though, so we should refine our model and make some bolder claims. For example, let´s make the claim that support does NOT influence turnover intent, support does not even influence satisfaction. No, support actually has a one directional relationship with stress, that is the degree of support we have impacts our degree of stress, but stress does not influence how much support we have. Let´s specify this model and fit it. model.2 &lt;- (&#39; #regressions stress ~ support satisfaction ~ stress turnover_intent ~ satisfaction &#39;) fit.2 &lt;- sem( model = model.2 , data = clean_data , estimator = &#39;MLM&#39;) summary(fit.2 , standardize = TRUE , fit.measures = TRUE , rsquare = TRUE ) ## lavaan 0.6-9 ended normally after 14 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 295 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 11.898 12.009 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.008 0.007 ## Scaling correction factor 0.991 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 260.797 241.478 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.080 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.965 0.962 ## Tucker-Lewis Index (TLI) 0.930 0.923 ## ## Robust Comparative Fit Index (CFI) 0.965 ## Robust Tucker-Lewis Index (TLI) 0.930 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1122.743 -1122.743 ## Loglikelihood unrestricted model (H1) -1116.794 -1116.794 ## ## Akaike (AIC) 2257.486 2257.486 ## Bayesian (BIC) 2279.608 2279.608 ## Sample-size adjusted Bayesian (BIC) 2260.580 2260.580 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.100 0.101 ## 90 Percent confidence interval - lower 0.045 0.046 ## 90 Percent confidence interval - upper 0.163 0.164 ## P-value RMSEA &lt;= 0.05 0.064 0.063 ## ## Robust RMSEA 0.100 ## 90 Percent confidence interval - lower 0.046 ## 90 Percent confidence interval - upper 0.163 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.058 0.058 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~ ## support -0.393 0.083 -4.756 0.000 -0.393 -0.362 ## satisfaction ~ ## stress -0.717 0.104 -6.920 0.000 -0.717 -0.489 ## turnover_intent ~ ## satisfaction -0.682 0.049 -13.783 0.000 -0.682 -0.591 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .stress 0.466 0.057 8.237 0.000 0.466 0.869 ## .satisfaction 0.875 0.067 12.995 0.000 0.875 0.761 ## .turnover_intnt 0.996 0.074 13.531 0.000 0.996 0.651 ## ## R-Square: ## Estimate ## stress 0.131 ## satisfaction 0.239 ## turnover_intnt 0.349 By looking at the fit measures we can see that this model fits very well. But notice that we now have 3 degrees of freedom, that is, by further constraining the model (not allowing correlations between stress and support and removing the regression between support and turnover intent) we have expressed a more theoretically parsimonious model - very cool. Let´s make the path diagram and interpret the coefficients. semPaths( object = fit.2 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , curvePivot = TRUE , esize = 10 , asize = 5 #arrowhead size , sizeMan2 = 3 #height of the manifest variables , nCharNodes = 0 , edge.label.cex = 1.5 , residuals = FALSE , sizeMan = 15 , rotation = 2 , theme = &#39;Borkulo&#39;) #Note. The new comments are just arguments we didn´t use in the previous plot Cool, notice how we rephrased the connection between support and stress to a one-sided arrow. This can now be interpreted as a regression weight, that is, for every unit increase in support we se a .36 unit decrease in stress. We also see a bigger coefficient for the relationship between stress and satisfaction. This is likely due to the fact that we presume that support does not impact satisfaction but influences stress and thus, some of the variation we can explain in stress by support also influences satisfaction - i.e., stress mediates the relationship between support and satisfaction. The relationship between satisfaction and turnover intent is also different, but only by .01, that is, super small and an unimportant difference. Now that we have two overidentified models we can compare the fit of them. This is surprisingly easy; we only use the anova function and our to model fits. Note that this is not a GLM anova but an LRT test, that is, a likelihood ratio test. anova(fit.1, fit.2) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.1 1 2858.5 2891.7 3.0141 ## fit.2 3 2257.5 2279.6 11.8977 9.2498 2 0.009805 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So what does this tell us? It tells us that the baseline model(fit.1) fits the data significantly better then the other model(fit.2). However, one should not that the AIC and BIC is lower for the fit.2 model, low scores on AIC and BIC is indicative of good fit, and models with low values are preferable to those with higher BIC and AIC. I do not feel particularly strongly about any of these models, but if I had to choose one I would choose the second one since I like parsimony, but the first model technically fits better (though its almost identified so do with that what you will). For more on the LRT you can see: http://econ.upf.edu/~satorra/dades/BryantSatorraPaperInPressSEM.pdf One last thing we can do before we move on to other things is the check the modification indices. This is one cool thing that you can do with a path analysis that normal regression can´t really do. We will not use them for anything, but it is interesting to look at them. modificationindices(fit.2) %&gt;% arrange(-mi) %&gt;% head(20) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 1 support ~ satisfaction 8.440 0.131 0.131 0.208 0.208 ## 2 satisfaction ~ support 8.440 0.252 0.252 0.158 0.235 ## 3 stress ~~ satisfaction 8.440 0.299 0.299 0.468 0.468 ## 4 stress ~ satisfaction 8.440 0.341 0.341 0.500 0.500 ## 5 stress ~ turnover_intent 6.336 -0.109 -0.109 -0.184 -0.184 ## 6 support ~ turnover_intent 3.887 -0.067 -0.067 -0.124 -0.124 ## 7 stress ~~ turnover_intent 3.408 -0.082 -0.082 -0.121 -0.121 ## 8 satisfaction ~ turnover_intent 2.351 -0.171 -0.171 -0.197 -0.197 ## 9 turnover_intent ~ stress 2.351 -0.139 -0.139 -0.083 -0.083 ## 10 satisfaction ~~ turnover_intent 2.351 -0.170 -0.170 -0.182 -0.182 ## 11 turnover_intent ~ support 0.298 -0.048 -0.048 -0.026 -0.039 #Note. the %&gt;% is a pipe function, they make code simpler to write so look into them! See the provided programming resources in the beginning of the book. The modification indices (mi column) shows how much the chi2 of the model would drop if the parameter was included. Thus, indices over 3.84 will significantly increase the fit of the model. We have a few modifications we can make to our model, some of which improves the fit quite a bit. Lastly, let´s do a sensitivity check by running the second model through the raw data. That is, the data with the outliers included. sensitivity.fit &lt;- sem( model = model.2 , data = data , estimator = &#39;MLM&#39;) summary(sensitivity.fit, standardize = TRUE, fit.measures = TRUE , rsquare = TRUE) ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 322 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 10.159 10.615 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.017 0.014 ## Scaling correction factor 0.957 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 267.940 254.344 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.053 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.973 0.969 ## Tucker-Lewis Index (TLI) 0.945 0.939 ## ## Robust Comparative Fit Index (CFI) 0.972 ## Robust Tucker-Lewis Index (TLI) 0.944 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1234.025 -1234.025 ## Loglikelihood unrestricted model (H1) -1228.945 -1228.945 ## ## Akaike (AIC) 2480.049 2480.049 ## Bayesian (BIC) 2502.697 2502.697 ## Sample-size adjusted Bayesian (BIC) 2483.665 2483.665 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.086 0.089 ## 90 Percent confidence interval - lower 0.032 0.034 ## 90 Percent confidence interval - upper 0.147 0.151 ## P-value RMSEA &lt;= 0.05 0.119 0.108 ## ## Robust RMSEA 0.087 ## 90 Percent confidence interval - lower 0.034 ## 90 Percent confidence interval - upper 0.146 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.054 0.054 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~ ## support -0.346 0.073 -4.730 0.000 -0.346 -0.330 ## satisfaction ~ ## stress -0.694 0.099 -7.007 0.000 -0.694 -0.482 ## turnover_intent ~ ## satisfaction -0.672 0.048 -14.120 0.000 -0.672 -0.586 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .stress 0.493 0.060 8.155 0.000 0.493 0.891 ## .satisfaction 0.878 0.065 13.536 0.000 0.878 0.767 ## .turnover_intnt 0.989 0.074 13.405 0.000 0.989 0.657 ## ## R-Square: ## Estimate ## stress 0.109 ## satisfaction 0.233 ## turnover_intnt 0.343 anova(sensitivity.fit, fit.2) ## Warning in lavTestLRT(object = object, ..., model.names = NAMES): lavaan WARNING: some ## models have the same degrees of freedom ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## sensitivity.fit 3 2480.1 2502.7 10.159 ## fit.2 3 2257.5 2279.6 11.898 1.7384 0 #Note. We are comparing two models with the same degrees of freedom, this is not very appropriate since they are not nested (and on different data) but its nice to see the measures anyways. So, the model based on the raw data is not that different from we get when we have cleaned the data. This becomes even more apparent if we were to check the parameter estimates for the two models like so: parameterestimates(fit.2) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 stress ~ support -0.393 0.083 -4.756 0 -0.555 -0.231 ## 2 satisfaction ~ stress -0.717 0.104 -6.920 0 -0.920 -0.514 ## 3 turnover_intent ~ satisfaction -0.682 0.049 -13.783 0 -0.779 -0.585 ## 4 stress ~~ stress 0.466 0.057 8.237 0 0.355 0.577 ## 5 satisfaction ~~ satisfaction 0.875 0.067 12.995 0 0.743 1.007 ## 6 turnover_intent ~~ turnover_intent 0.996 0.074 13.531 0 0.851 1.140 ## 7 support ~~ support 0.454 0.000 NA NA 0.454 0.454 parameterestimates(sensitivity.fit) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 stress ~ support -0.346 0.073 -4.730 0 -0.489 -0.202 ## 2 satisfaction ~ stress -0.694 0.099 -7.007 0 -0.888 -0.500 ## 3 turnover_intent ~ satisfaction -0.672 0.048 -14.120 0 -0.765 -0.579 ## 4 stress ~~ stress 0.493 0.060 8.155 0 0.374 0.611 ## 5 satisfaction ~~ satisfaction 0.878 0.065 13.536 0 0.751 1.005 ## 6 turnover_intent ~~ turnover_intent 0.989 0.074 13.405 0 0.844 1.133 ## 7 support ~~ support 0.504 0.000 NA NA 0.504 0.504 They are pretty much the same parameter estimates, certainly not significantly different. So, what more can we do for this fun little analysis? we can do some bootstrapping. Lavaan has a really nice bootstrapping function that gives a lot of freedom to choose what to do with the estimates. Let´s bootstrap all estimates from the second model(fit.2) and take a percentile intervall for them. Note that this can take a few seconds. boot_sample &lt;- bootstrapLavaan( object = fit.2 #the fit we want to sample , R = 1000 #the number of iterations we want ) Notice that boot_sample is a matrix of many estimates. Let´s put the regression coefficients in a dataframe and make our intervals. boot_sample &lt;- data.frame(boot_sample) boot_coef &lt;- select(boot_sample #the data we want to take variables/cols from , c(1,2,3,)) #a list of the cols we want to take(1,2,3) quantile(boot_coef$stress.support #the estimate we want to take intervals on , probs = c(.025, .5, .975)) #the percentile points we want to see ## 2.5% 50% 97.5% ## -0.5489199 -0.3825642 -0.2386760 #We see that the regression weight ranges between -.55 and -.24 quantile(boot_coef$satisfaction.stress , probs = c(.025, .5, .975)) ## 2.5% 50% 97.5% ## -0.9758261 -0.7247085 -0.5454487 #We see that the regression weight ranges between -.98 and -.55 quantile(boot_coef$turnover_intent.satisfaction , probs = c(.025, .5, .975)) ## 2.5% 50% 97.5% ## -0.7719642 -0.6811282 -0.5928512 #We see that the regression weight ranges between -.77 and -.59 This is not super interesting, but it is pretty cool and also kind of illustrates what you can do with bootstrapping in a more manual way. There are of course much more you can do than to take percentile intervals of the regression weights but that lies beyond the specific realm of path analysis. 10.3 Multiple regression(again) By now you should feel good, both with lavaan semPlot and the general procedure for path analysis. So, lets take a step back and rework the good ´ol Field example for multiple regression, but with a path analysis. And don´t worry, this will be short and sweet :) 10.4 Mediation analysis In the Field book he gives an example of mediation with the data set from Lambert et al.,(2012). Since we are very comfortable with working with paths, mediation becomes a simple thing. In the regression example in the previous chapter we only estimated direct effects, that is, effects of a on c (or x on y if you prefer those letters). Mediation can estimate both direct and indirect effects. The indirect effect is the multiplication of path a and path b. Let´s load the data. While the process of a moderation analysis is quite complicated in terms of functions and packages - as we saw in the regression part of the book, with mediation we can rely on good ´ol lavaan. #Packages we need library(lavaan) library(semPlot) #Loading the data data &lt;- read_sav(&quot;Lambert et al. (2012).sav&quot;) names(data) &lt;- c(&#39;consumption&#39;, &#39;ln_porn&#39;, &#39;commit&#39;, &#39;infi&#39;, &#39;hook_ups&#39;) Let´s specify the model from the Field book. med_model.1 &lt;- (&#39; # Direct effect infi ~ c*consumption # Mediator commit ~ a*consumption infi ~ b*commit # Indirect effect (a*b) ab := a*b # Total effect total := c + (a*b) &#39;) #Note. The defined variables a, b and c are needed for the mediation analysis to work. These are the paths of our model, without them we cannot calculate our main and indirect effects! the lavaan symbol &#39;:=&#39; describes a new parameter dependent on our specified model. In this case it´s the regression weights (I believe) Let´s fit and summarise the result of this model. med_fit.1 &lt;- sem(med_model.1, data) #Summarising a lavaan can take a lot of commands, I will note what they do here as I have done before, but note that there are many other alternatives you can use! summary(med_fit.1 #the model we want to use(med_fit.1) , standardize = T #do we want a standardized estimate? T mean true/yes , fit.measures = T #do we want fit measures? , rsquare = T) #do we want r2 ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 35.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -501.418 ## Loglikelihood unrestricted model (H1) -501.418 ## ## Akaike (AIC) 1012.836 ## Bayesian (BIC) 1030.218 ## Sample-size adjusted Bayesian (BIC) 1014.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## consumptin (c) 0.107 0.038 2.797 0.005 0.107 0.171 ## commit ~ ## consumptin (a) -0.092 0.042 -2.175 0.030 -0.092 -0.139 ## infi ~ ## commit (b) -0.268 0.058 -4.612 0.000 -0.268 -0.282 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.429 0.039 10.932 0.000 0.429 0.878 ## .commit 0.531 0.049 10.932 0.000 0.531 0.981 ## ## R-Square: ## Estimate ## infi 0.122 ## commit 0.019 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.025 0.013 1.967 0.049 0.025 0.039 ## total 0.132 0.040 3.329 0.001 0.132 0.211 Nice, this is exactly what we want. Notice also that our model is saturated/identified, that is, it has 0 degrees of freedom. This means that we can´t really assess the fit of this model. Let´s make a quick interpretation. Infidelity increases with .107 units for every unit increase in consumption. Commitment decreases by .092 units for every increase in consumption and infidelity decreases with .268 for every unit increase in commitment. ´ab´ is our indirect effect, that is, .025. Which can be interpreted as a kind of r2. semPaths(med_fit.1 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , layout = &#39;tree&#39; , nCharNodes = 0 , edge.label.cex = 1.5 , sizeMan = 20 , residuals = F , rotation = 4 , theme = &#39;Borkulo&#39;) #Note that this is NOT the same output as in the field book. He uses the log transformed variable for consumption. So let´s recreate his model. med_model.2 &lt;- (&#39; # direct effect infi ~ c*ln_porn # mediator commit ~ a*ln_porn infi ~ b*commit # indirect effect (a*b) ab := a*b # total effect total := c + (a*b) &#39;) med_fit.2 &lt;- sem(med_model.2, data) summary(med_fit.2, standardize = T, fit.measures = T, rsquare = T) ## lavaan 0.6-9 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 33.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -502.418 ## Loglikelihood unrestricted model (H1) -502.418 ## ## Akaike (AIC) 1014.835 ## Bayesian (BIC) 1032.218 ## Sample-size adjusted Bayesian (BIC) 1016.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## ln_porn (c) 0.457 0.193 2.365 0.018 0.457 0.145 ## commit ~ ## ln_porn (a) -0.470 0.212 -2.215 0.027 -0.470 -0.142 ## infi ~ ## commit (b) -0.271 0.058 -4.642 0.000 -0.271 -0.285 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.432 0.040 10.932 0.000 0.432 0.886 ## .commit 0.531 0.049 10.932 0.000 0.531 0.980 ## ## R-Square: ## Estimate ## infi 0.114 ## commit 0.020 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.127 0.064 1.999 0.046 0.127 0.040 ## total 0.585 0.200 2.925 0.003 0.585 0.186 semPaths(med_fit.2 , what = &#39;est&#39; , style = &#39;lisrel&#39; , layout = &#39;tree&#39; , nCharNodes = 0 , edge.label.cex = 1.5 , sizeMan = 20 , residuals = F , theme = &#39;Borkulo&#39; , rotation = 4) #Very nice, we have recreated the findings from Field. These are the same models, but with the log transformation. The reason why I didn´t use that in the first model is because it´s harder to interpret. I have a hard time understanding what this indirect effect of .127 means since its a combination of the influence of a log variable, through a variable that is not logged. A smarter person than I will have to describe what this means. Field also uses bootstrapped standard errors; this can be specified in the fit portion of our workflow. Let´s refit our second model but with bootstrapped SEs. #it´s going to take a few seconds so do not fret if you dont get an output set.seed(234) #setting seed so that we can recreate the random values med_fit.2 &lt;- sem(med_model.2 #what model we want to fit , se = &#39;bootstrap&#39; #how do we want to estimate the std.err? , data) #indicates the data we want to fit the model on. summary(med_fit.2, standardize = T, fit.measures = T, rsquare = T) ## lavaan 0.6-9 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 33.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -502.418 ## Loglikelihood unrestricted model (H1) -502.418 ## ## Akaike (AIC) 1014.835 ## Bayesian (BIC) 1032.218 ## Sample-size adjusted Bayesian (BIC) 1016.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## ln_porn (c) 0.457 0.251 1.822 0.068 0.457 0.145 ## commit ~ ## ln_porn (a) -0.470 0.234 -2.004 0.045 -0.470 -0.142 ## infi ~ ## commit (b) -0.271 0.071 -3.824 0.000 -0.271 -0.285 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.432 0.055 7.908 0.000 0.432 0.886 ## .commit 0.531 0.050 10.539 0.000 0.531 0.980 ## ## R-Square: ## Estimate ## infi 0.114 ## commit 0.020 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.127 0.071 1.798 0.072 0.127 0.040 ## total 0.585 0.250 2.339 0.019 0.585 0.186 parameterestimates(med_fit.2) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 infi ~ ln_porn c 0.457 0.251 1.822 0.068 -0.032 0.960 ## 2 commit ~ ln_porn a -0.470 0.234 -2.004 0.045 -0.917 0.019 ## 3 infi ~ commit b -0.271 0.071 -3.824 0.000 -0.404 -0.124 ## 4 infi ~~ infi 0.432 0.055 7.908 0.000 0.314 0.531 ## 5 commit ~~ commit 0.531 0.050 10.539 0.000 0.434 0.630 ## 6 ln_porn ~~ ln_porn 0.049 0.000 NA NA 0.049 0.049 ## 7 ab := a*b ab 0.127 0.071 1.798 0.072 -0.005 0.281 ## 8 total := c+(a*b) total 0.585 0.250 2.339 0.019 0.089 1.096 Cool, now we have a SE estimate of .071 instead of .064. Note also that the ci for the ab effect, that is, the indirect effect, ranges from -.005 to .28, we cross zero and can therefore not be certain in the existence of the mediation. The only robust effect is that of commitment on infidelity. Let´s finish of by fitting a normal linear interaction model and compare the outcomes. lm_model &lt;- lm(infi ~ consumption*commit, data) summary.lm(lm_model) ## ## Call: ## lm(formula = infi ~ consumption * commit, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.94415 -0.36188 -0.13989 0.00893 2.00893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.53260 0.42566 3.601 0.000387 *** ## consumption -0.05829 0.19836 -0.294 0.769133 ## commit -0.33807 0.10103 -3.346 0.000954 *** ## consumption:commit 0.04142 0.04864 0.852 0.395292 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6591 on 235 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.1251, Adjusted R-squared: 0.1139 ## F-statistic: 11.2 on 3 and 235 DF, p-value: 6.744e-07 This output seems very nice to me. It very much confirms what our mediation analysis found. The only reliable effect is that of commitment. "],["exploratory-factor-analysis-efa.html", "11 Exploratory factor analysis (EFA)", " 11 Exploratory factor analysis (EFA) "],["confirmatory-factor-analysis.html", "12 Confirmatory factor analysis", " 12 Confirmatory factor analysis "],["multi-level-modelling.html", "13 Multi-level modelling", " 13 Multi-level modelling "],["structural-equation-modelling.html", "14 Structural equation modelling", " 14 Structural equation modelling "],["meta-analysis.html", "15 Meta-analysis", " 15 Meta-analysis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
