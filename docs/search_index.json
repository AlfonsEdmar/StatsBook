<<<<<<< HEAD
[["index.html", "Not yet decided 1 Table of contents", " Not yet decided B. Alfons Edmar 2022-05-11 1 Table of contents Introduction Foreword Some useful resources Welcome to R The R environment Basic programming Organisation Rmarkdown What I expect you to know Descriptive statistics Basic null hypothesis testing Statistical phenomena (regression to the mean, CLT, LOLN) Useful R packages and functions (with links) Bayesian and Frequentist statistics Non-parametric statistics Asking all the question, making statements, and assuming The old guard Bootstrapping Correlation and Covariation in R Welcome to the matrix Regression analysis Univariate regression and Multiple regression The general linear model (ANOVA) The generalized linear model(logit) Cross validation LASSO (least absolute shrinkage and selection operator) Ridge regression Fully Bayesian regression Poststratification Path analysis Multiple regression(again) Mediation analysis Exploratory factor analysis (EFA) Confirmatory factor analysis (CFA) Multi-level modelling (MLM) Structural equation modelling (SEM) Meta-analysis "],["introduction.html", "2 Introduction 2.1 Foreword 2.2 Some useful resources", " 2 Introduction 2.1 Foreword I thought It would be fun to create a book covering the things that I have learnt this past year and a half while creating a (hopefully) useful tool for future students. I also really enjoy opensource projects such as R and really think that it is a brilliant workspace. It is not only flexible but creates a very structured environment where all your decisions throughout the analysis is easily documented and shared. In short, I like R and RStudio. The problem is that R is hard, the learning curve is slow, and if you are a psychologist, the opportunities to learn programming are sparse unless you seek them out for yourself. With this book I hope to create an accessible guide to how you can use R for your studies and hopefully your future work. Now is a good time to point out that I am not a statistician, as of writing this I have not even graduated from the master´s program. Thus, this book should not be treated as an authoritative voice on how you should do statistics or interpret your results. The aim here is simple, it is to guide you through some of the statistical analyses that you will encounter and introduce the R programming language in the meantime. There will of course be some explanations of the tools that we use in terms of their statistical properties and interpretations of the results from our analyses, but I cannot stress enough that this is not a book about statistics, it´s a book about doing statistics in R. It´s designed to be brief and easily skimmed though in the sense that you wont have to read so much of my blathering to understand the interesting bits - the code. The general layout of the applied chapters will be analyses of the working examples in Discovering statistics by Andy Field (when applicable) and some workshop exercises. I will also end each applied chapter by finding some real data from an article and working through their analysis. In sum, this is a compilation of the topics covered in the master´s programme in psychological science at the university of Gothenburg in 2021/22 with some additional chapters by yours truly on some of the things that I think are useful and important. It is a supplementary text that simply translates the analyses described in the literature into R-code. It is for students who want to learn R but dont know where to start. It can be hard to learn things on your own without any real incentives, and I hope that this book can coax some people into R through it´s accessibility and relevance to what you will be doing in class anyways. Good luck and happy reading/coding. P.s, If you are a statistician, stop reading here, only pain will follow. 2.2 Some useful resources "],["welcome-to-r.html", "3 Welcome to R 3.1 The R environment 3.2 Basic programming 3.3 Organisation 3.4 Rmarkdowm", " 3 Welcome to R 3.1 The R environment 3.2 Basic programming 3.3 Organisation 3.4 Rmarkdowm "],["what-i-expect-you-to-know.html", "4 What I expect you to know 4.1 Descriptive statistics 4.2 Basic null hypothesis testing 4.3 Statistical phenomena", " 4 What I expect you to know 4.1 Descriptive statistics 4.2 Basic null hypothesis testing 4.3 Statistical phenomena "],["useful-packages-and-functions-that-helped-me-learn.html", "5 Useful packages and functions that helped me learn 5.1 Tidyverse 5.2 Distribution functions - rnorm, dbeta, rbinom etc 5.3 Faux 5.4 lm 5.5 Lavaan 5.6 semPlot 5.7 ?", " 5 Useful packages and functions that helped me learn This is a bit of a strange chapter, I am not entirely sure why I included it, but I think it might be a good idea to have a few words on the packages that we will be using. This Chapter could be returned two If you get stuck with some code and for some really strange reason can´t find a better place to search for answers than here. In sum, there are some useful packages and function in R, and using them might be beneficial for both learning and doing you statistics. 5.1 Tidyverse Like the name implies, tidyverse is an entire universe of connected packages designed to make your coding comfortable and aesthetically pleasing. I neither have the expertise nor the time to go through even the basics of all the tidyverse packages but we will be using them frequently, and when it happens there will be some notation. In the meantime, the first chapter of R for data science is an absolute gem if you are serious about learning R. Very nice introduction, assuming no previous coding experience (and it´s free) I cannot recommend it enough (hence the repeat) https://r4ds.had.co.nz/ 5.2 Distribution functions - rnorm, dbeta, rbinom etc These functions were probably what got me to actually think I understood some things about statistics, which is quite impressive for me. They are essentially functions that describe the shape of certain distributions and gives you the ability to generate random variables from those distributions. The possibilities of this is really endless but I will show some examples of how to generate data below. #Since the data is &quot;random&quot; it will be impossible to reproduce if you re-run the code. This is why we usually set a seed. This makes it so the random data can be reproduced. We do this through the set.seed function before we generate random numbers. Fun fact, the numbers are not truly random since they can be reproduced exactly, they are pseudo-random. #Random data following the normal distribution set.seed(5395) #setting the seed, this can by any combination of integers. norm_data &lt;- rnorm( n = 500 #the number of observations we want , mean = 4 #the mean of the numbers , sd = 1 #the standard deviation of the data ) #Prof of normality through histogram hist(norm_data) #Voila. #We can do the same with the t-distribution set.seed(453) t_data &lt;- rt( n = 50 #number of observations , df= 6 #degrees of freedom ) hist(t_data) #the dame things can be done to the F distribution using rf() #If we want binary outcomes we can use the binomial distribution with r(binom) #lets say we want to simulate 10 tosses of a fair coin for example. set.seed(5325) coin_data &lt;- rbinom( n = 10 #number of observations/tosses , size = 1 #number of trials per observation(one this time) , prob = .5 #the probability of a successful outcome (50/50) ) hist(coin_data) #Cool, If we say 1 is tails we have 6 tails and 4 heads. These might seem like very trivial things, but by manipulating the data and doing analyses on data sets where you actually KNOW what the true population parameters are, can be very useful and enlightening. Simulation is the best kind of preparation; it also highlights very well what you expect from the data. The next package I can recommend handles data generation quite smoothly. 5.3 Faux One of the most powerful properties of programming languages is their ability to generate pseudo random variables. We have seen that we can easily generate independent random numbers following some distribution, but what if we want to simulate relationships between variables? enter faux. There are probably many other packages doing the same things, but this is the one I use, and it has served me quite well.  let´s generate the data that we will work with through this chapter. library(&#39;faux&#39;) ## ## ************ ## Welcome to faux. For support and examples visit: ## https://debruine.github.io/faux/ ## - Get and set global package options with: faux_options() ## ************ set.seed(324) #the seed returns! data &lt;- rnorm_multi(n = 500 #the number of observations we want , vars = 3 #number of variables we want , mu = c(3,5,2)#the three means for our variables. , sd = c(1.3, 2, .6) #the three SDs , r = c(1, .4, .7, #this is where it gets tricky. r is .4, 1, .4, #the correlation matrix. If we have .7, .4, 1) #many variables it can be tedious , varnames = c(&#39;y&#39;, &#39;x&#39;, &#39;z&#39;)) #the names of our variables #lets look at the correlations and the mean of our data cor(data) #correlates all variables in the data set with each other ## y x z ## y 1.0000000 0.3674651 0.6744010 ## x 0.3674651 1.0000000 0.4105545 ## z 0.6744010 0.4105545 1.0000000 summary(data) #summarises the content in the data ## y x z ## Min. :-0.3747 Min. :-0.8381 Min. :0.2554 ## 1st Qu.: 1.9188 1st Qu.: 3.6476 1st Qu.:1.5958 ## Median : 2.9566 Median : 4.9841 Median :2.0288 ## Mean : 2.9204 Mean : 5.0046 Mean :2.0239 ## 3rd Qu.: 3.7996 3rd Qu.: 6.3985 3rd Qu.:2.4164 ## Max. : 6.5359 Max. :12.3798 Max. :3.8473 #Notice how the values are similar, but not exactly the same, why is that? it is because of randomness. We can remedy this if you want to have full control over the parameter estimates in your data. We do this adding the empirical = true to the data generation formula like so: data.2 &lt;- rnorm_multi(n = 500 , vars = 3 , mu = c(3,5,2) , sd = c(1.3, 2, .6) , r = c(1, .4, .7, .4, 1, .4, .7, .4, 1) , varnames = c(&#39;y&#39;, &#39;x&#39;, &#39;z&#39;) , empirical = TRUE) #do we want empirical estimates? #This is now empirical data and not data sample from a population cor(data.2) ## y x z ## y 1.0 0.4 0.7 ## x 0.4 1.0 0.4 ## z 0.7 0.4 1.0 summary(data.2) ## y x z ## Min. :-0.8063 Min. :-0.8758 Min. :0.293 ## 1st Qu.: 2.1405 1st Qu.: 3.7060 1st Qu.:1.590 ## Median : 2.9691 Median : 4.9931 Median :2.015 ## Mean : 3.0000 Mean : 5.0000 Mean :2.000 ## 3rd Qu.: 3.9539 3rd Qu.: 6.2568 3rd Qu.:2.398 ## Max. : 6.5047 Max. :11.9103 Max. :3.605 #And there it is, identical to how we specified it in the rnorm_multi function. Now that we have some data, let´s try to fit it to a model 5.4 lm As we will a little bit more in-depth below, R generally uses strings from specifying models. That is, if we want to regress Y on x we specify that as Y~X. You will be doing a lot of that little squiggly(~), or, as some call it tilde(it´s proper name). Most if not all simple statistics can be done using the linear model function lm(). There are many functions for various types of t-test and chi2 and others but you will get really far with just the lm(), at the end of the day most tests all fall under the general linear model. Let´s fit a linear model with the data we just generated, we can predict y with x and z. fit.lm &lt;- lm( formula = y ~ x + z #the formula of the regression , data = data #the data we want to use ) #When we have fitted a model, we need to summarise the output. There is on specifically for linear models that we can use. summary.lm(object = fit.lm) ## ## Call: ## lm(formula = y ~ x + z, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7678 -0.5999 -0.0311 0.6150 2.9941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.25511 0.16116 -1.583 0.1141 ## x 0.07015 0.02318 3.027 0.0026 ** ## z 1.39556 0.07977 17.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9565 on 497 degrees of freedom ## Multiple R-squared: 0.4647, Adjusted R-squared: 0.4625 ## F-statistic: 215.7 on 2 and 497 DF, p-value: &lt; 2.2e-16 Cool, this is a run of the mill multiple regression. But remember that you cannot only fit our model, you must also summarise it, be that through summary.lm() or by any other means. We´ll get more in to lm() in the regression chapters. Let´s move on to a real big boy, lavaan. 5.5 Lavaan Lavaan, or, latent variable analysis, is an R package that we will use quite a bit. As the name implies it´s main function is to do latent variable analysis. If you do not know what latent variable analysis is, don´t worry, you soon will. The main thing that can be a bit tricky with lavaan is that it uses different kind of model specification than we usually do in simple analyses such as linear models. Contrary to those models, lavaan works in three steps. - first: you specify your model - second: you fit your model - third: you summarise the output of the model fit. If we compare this to the previous two step process of fitting an lm, it is the separation of model specification and model fitting that can take you for a loop. Lavaan models are specified as strings, that is, characters. These characters are then applied to a fitting function. To make things clearer, let´s play around with it a bit. We preciously specified the model of y ~ x + z. This is a character string that communicates that y is regressed on x + z. Lavaan works the same way, so if we wanted to fit a regression in lavaan we can use that exact string, the difference is that we specify the modal as a separate object from the fit. library(lavaan) ## This is lavaan 0.6-9 ## lavaan is FREE software! Please report any bugs. #Step 1: specify the model model &lt;- (&#39;y ~ x + z&#39;) #Step 2: fit the specifid model with the sem function fit &lt;- sem(model = model, data = data) #Step 3: summarise the output summary(fit) ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 0.070 0.023 3.036 0.002 ## z 1.396 0.080 17.547 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.909 0.058 15.811 0.000 Now, if we look at the regression section in the bottom, we can see that the regression output is exactly the same as we would get if we did a normal lm regression. Let´s print out that regression we fitted earlier. summary.lm(fit.lm) ## ## Call: ## lm(formula = y ~ x + z, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7678 -0.5999 -0.0311 0.6150 2.9941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.25511 0.16116 -1.583 0.1141 ## x 0.07015 0.02318 3.027 0.0026 ** ## z 1.39556 0.07977 17.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9565 on 497 degrees of freedom ## Multiple R-squared: 0.4647, Adjusted R-squared: 0.4625 ## F-statistic: 215.7 on 2 and 497 DF, p-value: &lt; 2.2e-16 And there you go, it´s very much the same. Now, why would you want to do all this other stuff just to fit a regression? well, you wouldnt. But this is not the point of lavaan, lavaan can specify very complex models ranging from latent variable models to multilevel models, and all this is done though the mode character string, also knowns as the lavaan syntax. Below I will give some examples of what type of models you can fit with lavaan. I will plot them out using a package called semPlot - but we will get to that in due time. library(semPlot) #A simple correlation model cor_model &lt;- (&#39; #Correlations are described by &quot;~~&quot; y ~~ x&#39; ) semPaths(sem(cor_model)) #A simple regression model reg_model &lt;- (&#39; #Regressions are described by &quot;~&quot; y ~ x&#39;) semPaths(sem(reg_model), rotation = 2) #A simple mediation model med_model &lt;- (&#39; y ~ x + z z ~ x&#39; ) semPaths(sem(med_model)) #a simple latent variable model sem_model &lt;- (&#39; #Latent variable as describe by &quot;=~&quot; y =~ a + b + c + d x =~ c + d + e + f &#39;) semPaths(sem(sem_model)) This is just a little taste of what lavaan can do. If your data can handle it, you can make incredibly interesting models. We will use lavaan for path analysis, mediation analysis, SEM and multi-level SEM. If you want to go Bayesian, there is even a Bayesian alternative under the name blavaan. Again, this is barely scratching the surface, and if you want to get deeper insight into the workings of lavaan i can recommend going straight to the source: https://lavaan.ugent.be/tutorial/index.html 5.6 semPlot The semPlot package is used to visual/plot SEMs, CFAs and path analyses. We have already used semPlot to graph the sem models above. Now, graphing sems can be quite annoying in the beginning. The documentation is very very extensive, but not super easy to understand (IMO) and before you have a basic understanding of the semPaths function reading the documentation might feel overwhelming. I would recommend going to this very concise video by the author of the package for a nice little overview: https://www.youtube.com/watch?v=rUUzF1_yaXg&amp;ab_channel=SachaEpskamp 5.7 ? Last, but certainly not least, the question mark. This might be the most useful little trick R has to offer. All packages/functions have built in documentation that explains it´s uses and intricacies. Most even have examples of how to use the function. I cannot stress enough how useful this is, not only is it good for solving problems in your code quickly, but all this documentation is a wealth of knowledge. Though brief, most if not all documentation has useful references and information that can broaden your knowledge of what you are actually doing. So how does it work? you simply type ? before a function. Let´s say we want to know more about how to fit lavaan models with the sem function, then we simple write: ?sem #Maybe we have issues generating correct correlations and have to revisit how to use rnorm_multi ?rnorm_multi #Perhaps just the rnorm? ?rnorm Make use of this tool! it can help you a lot. Also, I don´t like leaving a mess. So now that we are done, we should clean up. The fastest way of removing things in R is with the rm() function. This removes singular things from the environment. If we want to remove the coin_data we can simply write: rm(coin_data) And it´s gone. Now you will be glad that you have all your things documented neatly, that way you can load and remove things at the press of a button without having to worry about losing things. But if we want to clean everything out (as I often want) we can write the line: rm(list = ls()) To be completely honest I not know exactly what this line means, but it does a great job cleaning. "],["bayesian-and-frequentists-statistics.html", "6 Bayesian and Frequentists statistics 6.1 Likelihood functions 6.2 Density distributes 6.3 All hail the almighty Gauss", " 6 Bayesian and Frequentists statistics 6.1 Likelihood functions 6.2 Density distributes 6.3 All hail the almighty Gauss "],["non-parametric-statistics.html", "7 Non-parametric statistics 7.1 Asking all the question, making statements, and assuming 7.2 The old guard 7.3 Bootstrapping", " 7 Non-parametric statistics 7.1 Asking all the question, making statements, and assuming 7.2 The old guard 7.3 Bootstrapping "],["correlation-and-covariation-in-r.html", "8 Correlation and Covariation in R 8.1 Welcome to the matrix", " 8 Correlation and Covariation in R 8.1 Welcome to the matrix "],["regression-analysis.html", "9 Regression analysis 9.1 Univariate regression and multiple regression 9.2 The general linear model (ANOVA) 9.3 The generalized linear model(logit) 9.4 Cross validation 9.5 LASSO (least absolute shrinkage and selection operator) 9.6 Ridge regression 9.7 Fully Bayesian regression 9.8 Poststratification", " 9 Regression analysis 9.1 Univariate regression and multiple regression Packages need to follow along with the code, special packages will be loaded when and if needed. library(tidyverse) library(car) library(haven) library(here) Regression is not always easily visualised. This here is a function that will visualise a univariate slope along with some of the most important measurements. This is a slight adaption from a function I found here: find the article regplot &lt;- function (fit, name) { ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + geom_jitter( alpha = .5, size = 2) + stat_smooth(method = &quot;lm&quot;) + labs(title = name, subtitle = paste(&quot;Adj R2 = &quot;,signif(summary(fit)$adj.r.squared, 5), &quot;Intercept =&quot;,signif(fit$coef[[1]],5 ), &quot; Slope =&quot;,signif(fit$coef[[2]], 5), &quot; P =&quot;,signif(summary(fit)$coef[4], 5)))+ theme_bw() } Field uses data from album sales. When loading data in r the best way is to put your data files in an easy to access place - note that this is my path, the data file is album sales from chapter 8 or 9 depending on edition so load that. data &lt;- read_sav(here(&quot;Album Sales.sav&quot;)) #Let´s change the names to low caps so we can type easily. names(data) &lt;- c(&#39;adverts&#39;, &#39;sales&#39;, &#39;airplay&#39;, &#39;image&#39;) #Taking a peek at the data head(data) ## # A tibble: 6 x 4 ## adverts sales airplay image ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.3 330 43 10 ## 2 986. 120 28 7 ## 3 1446. 360 35 7 ## 4 1188. 270 33 7 ## 5 575. 220 44 5 ## 6 569. 170 19 5 summary(data) ## adverts sales airplay image ## Min. : 9.104 Min. : 10.0 Min. : 0.00 Min. : 1.00 ## 1st Qu.: 215.918 1st Qu.:137.5 1st Qu.:19.75 1st Qu.: 6.00 ## Median : 531.916 Median :200.0 Median :28.00 Median : 7.00 ## Mean : 614.412 Mean :193.2 Mean :27.50 Mean : 6.77 ## 3rd Qu.: 911.226 3rd Qu.:250.0 3rd Qu.:36.00 3rd Qu.: 8.00 ## Max. :2271.860 Max. :360.0 Max. :63.00 Max. :10.00 All variables are treated as numerical. Before any formal analysis can be pursued, we should first visually inspect the variables. #I usually prefer histograms without density curves but lets use them both to visualise the distribution of our data. ggplot(data, aes(adverts))+ geom_histogram(aes(y = ..density..), color = &#39;steelblue&#39;, size = 1)+ geom_density() ggplot(data, aes(sales))+ geom_histogram(aes(y = ..density..), color = &#39;steelblue&#39;, size = 1)+ geom_density() ggplot(data, aes(airplay))+ geom_histogram(aes(y = ..density..), color = &#39;steelblue&#39;, size = 1)+ geom_density() ggplot(data, aes(image))+ geom_histogram(aes(y = ..density..), color = &#39;steelblue&#39;, size = 1)+ geom_density() #looks pretty good, the ..density.. call is what gives the curve. So this is what we are working with, our IVs dont need to be normal, the residuals need to be normal however. We dont want heteroskedasticity. The neat thing with having multiple continuous variables is that it opens up plotting possibilities. Lets make some graphs ggplot(data = data, aes(y = sales, x = adverts , size = image, colour = airplay))+ geom_point(alpha = .8)+ theme_bw() #It is a bit messy, lets clean our plot by colouring by image. Lets first colour by images thats less than 5 ggplot(data = data, aes(y = sales, x = adverts , size = airplay, colour = image &lt; 5))+ geom_point(alpha = .5)+ theme_bw() #interesting, most bands have an image at 5 or above, and those that dont do not sell well regardless of their advert budget or image (the size of the points are roughly the same). Lets colour by image &lt;8 ggplot(data = data, aes(y = sales, x = adverts , size = airplay, colour = image &lt; 8))+ geom_point(alpha = .5)+ theme_bw() #Pretty graphs right? This is a better spread. But we can see that image does not seem to impact sales that much. There seems to be an effect, however. The most interesting thing with this graph however is that a high advert budget more or less promises good sales. No band with a budget less than 1 000 000 pounds have less than 100 000 album sales. A crude heuristic for bands wanting to fund their adverts with album sales could be to not sell their album for less than 10£. But this is of course bad advice. We have no information on the cost of the albums and sales are most likely affected by the cost of the album. Nevertheless, this is a fun graph. Lets make image into a factor so we can split our graphs better using the facet_wrap function plotdata &lt;- data plotdata$image &lt;- as.factor(plotdata$image) #Now image is a factor with to levels. Lets use the image of the band as colour ggplot(data = plotdata, aes(y = sales, x = adverts , size = airplay, colour = image))+ geom_point(alpha = .8)+ facet_wrap(~image)+ theme_bw() #Or ggplot(data = plotdata, aes(y = sales, x = adverts , colour = airplay))+ geom_point(size = 3, alpha = .8)+ facet_wrap(~image)+ theme_bw() #Note that one graph colours airplay while the other image Another interesting graph. Interestingly we can see that the band with the highest image has no advert budget. However, very few bands with that high image exists. The main idea this graph gives is that the slope between sales and advert budget does not seem to depend heavily on the image of the band. However, bands with low image often have less advert budget - and less sales. we can also see a relationship between airplay and sales, bigger points are usually a bit higher up on sales than small points. So what are our expectations given the graphs? we should expect an effect of image airplay and adverts on sales, but there will probably not be any meaningful interactions between them since no relationship was observable in the graphs. lets see if this holds up. our first model is a simple regression of adverts on album sales. We would expect more sales with more advert budget take a look at the lm function for syntax and code reference. ?lm #fitting the model m1 &lt;- lm(sales ~ adverts, data = data) One of the most important things we have to do when analysing data using statistical methods is checking that our assumptions for our method holds. For regression that: normality of residuals, independent errors, no multicollinearity, and homogeneity of variance in the residuals. Lets check these things out. #Checking the residuals and normality assumption ggplot(data = data, aes(y = predict.lm(m1), x = m1$residuals))+ geom_point(size = 3, alpha = .4)+ geom_smooth(method = &#39;lm&#39;) #Nice, the predicted values of our model is distributed quite normally across the residuals. This indicates that the variation in our model is roughly equal on low, medium, and high point estimates. We can also check the density of the #Residuals - they should be normal around 0. ggplot(data = data, aes(x = m1$residuals))+ geom_density() ggplot(data = data, aes(x = m1$residuals))+ geom_histogram(colour = &#39;steelblue&#39;, size = 1) #Nice, this means that our error in measurement isnt systematically lower or higher than our fitted model. #Checking error independence with Durbin Watson test durbinWatsonTest(m1) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.04394305 2.032324 0.772 ## Alternative hypothesis: rho != 0 #Nice, the p-value is insignificant and the test-statistic is close to 2. Now that we have checked our assumptions and have a good ground to stand on, we can check the fit of our model. #Checking the fit of the model summary.lm(m1) ## ## Call: ## lm(formula = sales ~ adverts, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.949 -43.796 -0.393 37.040 211.866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.341e+02 7.537e+00 17.799 &lt;2e-16 *** ## adverts 9.612e-02 9.632e-03 9.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 65.99 on 198 degrees of freedom ## Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 ## F-statistic: 99.59 on 1 and 198 DF, p-value: &lt; 2.2e-16 #Lets plot this manually in ggplot using geom_smooth, on our model ggplot(data, aes(y = sales, x = adverts))+ geom_point(alpha = .5, size = 2)+ geom_smooth(method = &#39;lm&#39;)+ theme_bw() #Geom smooth fits a linear model to the data, method = &#39;lm&#39; tells it that its a linear model. And there we have it, but by using the regplot function we created above we can put some of the info from the model on the plot. regplot(m1, &#39;m1&#39;) summary.lm(m1) ## ## Call: ## lm(formula = sales ~ adverts, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.949 -43.796 -0.393 37.040 211.866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.341e+02 7.537e+00 17.799 &lt;2e-16 *** ## adverts 9.612e-02 9.632e-03 9.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 65.99 on 198 degrees of freedom ## Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 ## F-statistic: 99.59 on 1 and 198 DF, p-value: &lt; 2.2e-16 #Nice graph with good info :) Pretty neat right? now, what can we make of this? the slope estimates of ca 0.1 indicates that sales increase by .1 for every unit of adverts. The equates to roughly 100 sales per thousand pounds in advertising budget. Also, note the shaded line. This is our marginal 95% confidence interval. We can see that our uncertainty in the parameter estimate increases at the mode extreme values since we dont have a lot of data regarding the dispersion on those levels of adverts. We should also check for influential cases analytically, not just visually. we can do this with the function lm.influence. summary(influence.measures(m1)) ## Potentially influential observations of ## lm(formula = sales ~ adverts, data = data) : ## ## dfb.1_ dfb.advr dffit cov.r cook.d hat ## 1 0.35 -0.27 0.35_* 0.93_* 0.06 0.01 ## 10 0.22 -0.15 0.22 0.97_* 0.02 0.01 ## 11 0.01 -0.02 -0.03 1.04_* 0.00 0.03_* ## 23 -0.02 0.04 0.04 1.05_* 0.00 0.03_* ## 42 0.29 -0.22 0.29 0.95_* 0.04 0.01 ## 43 -0.02 0.04 0.05 1.06_* 0.00 0.05_* ## 75 -0.02 0.04 0.05 1.03_* 0.00 0.02 ## 87 -0.06 0.11 0.12 1.05_* 0.01 0.05_* ## 88 0.02 -0.03 -0.03 1.05_* 0.00 0.04_* ## 92 -0.01 0.01 0.01 1.03_* 0.00 0.02 ## 93 -0.01 0.03 0.03 1.04_* 0.00 0.03 ## 113 -0.13 0.04 -0.17 0.96_* 0.01 0.01 ## 124 0.09 0.02 0.18 0.96_* 0.02 0.01 ## 126 0.05 -0.10 -0.11 1.03_* 0.01 0.02 ## 169 0.32 -0.23 0.33_* 0.92_* 0.05 0.01 ## 175 0.03 -0.07 -0.08 1.03_* 0.00 0.02 ## 184 0.08 -0.13 -0.13 1.08_* 0.01 0.06_* summary(cooks.distance(m1)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.000e-08 3.380e-04 1.576e-03 4.416e-03 6.314e-03 5.716e-02 #lets look at the hat values manually summary(hatvalues(m1)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.005000 0.005732 0.008030 0.010000 0.011305 0.063529 hatvalues(m1)%&gt;% sort.default(decreasing = TRUE) %&gt;% head(20) ## 184 43 87 88 23 11 93 126 ## 0.06352892 0.04590335 0.04502948 0.03687259 0.03442720 0.03108016 0.02569576 0.02435532 ## 175 28 55 46 62 75 199 92 ## 0.02353610 0.02344638 0.02334463 0.02201132 0.02170914 0.02170914 0.02170914 0.01997151 ## 3 128 20 102 ## 0.01971805 0.01894939 0.01751014 0.01656749 Going by cooks distance we dont seem to have any very influential values, that is, values exceeding 1. However, we have some values that exceeds three times the leverage or hatvalues. if we go by hoaglin and welch guides of looking at values exceeding three times the average, that is values exceeding .03. that is, observation 11, 23, 88, 87, 43 and 184. What happens if we remove them? clean_data &lt;- slice(data, -c(11, 23, 88, 87, 43, 184)) clean_m1 &lt;- lm(sales~adverts, clean_data) regplot(clean_m1, &#39;clean m1&#39;) summary.lm(clean_m1) ## ## Call: ## lm(formula = sales ~ adverts, data = clean_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.910 -44.484 -0.291 38.368 211.845 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 134.18426 8.00113 16.771 &lt; 2e-16 *** ## adverts 0.09596 0.01116 8.602 2.76e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 66.9 on 192 degrees of freedom ## Multiple R-squared: 0.2782, Adjusted R-squared: 0.2744 ## F-statistic: 73.99 on 1 and 192 DF, p-value: 2.763e-15 #. Does not change much in terms of estimation. We can compare the graphs using the grid arrange function from the gridExtra package library(gridExtra) m1_plot &lt;- regplot(m1, &#39;original model&#39;) clean_m1_plot &lt;- regplot(clean_m1, &#39;influential observations removed&#39;) grid.arrange(m1_plot, clean_m1_plot, name = c(&#39;original&#39;, &#39;clean&#39;)) #. Here we can easily see that the influential observations were the cases with very high advertising budgets. it might be prudent to use the updated model, but no model is better than the other. We should use the model that helps us answer our research question best - horses for courses A fast way, but less pleasing and not as hands on, is to simply use the plot function. It gives much of the same information plot(m1) plot(clean_m1) #. Note that the spread of fitted to residuals is much nicer when we exclude the influential observations. lets kick this up a notch and run a multiple regression on a split sample so that we can cross-validate our model. lets take all the variables in our data #Splitting our sample randomly using rbinom set.seed(123) data_partition &lt;- rbinom(200, 1, prob = .5) data &lt;- cbind(data, data_partition) rm(data_partition) #The binary variable &quot;data partition&quot; is our random divide of the sample in to 2. We will run the regression on one part of the sample and see how well it predicts the data in the second sample. lets call it training data and test data. training_data &lt;- filter(data, data_partition == 1) glimpse(training_data$data_partition) ## int [1:97] 1 1 1 1 1 1 1 1 1 1 ... test_data &lt;- filter(data, data_partition == 0) glimpse(test_data$data_partition) ## int [1:103] 0 0 0 0 0 0 0 0 0 0 ... Now we have our two samples, lets fit our training model. it works the same way as the simple regression, but we just add the additional IVs like you would in a normal regression formula. but first lets standardize the data so its more easily comparable #Standardising, or, &quot;scaling&quot; our data z_training_data &lt;- data.frame(scale(training_data)) z_test_data &lt;- data.frame(scale(test_data)) #Fitting the multiple regression training_model &lt;- lm(sales ~ adverts + image + airplay, data = z_training_data) Like for the simple regression we need to check our assumptions #Checking the residuals and normality assumtions ggplot(data = training_data, aes( y = predict.lm(training_model) , x = training_model$residuals))+ geom_point(size = 3, alpha = .4)+ geom_smooth(method = &#39;lm&#39;) #Looks good ggplot(data = training_data, aes(x = training_model$residuals))+ geom_density() ggplot(data = training_data, aes(x = training_model$residuals))+ geom_histogram(colour = &#39;steelblue&#39;, size = 1) #Nice #We need to check for multicollinearity when we do multiple regression. A cool way to do this is with GGally, but we can also just calculate VIF and tolerance(1/vif) vif(training_model) ## adverts image airplay ## 1.063294 1.066953 1.053305 1/vif(training_model) ## adverts image airplay ## 0.9404740 0.9372484 0.9493924 #Tolerance is simply 1 divided by the VIF. Our vif should be &lt;10 and our tolerance # &gt;.2. library(GGally) ggpairs(training_model) #Don´t worry about this, this is just a cool diagnostic tool with loads of info in a single plot with a very simple call. #Checking error independence with Durbin Watson test durbinWatsonTest(training_model) ## lag Autocorrelation D-W Statistic p-value ## 1 0.00282891 1.887567 0.586 ## Alternative hypothesis: rho != 0 #Still looks good, but notice that the test statistic is less close to 2. #Small sample = bad Now, lets check the fit of out model summary.lm(training_model) ## ## Call: ## lm(formula = sales ~ adverts + image + airplay, data = z_training_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4006 -0.2931 -0.0123 0.3958 1.3417 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.215e-16 6.004e-02 0.000 1.000000 ## adverts 4.901e-01 6.223e-02 7.876 6.16e-12 *** ## image 2.161e-01 6.234e-02 3.466 0.000802 *** ## airplay 4.666e-01 6.194e-02 7.532 3.18e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5913 on 93 degrees of freedom ## Multiple R-squared: 0.6613, Adjusted R-squared: 0.6503 ## F-statistic: 60.51 on 3 and 93 DF, p-value: &lt; 2.2e-16 regplot(training_model, &#39;training model&#39;) #. All estimates are significant, indicating that all variables contribute to the number of sales, we have also doubled the R2, we can explain much more of the variation with this model. Note also how the visuals suck. its hard to visualize multiple regression. But lets try some ways of visualising. One way is to use the broom package like so: library(broom) #We need to make image into a factor training_data$image &lt;- as.factor(training_data$image) #We then fit the model that we want to plot plot1 &lt;- lm(sales ~ adverts + image, data = training_data) #Then, using the augment function we can fit a slope to each level of &quot;image&quot; ggplot(augment(plot1), aes(y = sales, x = adverts, col = image))+ geom_point()+ geom_smooth(method = &#39;lm&#39;, se = FALSE)+ theme_bw() #. This is a visualization of the sales across advert budget grouped by the band image, we can see that bands with a bad image (1), have fever sales, but they also have less budget. But we also see differences in slope depending on image. the band with the best image has a flatter slope then the bands with slightly worse image. Note that we already know this from our previous graphs, and they are way less messy, this is not a very good graph I think. Again, visualizing multiple regression is hard, since the dimensions become hard to grasp after two or more variables, given that we care about interactions. One other way is though plotting marginal predictions or coefficients. This illustrates the regression output quite nicely I think, this is sometimes referred to as a simple slopes graph. #Lets extract the coefficients from the model cf &lt;- training_model$coef intercept &lt;- cf[1] adverts &lt;- cf[2] adverts &lt;- round(adverts, 4) image &lt;- cf[3] image &lt;- round(image, 4) airplay &lt;- cf[4] airplay &lt;- round(airplay, 4) #Here they are paste(adverts, image, airplay) ## [1] &quot;0.4901 0.2161 0.4666&quot; #This is a grid of values so that the plot has structure - this will be invisible in the graph statgrid &lt;- data.frame( y = seq(from = 0, to = adverts, length = 100) ,x = seq(from = 0, to = 1, length = 100)) #Plotting the slopes ggplot(data = statgrid, aes(x = x, y = y))+ geom_jitter(alpha = 0)+ geom_abline(intercept = intercept, slope = adverts , col = &#39;blue&#39;, size = 1)+ geom_abline(intercept = intercept, slope = image , col = &#39;red&#39;, size = 1)+ geom_abline(intercept = intercept, slope = airplay , col = &#39;green&#39;, size = 1)+ ylab(label = &#39;standardised sale value&#39;)+ xlab(label = &#39;dependent variable slope&#39;)+ theme_bw()+ labs(title = &#39;Adverts = blue | Image = red | Airplay = green&#39;) #Our simple slopes plot This visualization shows the slopes from the model. That is, the increase in units of sales when the dependent variable increases with one independently. I do not see this type of visualisation often but i quite like it. I think its a neat way of visualising a regression output. It is quite abstract though, since it requires that you scale you DVs so they can fit the same plot. Lastly, lets see if we have any significant interaction effects, this is a little teaser for what we will do later on in the moderation chapter but I think it might be a good idea to see if we have an interaction here. Going by the graphs earlier we should not expect anything big. # A third model that uses the interaction between image and adverts m3 &lt;- lm(sales ~ adverts*image + airplay, data = z_training_data) summary.lm(m3) ## ## Call: ## lm(formula = sales ~ adverts * image + airplay, data = z_training_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4197 -0.2856 0.0063 0.4046 1.3321 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01604 0.06137 0.261 0.79438 ## adverts 0.49317 0.06214 7.937 4.87e-12 *** ## image 0.18976 0.06594 2.878 0.00498 ** ## airplay 0.45808 0.06220 7.365 7.36e-11 *** ## adverts:image -0.07987 0.06652 -1.201 0.23294 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5899 on 92 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.652 ## F-statistic: 45.96 on 4 and 92 DF, p-value: &lt; 2.2e-16 #The interaction is not significant. lets go all out and fit a full interaction model - our fourth model m4 &lt;- lm(sales ~ adverts*image*airplay, data = z_training_data) summary.lm(m4) ## ## Call: ## lm(formula = sales ~ adverts * image * airplay, data = z_training_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.39859 -0.27268 -0.00666 0.38561 1.33742 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.011502 0.068353 0.168 0.8668 ## adverts 0.505923 0.070951 7.131 2.54e-10 *** ## image 0.185909 0.081013 2.295 0.0241 * ## airplay 0.445473 0.068210 6.531 3.93e-09 *** ## adverts:image -0.102109 0.086212 -1.184 0.2394 ## adverts:airplay 0.002962 0.084294 0.035 0.9720 ## image:airplay 0.074178 0.100145 0.741 0.4608 ## adverts:image:airplay 0.029568 0.131518 0.225 0.8226 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5975 on 89 degrees of freedom ## Multiple R-squared: 0.6691, Adjusted R-squared: 0.643 ## F-statistic: 25.71 on 7 and 89 DF, p-value: &lt; 2.2e-16 So, simply going by this the interaction terms does not do much for us. lets just stick with our original training model and see how well it predicts our test data. Lets extract our predictions #We need to load in the caret package for this (more on this later in the cross validation chapter) library(caret) # Make predictions and compute the R2, RMSE and MAE predictions &lt;- training_model %&gt;% predict(test_data) data.frame( R2 = R2(predictions, test_data$sales), RMSE = RMSE(predictions, test_data$sales), MAE = MAE(predictions, test_data$sales)) ## R2 RMSE MAE ## 1 0.318754 242.4139 189.1693 #We have an RMSE of 242, if the training model has a lower one, it fits better #We need to make image numeric for prediction purposes training_data$image &lt;- as.numeric(training_data$image) predictions.2 &lt;- training_model %&gt;% predict(training_data) data.frame( R2 = R2(predictions.2, training_data$sales), RMSE = RMSE(predictions.2, training_data$sales), MAE = MAE(predictions.2, training_data$sales)) ## R2 RMSE MAE ## 1 0.390846 227.6169 167.243 #So, not very unexpectedly, we have better fit on this model. #Let´s store some predictions for plotting #Prediction on the training data pred_train &lt;- training_model %&gt;% predict(z_training_data) #Prediction on the test data pred_test &lt;- training_model %&gt;% predict(z_test_data) Lets plot the predictions #For reference, this is what a perfect prediction would look like ggplot(data = data.frame(data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =sales, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;sales&#39;)+ labs(title = &#39;perfect prediction&#39;)+ theme_bw() #Training data prediction ggplot(data = data.frame(z_training_data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =pred_train, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;model predicted sales&#39;)+ theme_bw() #. This shows how well our model performs, the red line indicating perfect predictions that is, if we could estimate the number of sales perfectly. Which we clearly cant, let´s look at our out of sample prediction. ggplot(data = data.frame(z_test_data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =pred_test, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;model predicted sales&#39;)+ theme_bw() #. Lets compare all the plots using the grid arrange function again perfect_pred &lt;- ggplot(data = data.frame(scale(data)), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =sales, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;sales&#39;)+ labs(title = &#39;perfect prediction&#39;)+ theme_bw() insample_pred &lt;- ggplot(data = data.frame(z_training_data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =pred_train, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;predicted sales&#39;)+ labs(title = &#39;insample prediction&#39;)+ theme_bw() outsample_pred &lt;- ggplot(data = data.frame(z_test_data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =pred_test, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;predicted sales&#39;)+ labs(title = &#39;out of sample prediction&#39;)+ theme_bw() grid.arrange(perfect_pred, insample_pred, outsample_pred) #This last piece of code compiles the graphs And there we have it, this model seems pretty good. Nothing amazing, but pretty good. In reality out of sample predictions are usually worse, but since we have such a simple model with reasonable dependent variables, this is not that unexpected. We certainly dont have issues with over-fitting our model, which can cause it to make poor out of sample predictions. In this chapter we have gone through a little bit of everything, we have some prediction some moderation some cross validation and of course the simple regression analysis. In future chapters we will take a closer look at all of these methods, if you want to have a more real example of regression check out the last chapter where we use real messy data for various analyses. 9.2 The general linear model (ANOVA) 9.3 The generalized linear model(logit) 9.4 Cross validation 9.5 LASSO (least absolute shrinkage and selection operator) 9.6 Ridge regression 9.7 Fully Bayesian regression 9.8 Poststratification "],["path-analysis.html", "10 Path analysis 10.1 Streiner(2005) 10.2 workshop analysis 10.3 Multiple regression(again) 10.4 Mediation analysis", " 10 Path analysis 10.1 Streiner(2005) Path analysis is not really a type of analysis like OLS regression or logistic regression but rather a mode for communicating and fitting more complex models. The path in path analysis refers to the specification of relationships between variables. What the path describe can vary. For example, a regression analysis where Y is regressed on X we have a path between X and Y that describes a causal effect of X on Y. Note that it is us as researchers that make the call that the relationship is causal, just because X and Y correlate does not mean that either variable causes the other. If you recall the lavaan notations from way back in chapter 4 you should recognise the difference between fitting models in a path context versus a regression context. To monkey what all the smart people say, OLS/ML regression is only a special case of a path analysis/SEM. Hopefully the distinction between path analysis and regression will become a bit fuzzy by the end of this chapter, they are very much the same. One might intuit them as slightly different tools for very similar jobs. In my opinion, the strength of path analysis lies in the visualisation opportunities combined with the ability to fit complex models, restrict those models and lastly compare the fit of those models. The little less cool thing with complex path analysis is that it is, wellcomplex. Another weakness of path analysis compared to normal regression is that it is very easy to fit nested regression models and there is a very natural progression of model building in the multiple regression context - as we saw in the previous chapter. Before getting in to live datasets, we can simulate some data. For this lecture I/we(class of 2021) were assigned an article by Streiner, it is a pretty nice article IMO so even if you havent read it I can recommend it. In the article Streiner analyses data from an unknown disorder called photonumerophobia, he describes it as the fear, that our fears of numbers will come to light. He then defines three predictors for this disorder, namely: HSM &lt;- high school math grade ANX &lt;- overall anxiety TAX &lt;- the difference between predicted tax and actual tax (weird I know, but lets run with it). Note that Streiner does not supply us with any data, he does however give as ample descriptives, enough to simulate the data quite closely, so let´s do that. All numbers are taken from table 1 in Streiner (2005): https://journals.sagepub.com/doi/pdf/10.1177/070674370505000207 #This is a perfect time for some faux. A nice little function is the cormat_from_triangle calls. It is quite simple, if you write in the top right triangle of correlations the function fills out the missing bits in the matrix. Now you dont have to write so much - thanks computer. library(faux) cors &lt;- cormat_from_triangle(c(.509 , -.366 ,.346, -.264 ,.338, .260)) #Now that we have the relationship between the data we can define the rest of the variables with the other descriptives given. set.seed(4543) #setting seed is always good data &lt;- rnorm_multi( n = 200 #let&#39;s take 200 observations , vars = 4 , mu = c(26.79, 20.33, 74.69, 1983.23) , sd = c(7.33 , 5.17 , 5.37 , 525.49) , r = cors #this is our previously defined cor matrix , varnames = c(&#39;pnp&#39;, &#39;anx&#39;, &#39;hsm&#39;, &#39;tax&#39;) , empirical = T)#we want to perfectly reproduce the data #Let´s look at the correlation matrix for the data we just generated cor(data) ## pnp anx hsm tax ## pnp 1.000 0.509 -0.366 0.346 ## anx 0.509 1.000 -0.264 0.338 ## hsm -0.366 -0.264 1.000 0.260 ## tax 0.346 0.338 0.260 1.000 #It´s a thing of beauty is it not? What about the descriptives? pastecs::stat.desc(data, desc = T ,basic = F) ## pnp anx hsm tax ## median 26.4386929 20.0561011 74.67091194 1.954408e+03 ## mean 26.7900000 20.3300000 74.69000000 1.983230e+03 ## SE.mean 0.5183093 0.3655742 0.37971634 3.715775e+01 ## CI.mean.0.95 1.0220834 0.7208964 0.74878412 7.327348e+01 ## var 53.7289000 26.7289000 28.83690000 2.761397e+05 ## std.dev 7.3300000 5.1700000 5.37000000 5.254900e+02 ## coef.var 0.2736096 0.2543040 0.07189717 2.649667e-01 #Lovely. Now we have data that very much resembles that from the article we can fit the path models he describes. Let´s start with table 4. pnp is regressed on all variables and the predictors are allowed to correlate. We will use the package lavaan for this so if you need a refresher go back to chapter 4 and check out the links provided there. #Loading lavaan and semPlot library(lavaan) library(semPlot) #Specifying our model with the lavaan syntax model.1 &lt;- (&#39; #regressions pnp ~ tax + hsm + anx #correlations anx ~~ hsm anx ~~ tax tax ~~ hsm &#39;) #Fitting our model using the sem function fit.1 &lt;- sem(model = model.1, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #The warning here is due to the fact that the tax variable is much larger than the others. Let´s plot the model. semPaths( object = fit.1 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , curvePivot = TRUE , edge.label.cex = 1.5 , rotation = 2 , sizeMan = 10) #And there it is, pretty as a picture. Now, Streiner actually specifies two other models, those seen in figure 5. Lets create and plot those two. After that we can check how they fit. #lets start with model a model.a &lt;- (&#39; #regressions pnp ~ tax tax ~ hsm hsm ~ anx &#39;) #Fitting the model fit.a &lt;- sem(model = model.a, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #Ploting the model semPaths( object = fit.a , what = &#39;est, std&#39; , layout = &#39;spring&#39; , style = &#39;lisrel&#39; , curvePivot = T , edge.label.cex = 1.5 , rotation = 1 , residuals = T , sizeMan = 10) #The control you have over the layout of these plots are vast, but the require tinkering that is not often worth it. This looks good enough. Let´s do the next model. model.b &lt;- (&#39; #regressions pnp ~ tax + hsm tax ~ anx hsm ~ anx &#39;) #Fitting the model fit.b &lt;- sem(model = model.b, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #Ploting the model semPaths( object = fit.b , what = &#39;est, std&#39; , layout = &#39;tree2&#39; , style = &#39;lisrel&#39; , curvePivot = T , edge.label.cex = 1.5 , rotation = 2 , residuals = T , sizeMan = 10) Cool, this is the exact models he fits. Now, are these models any good? we can explore that question by summarising the fits (fit.1, fit.a and fit.b), but first, how many parameters can we estimate? This is one of the trickier parts of path analysis and SEM. We cannot simply think about the number of variables we have; we must think about our data in terms of a variance/co-variance matrix. For each variable we have one variance, in this case 4, each variable pair has a covariation, thus we have ([k^2  k] / 2) covariances where k is the number of variables we have. Let´s calculate how many pieces of information we have k &lt;- 4 co &lt;- ((k^2-k)/2) #We have 4 variances and 6 covariances, this totals out at 10. This means that we can make a total 10 parameter estimates before we exhaust our degrees of freedom. Read the article more closely for a better understanding of degrees of freedom, they are fascinating. So, how many parameters are we estimating? in the first model(fit.1) we are actually estimating 10 parameters (three regressions, tree covariations, and 4 variances). This means that we have 0 degrees of freedom and a perfect fit. In second model(fit.a) we estimate 6 parameters(three regressions and tree variances) and in the last model(fit.b) we estimate 7 parameters(4 regressions and 3 variances). This can be tricky to wrap your mind around in the beginning but once you play around with it a bit it becomes more straight forward. Since we have two overidentified models we can compare the fit of them(the identified model fits perfectly), we can do this with the anova function. anova(fit.a, fit.b) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.b 2 5565.9 5589.0 52.613 ## fit.a 3 5632.6 5652.4 121.295 68.681 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #note that this if not an F test but a chi-squared test(LRT) The significance of the test indicates that model.b fits significantly better than model.a. However, the information difference in terms of AIC and BIC is very small. But this is an issue that doesnt really apply itself well to entirely imaginary data. We don´t really have a good frame of reference in terms of theory. 10.2 workshop analysis Now that we have gotten familiar with path analysis, we can try out another example. The data is still very much simulated, but this time we have to deal with some actual data points, not just generation by ourselves. The following analysis is based of the workshop on path analysis at GU. Hopefully i can take the data from the workshop without any issues. Let´s load the data (it should be available on the repository, hopefully.) library(haven) data &lt;- read_sav(&quot;data path analysis spring 22.sav&quot;) #Note. this is my path to the data and to access the file you need to import it yourself These are the main packages we will be using. We have the usual suspects + MVN for analysis of multivariate normality. library(tidyverse) library(lavaan) library(semPlot) library(MVN) Let´s take a peek at what we are working with. summary(data) ## stress satisfaction turnover_intent demands support ## Min. :0.600 Min. :0.000 Min. :0.000 Min. :1.000 Min. :1.000 ## 1st Qu.:1.500 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:2.000 1st Qu.:2.333 ## Median :2.000 Median :2.000 Median :1.500 Median :2.000 Median :2.667 ## Mean :1.993 Mean :2.578 Mean :1.896 Mean :2.363 Mean :2.732 ## 3rd Qu.:2.333 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:3.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 All variables are numeric and seems to range between 0 and 5. Note that some variables are ordinal, that is, they are whole numbers that are not continuous, that is, the distance between 1 and 2 cannot be assumed to be the same as between 3 and 4. If we want to assume this, the data needs to normally distributed. So let´s take a look at the distributions. Note however that this is NOT the normality assumption of the linear regression but rather an assumption of using ordinal data as if they were continuous. ggplot(data, aes(stress))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #It has a normal look, and it seems to be continuous measure, this is good. ggplot(data, aes(satisfaction))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Again, quite normal looking - this is ordinal but could be treated as if continuous with some mental gymnastics ggplot(data, aes(turnover_intent))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Interesting, perhaps its measured in half steps? but that begs the question why none of the higher valus such as 4.5 exists, it seems to stop after 2. Strange. ggplot(data, aes(demands))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Like satisfaction, looks pretty good. ggplot(data, aes(support))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Now this is a strange looking distribution...it looks to be a continuous measure and the density of the curve should be quite normal. #However, we need to check the normality of our predicted variables when we have them. Before that we can check the normality tests. One common measure is mardia, so let´s use that. We can also look at some of the qq-plots and the outlier measures. Let´s save the scores in a list called normality_diagnostics normality_diagnostics &lt;- mvn(data , mvnTest = &#39;mardia&#39; #takes maridas test , multivariatePlot = &#39;qq&#39; #normal chi2 qq-plot , univariateTest = &#39;AD&#39; #anderson-darling test for univariate normality , showOutliers = TRUE , showNewData = TRUE #stores a tibble without the outliers , multivariateOutlierMethod = &#39;adj&#39;) #takes the adjusted mahalanobis distance #Checking the normality descriptives normality_diagnostics$Descriptives ## n Mean Std.Dev Median Min Max 25th 75th Skew ## stress 322 1.992847 0.7447653 2.000000 0.6 5 1.500000 2.3325 1.5307216 ## satisfaction 322 2.577640 1.0715218 2.000000 0.0 5 2.000000 3.0000 0.6089998 ## turnover_intent 322 1.895963 1.2288900 1.500000 0.0 5 1.000000 3.0000 0.7192168 ## demands 322 2.363354 0.9145545 2.000000 1.0 5 2.000000 3.0000 0.4177021 ## support 322 2.731884 0.7106893 2.666667 1.0 5 2.333333 3.0000 0.2551844 ## Kurtosis ## stress 4.593582827 ## satisfaction 0.130539733 ## turnover_intent -0.169774465 ## demands -0.002784417 ## support 0.298869065 #Checking univariate normality normality_diagnostics$univariateNormality ## Test Variable Statistic p value Normality ## 1 Anderson-Darling stress 8.7723 &lt;0.001 NO ## 2 Anderson-Darling satisfaction 17.5045 &lt;0.001 NO ## 3 Anderson-Darling turnover_intent 9.3350 &lt;0.001 NO ## 4 Anderson-Darling demands 16.5444 &lt;0.001 NO ## 5 Anderson-Darling support 3.2055 &lt;0.001 NO #Checking multivariate normality normality_diagnostics$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 460.776024749078 1.05636187815355e-75 NO ## 2 Mardia Kurtosis 8.96312473441598 0 NO ## 3 MVN &lt;NA&gt; &lt;NA&gt; NO So, nothing is normal and we have quite a few outliers according to Mahalanobis. If we want an outlier free data set the mvn function saves it for us. Let´s try to extract the clean data from the normality diagnostics. clean_data &lt;- normality_diagnostics$newData Here we have the data without the 27 outliers indicated by the normality diagnostics. I´m not a fan of this type of diagnostics, but we can use the raw data as a type of sensitivity analysis later on, for now lets move on using the clean data. Since we don´t really know what we are doing it´s always good to look at the correlations to inform us about what connections lie within our sample. So let´s do that and then try to find a good model for our data. #This gives a normal correlation matrix cor(clean_data) ## stress satisfaction turnover_intent demands support ## stress 1.00000000 -0.4892677 0.22641261 -0.04095583 -0.3617246 ## satisfaction -0.48926773 1.0000000 -0.59110506 0.12206989 0.3145087 ## turnover_intent 0.22641261 -0.5911051 1.00000000 0.03633829 -0.2111387 ## demands -0.04095583 0.1220699 0.03633829 1.00000000 0.1464965 ## support -0.36172462 0.3145087 -0.21113872 0.14649650 1.0000000 #we can however plot the data out using the corplot function from the psych package without loading it by using the :: command like so: psych::corPlot(cor(clean_data)) #Note. the omitted variable in the bottom is turnover_intent Stress, support and satisfaction seems to be correlated just fine. Demands seem to be a quite superfluous variable having low correlations to all other variables. So let´s build a model where stress and support are allowed to correlate and that both have a causal impact on the satisfaction you have in the workplace. Let´s also say that support and satisfaction have a causal impact on your turnover intent. Let´s specify this model using the lavaan syntax. model.1 &lt;- (&#39; #regressions satisfaction ~ stress + support turnover_intent ~ satisfaction turnover_intent ~ support #covariates stress ~~ support &#39;) Now that we have our model - specified in text - we can fit that model using lavaans sem function just like we did before. fit.1 &lt;- sem( model = model.1 #the model we want to fit , data = clean_data #the data we want to fit the model on , estimator = &#39;MLM&#39; #the estimator we use (robust maximum likelihood) ) Now that we have our fit, we can summarise the results summary(fit.1 #the fit we want to summarize , standardize = TRUE #we want the standardised estimates , fit.measures = TRUE #we also want the fit measures(CFI, TLI) , rsquare = TRUE #if we want to r2 for the model ) ## lavaan 0.6-9 ended normally after 18 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 295 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 3.014 2.867 ## Degrees of freedom 1 1 ## P-value (Chi-square) 0.083 0.090 ## Scaling correction factor 1.051 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 260.797 208.698 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.250 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.992 0.991 ## Tucker-Lewis Index (TLI) 0.953 0.945 ## ## Robust Comparative Fit Index (CFI) 0.992 ## Robust Tucker-Lewis Index (TLI) 0.954 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1420.264 -1420.264 ## Loglikelihood unrestricted model (H1) -1418.757 -1418.757 ## ## Akaike (AIC) 2858.528 2858.528 ## Bayesian (BIC) 2891.711 2891.711 ## Sample-size adjusted Bayesian (BIC) 2863.169 2863.169 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.083 0.080 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.197 0.191 ## P-value RMSEA &lt;= 0.05 0.195 0.207 ## ## Robust RMSEA 0.082 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.199 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.022 0.022 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## stress -0.633 0.107 -5.902 0.000 -0.633 -0.432 ## support 0.252 0.087 2.909 0.004 0.252 0.158 ## turnover_intent ~ ## satisfaction -0.672 0.051 -13.158 0.000 -0.672 -0.582 ## support -0.051 0.084 -0.610 0.542 -0.051 -0.028 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~~ ## support -0.178 0.044 -4.093 0.000 -0.178 -0.362 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .satisfaction 0.850 0.066 12.878 0.000 0.850 0.739 ## .turnover_intnt 0.994 0.074 13.440 0.000 0.994 0.650 ## stress 0.536 0.081 6.649 0.000 0.536 1.000 ## support 0.454 0.043 10.580 0.000 0.454 1.000 ## ## R-Square: ## Estimate ## satisfaction 0.261 ## turnover_intnt 0.350 So our model fits remarkably well. Not even the chi2 is significant. Also, not how much better our model is than the baseline model. One bad thing is the fact that we are not very courageous, we only have one degree of freedom. Let´s plot this model out using semPlot and then try improving our fit even more. #This is a more detailed notation of the semPlot alternatives semPaths( object = fit.1 #the fit we want to plot(our model) , what = &#39;est, std&#39; #what we want to plot(the z-coefficients) , style = &#39;lisrel&#39; #the style of the plot , curvePivot = TRUE #manipulates how the lines look , esize = 4 #the size of our lines , nCharNodes = 0 #takes all the characters in our manifest vars , edge.label.cex = 1.5#how big the estimates are(indicated in &quot;what =&quot;) , residuals = FALSE #removes the residual loops , sizeMan = 15 #indicates the width of manifest variables , rotation = 2 #how we want to rotate the plot(wich direction) , theme = &#39;Borkulo&#39;) #the theme we want to use, i like this one #Looks pretty good right? but notice the very weak trace between support and turnover intent and satisfaction! Let´s interpret this. We can see that the correlation between support and stress is -.36 (as we saw in the correlation plot). We can also see that for every unit increase in stress satisfaction decreased with .43 standard deviations. We also see that for every unit increase in satisfaction we see a decrease in turnover_intent at .58 standard deviations. All these estimates, together with the unstandardized counterparts are available in the summary above. In the summary we also see an r2 of .26 in satisfaction and .35 in turnover intent, meaning that we can explain roughly 26% of the variation in satisfaction through stress and support and .35% of the variation in turnover intent through support and satisfaction. Pretty neat right? So where do we go from here? we have a model and its fits the data very well. We want to be brave scientists though, so we should refine our model and make some bolder claims. For example, let´s make the claim that support does NOT influence turnover intent, support does not even influence satisfaction. No, support actually has a one directional relationship with stress, that is the degree of support we have impacts our degree of stress, but stress does not influence how much support we have. Let´s specify this model and fit it. model.2 &lt;- (&#39; #regressions stress ~ support satisfaction ~ stress turnover_intent ~ satisfaction &#39;) fit.2 &lt;- sem( model = model.2 , data = clean_data , estimator = &#39;MLM&#39;) summary(fit.2 , standardize = TRUE , fit.measures = TRUE , rsquare = TRUE ) ## lavaan 0.6-9 ended normally after 14 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 295 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 11.898 12.009 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.008 0.007 ## Scaling correction factor 0.991 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 260.797 241.478 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.080 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.965 0.962 ## Tucker-Lewis Index (TLI) 0.930 0.923 ## ## Robust Comparative Fit Index (CFI) 0.965 ## Robust Tucker-Lewis Index (TLI) 0.930 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1122.743 -1122.743 ## Loglikelihood unrestricted model (H1) -1116.794 -1116.794 ## ## Akaike (AIC) 2257.486 2257.486 ## Bayesian (BIC) 2279.608 2279.608 ## Sample-size adjusted Bayesian (BIC) 2260.580 2260.580 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.100 0.101 ## 90 Percent confidence interval - lower 0.045 0.046 ## 90 Percent confidence interval - upper 0.163 0.164 ## P-value RMSEA &lt;= 0.05 0.064 0.063 ## ## Robust RMSEA 0.100 ## 90 Percent confidence interval - lower 0.046 ## 90 Percent confidence interval - upper 0.163 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.058 0.058 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~ ## support -0.393 0.083 -4.756 0.000 -0.393 -0.362 ## satisfaction ~ ## stress -0.717 0.104 -6.920 0.000 -0.717 -0.489 ## turnover_intent ~ ## satisfaction -0.682 0.049 -13.783 0.000 -0.682 -0.591 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .stress 0.466 0.057 8.237 0.000 0.466 0.869 ## .satisfaction 0.875 0.067 12.995 0.000 0.875 0.761 ## .turnover_intnt 0.996 0.074 13.531 0.000 0.996 0.651 ## ## R-Square: ## Estimate ## stress 0.131 ## satisfaction 0.239 ## turnover_intnt 0.349 By looking at the fit measures we can see that this model fits very well. But notice that we now have 3 degrees of freedom, that is, by further constraining the model (not allowing correlations between stress and support and removing the regression between support and turnover intent) we have expressed a more theoretically parsimonious model - very cool. Let´s make the path diagram and interpret the coefficients. semPaths( object = fit.2 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , curvePivot = TRUE , esize = 10 , asize = 5 #arrowhead size , sizeMan2 = 3 #height of the manifest variables , nCharNodes = 0 , edge.label.cex = 1.5 , residuals = FALSE , sizeMan = 15 , rotation = 2 , theme = &#39;Borkulo&#39;) #Note. The new comments are just arguments we didn´t use in the previous plot Cool, notice how we rephrased the connection between support and stress to a one-sided arrow. This can now be interpreted as a regression weight, that is, for every unit increase in support we se a .36 unit decrease in stress. We also see a bigger coefficient for the relationship between stress and satisfaction. This is likely due to the fact that we presume that support does not impact satisfaction but influences stress and thus, some of the variation we can explain in stress by support also influences satisfaction - i.e., stress mediates the relationship between support and satisfaction. The relationship between satisfaction and turnover intent is also different, but only by .01, that is, super small and an unimportant difference. Now that we have two overidentified models we can compare the fit of them. This is surprisingly easy; we only use the anova function and our to model fits. Note that this is not a GLM anova but an LRT test, that is, a likelihood ratio test. anova(fit.1, fit.2) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.1 1 2858.5 2891.7 3.0141 ## fit.2 3 2257.5 2279.6 11.8977 9.2498 2 0.009805 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So what does this tell us? It tells us that the baseline model(fit.1) fits the data significantly better then the other model(fit.2). However, one should not that the AIC and BIC is lower for the fit.2 model, low scores on AIC and BIC is indicative of good fit, and models with low values are preferable to those with higher BIC and AIC. I do not feel particularly strongly about any of these models, but if I had to choose one I would choose the second one since I like parsimony, but the first model technically fits better (though its almost identified so do with that what you will). For more on the LRT you can see: http://econ.upf.edu/~satorra/dades/BryantSatorraPaperInPressSEM.pdf One last thing we can do before we move on to other things is the check the modification indices. This is one cool thing that you can do with a path analysis that normal regression can´t really do. We will not use them for anything, but it is interesting to look at them. modificationindices(fit.2) %&gt;% arrange(-mi) %&gt;% head(20) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 1 support ~ satisfaction 8.440 0.131 0.131 0.208 0.208 ## 2 satisfaction ~ support 8.440 0.252 0.252 0.158 0.235 ## 3 stress ~~ satisfaction 8.440 0.299 0.299 0.468 0.468 ## 4 stress ~ satisfaction 8.440 0.341 0.341 0.500 0.500 ## 5 stress ~ turnover_intent 6.336 -0.109 -0.109 -0.184 -0.184 ## 6 support ~ turnover_intent 3.887 -0.067 -0.067 -0.124 -0.124 ## 7 stress ~~ turnover_intent 3.408 -0.082 -0.082 -0.121 -0.121 ## 8 satisfaction ~ turnover_intent 2.351 -0.171 -0.171 -0.197 -0.197 ## 9 turnover_intent ~ stress 2.351 -0.139 -0.139 -0.083 -0.083 ## 10 satisfaction ~~ turnover_intent 2.351 -0.170 -0.170 -0.182 -0.182 ## 11 turnover_intent ~ support 0.298 -0.048 -0.048 -0.026 -0.039 #Note. the %&gt;% is a pipe function, they make code simpler to write so look into them! See the provided programming resources in the beginning of the book. The modification indices (mi column) shows how much the chi2 of the model would drop if the parameter was included. Thus, indices over 3.84 will significantly increase the fit of the model. We have a few modifications we can make to our model, some of which improves the fit quite a bit. Lastly, let´s do a sensitivity check by running the second model through the raw data. That is, the data with the outliers included. sensitivity.fit &lt;- sem( model = model.2 , data = data , estimator = &#39;MLM&#39;) summary(sensitivity.fit, standardize = TRUE, fit.measures = TRUE , rsquare = TRUE) ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 322 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 10.159 10.615 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.017 0.014 ## Scaling correction factor 0.957 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 267.940 254.344 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.053 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.973 0.969 ## Tucker-Lewis Index (TLI) 0.945 0.939 ## ## Robust Comparative Fit Index (CFI) 0.972 ## Robust Tucker-Lewis Index (TLI) 0.944 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1234.025 -1234.025 ## Loglikelihood unrestricted model (H1) -1228.945 -1228.945 ## ## Akaike (AIC) 2480.049 2480.049 ## Bayesian (BIC) 2502.697 2502.697 ## Sample-size adjusted Bayesian (BIC) 2483.665 2483.665 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.086 0.089 ## 90 Percent confidence interval - lower 0.032 0.034 ## 90 Percent confidence interval - upper 0.147 0.151 ## P-value RMSEA &lt;= 0.05 0.119 0.108 ## ## Robust RMSEA 0.087 ## 90 Percent confidence interval - lower 0.034 ## 90 Percent confidence interval - upper 0.146 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.054 0.054 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~ ## support -0.346 0.073 -4.730 0.000 -0.346 -0.330 ## satisfaction ~ ## stress -0.694 0.099 -7.007 0.000 -0.694 -0.482 ## turnover_intent ~ ## satisfaction -0.672 0.048 -14.120 0.000 -0.672 -0.586 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .stress 0.493 0.060 8.155 0.000 0.493 0.891 ## .satisfaction 0.878 0.065 13.536 0.000 0.878 0.767 ## .turnover_intnt 0.989 0.074 13.405 0.000 0.989 0.657 ## ## R-Square: ## Estimate ## stress 0.109 ## satisfaction 0.233 ## turnover_intnt 0.343 anova(sensitivity.fit, fit.2) ## Warning in lavTestLRT(object = object, ..., model.names = NAMES): lavaan WARNING: some ## models have the same degrees of freedom ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## sensitivity.fit 3 2480.1 2502.7 10.159 ## fit.2 3 2257.5 2279.6 11.898 1.7384 0 #Note. We are comparing two models with the same degrees of freedom, this is not very appropriate since they are not nested (and on different data) but its nice to see the measures anyways. So, the model based on the raw data is not that different from we get when we have cleaned the data. This becomes even more apparent if we were to check the parameter estimates for the two models like so: parameterestimates(fit.2) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 stress ~ support -0.393 0.083 -4.756 0 -0.555 -0.231 ## 2 satisfaction ~ stress -0.717 0.104 -6.920 0 -0.920 -0.514 ## 3 turnover_intent ~ satisfaction -0.682 0.049 -13.783 0 -0.779 -0.585 ## 4 stress ~~ stress 0.466 0.057 8.237 0 0.355 0.577 ## 5 satisfaction ~~ satisfaction 0.875 0.067 12.995 0 0.743 1.007 ## 6 turnover_intent ~~ turnover_intent 0.996 0.074 13.531 0 0.851 1.140 ## 7 support ~~ support 0.454 0.000 NA NA 0.454 0.454 parameterestimates(sensitivity.fit) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 stress ~ support -0.346 0.073 -4.730 0 -0.489 -0.202 ## 2 satisfaction ~ stress -0.694 0.099 -7.007 0 -0.888 -0.500 ## 3 turnover_intent ~ satisfaction -0.672 0.048 -14.120 0 -0.765 -0.579 ## 4 stress ~~ stress 0.493 0.060 8.155 0 0.374 0.611 ## 5 satisfaction ~~ satisfaction 0.878 0.065 13.536 0 0.751 1.005 ## 6 turnover_intent ~~ turnover_intent 0.989 0.074 13.405 0 0.844 1.133 ## 7 support ~~ support 0.504 0.000 NA NA 0.504 0.504 They are pretty much the same parameter estimates, certainly not significantly different. So, what more can we do for this fun little analysis? we can do some bootstrapping. Lavaan has a really nice bootstrapping function that gives a lot of freedom to choose what to do with the estimates. Let´s bootstrap all estimates from the second model(fit.2) and take a percentile intervall for them. Note that this can take a few seconds. boot_sample &lt;- bootstrapLavaan( object = fit.2 #the fit we want to sample , R = 1000 #the number of iterations we want ) Notice that boot_sample is a matrix of many estimates. Let´s put the regression coefficients in a dataframe and make our intervals. boot_sample &lt;- data.frame(boot_sample) boot_coef &lt;- select(boot_sample #the data we want to take variables/cols from , c(1,2,3,)) #a list of the cols we want to take(1,2,3) quantile(boot_coef$stress.support #the estimate we want to take intervals on , probs = c(.025, .5, .975)) #the percentile points we want to see ## 2.5% 50% 97.5% ## -0.5489199 -0.3825642 -0.2386760 #We see that the regression weight ranges between -.55 and -.24 quantile(boot_coef$satisfaction.stress , probs = c(.025, .5, .975)) ## 2.5% 50% 97.5% ## -0.9758261 -0.7247085 -0.5454487 #We see that the regression weight ranges between -.98 and -.55 quantile(boot_coef$turnover_intent.satisfaction , probs = c(.025, .5, .975)) ## 2.5% 50% 97.5% ## -0.7719642 -0.6811282 -0.5928512 #We see that the regression weight ranges between -.77 and -.59 This is not super interesting, but it is pretty cool and also kind of illustrates what you can do with bootstrapping in a more manual way. There are of course much more you can do than to take percentile intervals of the regression weights but that lies beyond the specific realm of path analysis. 10.3 Multiple regression(again) By now you should feel good, both with lavaan semPlot and the general procedure for path analysis. So, lets take a step back and rework the good ´ol Field example for multiple regression, but with a path analysis. And don´t worry, this will be short and sweet :) 10.4 Mediation analysis In the Field book he gives an example of mediation with the data set from Lambert et al.,(2012). Since we are very comfortable with working with paths, mediation becomes a simple thing. In the regression example in the previous chapter we only estimated direct effects, that is, effects of a on c (or x on y if you prefer those letters). Mediation can estimate both direct and indirect effects. The indirect effect is the multiplication of path a and path b. Let´s load the data. While the process of a moderation analysis is quite complicated in terms of functions and packages - as we saw in the regression part of the book, with mediation we can rely on good ´ol lavaan. #Packages we need library(lavaan) library(semPlot) #Loading the data data &lt;- read_sav(&quot;Lambert et al. (2012).sav&quot;) names(data) &lt;- c(&#39;consumption&#39;, &#39;ln_porn&#39;, &#39;commit&#39;, &#39;infi&#39;, &#39;hook_ups&#39;) Let´s specify the model from the Field book. med_model.1 &lt;- (&#39; # Direct effect infi ~ c*consumption # Mediator commit ~ a*consumption infi ~ b*commit # Indirect effect (a*b) ab := a*b # Total effect total := c + (a*b) &#39;) #Note. The defined variables a, b and c are needed for the mediation analysis to work. These are the paths of our model, without them we cannot calculate our main and indirect effects! the lavaan symbol &#39;:=&#39; describes a new parameter dependent on our specified model. In this case it´s the regression weights (I believe) Let´s fit and summarise the result of this model. med_fit.1 &lt;- sem(med_model.1, data) #Summarising a lavaan can take a lot of commands, I will note what they do here as I have done before, but note that there are many other alternatives you can use! summary(med_fit.1 #the model we want to use(med_fit.1) , standardize = T #do we want a standardized estimate? T mean true/yes , fit.measures = T #do we want fit measures? , rsquare = T) #do we want r2 ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 35.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -501.418 ## Loglikelihood unrestricted model (H1) -501.418 ## ## Akaike (AIC) 1012.836 ## Bayesian (BIC) 1030.218 ## Sample-size adjusted Bayesian (BIC) 1014.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## consumptin (c) 0.107 0.038 2.797 0.005 0.107 0.171 ## commit ~ ## consumptin (a) -0.092 0.042 -2.175 0.030 -0.092 -0.139 ## infi ~ ## commit (b) -0.268 0.058 -4.612 0.000 -0.268 -0.282 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.429 0.039 10.932 0.000 0.429 0.878 ## .commit 0.531 0.049 10.932 0.000 0.531 0.981 ## ## R-Square: ## Estimate ## infi 0.122 ## commit 0.019 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.025 0.013 1.967 0.049 0.025 0.039 ## total 0.132 0.040 3.329 0.001 0.132 0.211 Nice, this is exactly what we want. Notice also that our model is saturated/identified, that is, it has 0 degrees of freedom. This means that we can´t really assess the fit of this model. Let´s make a quick interpretation. Infidelity increases with .107 units for every unit increase in consumption. Commitment decreases by .092 units for every increase in consumption and infidelity decreases with .268 for every unit increase in commitment. ´ab´ is our indirect effect, that is, .025. Which can be interpreted as a kind of r2. semPaths(med_fit.1 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , layout = &#39;tree&#39; , nCharNodes = 0 , edge.label.cex = 1.5 , sizeMan = 20 , residuals = F , rotation = 4 , theme = &#39;Borkulo&#39;) #Note that this is NOT the same output as in the field book. He uses the log transformed variable for consumption. So let´s recreate his model. med_model.2 &lt;- (&#39; # direct effect infi ~ c*ln_porn # mediator commit ~ a*ln_porn infi ~ b*commit # indirect effect (a*b) ab := a*b # total effect total := c + (a*b) &#39;) med_fit.2 &lt;- sem(med_model.2, data) summary(med_fit.2, standardize = T, fit.measures = T, rsquare = T) ## lavaan 0.6-9 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 33.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -502.418 ## Loglikelihood unrestricted model (H1) -502.418 ## ## Akaike (AIC) 1014.835 ## Bayesian (BIC) 1032.218 ## Sample-size adjusted Bayesian (BIC) 1016.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## ln_porn (c) 0.457 0.193 2.365 0.018 0.457 0.145 ## commit ~ ## ln_porn (a) -0.470 0.212 -2.215 0.027 -0.470 -0.142 ## infi ~ ## commit (b) -0.271 0.058 -4.642 0.000 -0.271 -0.285 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.432 0.040 10.932 0.000 0.432 0.886 ## .commit 0.531 0.049 10.932 0.000 0.531 0.980 ## ## R-Square: ## Estimate ## infi 0.114 ## commit 0.020 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.127 0.064 1.999 0.046 0.127 0.040 ## total 0.585 0.200 2.925 0.003 0.585 0.186 semPaths(med_fit.2 , what = &#39;est&#39; , style = &#39;lisrel&#39; , layout = &#39;tree&#39; , nCharNodes = 0 , edge.label.cex = 1.5 , sizeMan = 20 , residuals = F , theme = &#39;Borkulo&#39; , rotation = 4) #Very nice, we have recreated the findings from Field. These are the same models, but with the log transformation. The reason why I didn´t use that in the first model is because it´s harder to interpret. I have a hard time understanding what this indirect effect of .127 means since its a combination of the influence of a log variable, through a variable that is not logged. A smarter person than I will have to describe what this means. Field also uses bootstrapped standard errors; this can be specified in the fit portion of our workflow. Let´s refit our second model but with bootstrapped SEs. #it´s going to take a few seconds so do not fret if you dont get an output set.seed(234) #setting seed so that we can recreate the random values med_fit.2 &lt;- sem(med_model.2 #what model we want to fit , se = &#39;bootstrap&#39; #how do we want to estimate the std.err? , data) #indicates the data we want to fit the model on. summary(med_fit.2, standardize = T, fit.measures = T, rsquare = T) ## lavaan 0.6-9 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 33.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -502.418 ## Loglikelihood unrestricted model (H1) -502.418 ## ## Akaike (AIC) 1014.835 ## Bayesian (BIC) 1032.218 ## Sample-size adjusted Bayesian (BIC) 1016.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## ln_porn (c) 0.457 0.251 1.822 0.068 0.457 0.145 ## commit ~ ## ln_porn (a) -0.470 0.234 -2.004 0.045 -0.470 -0.142 ## infi ~ ## commit (b) -0.271 0.071 -3.824 0.000 -0.271 -0.285 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.432 0.055 7.908 0.000 0.432 0.886 ## .commit 0.531 0.050 10.539 0.000 0.531 0.980 ## ## R-Square: ## Estimate ## infi 0.114 ## commit 0.020 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.127 0.071 1.798 0.072 0.127 0.040 ## total 0.585 0.250 2.339 0.019 0.585 0.186 parameterestimates(med_fit.2) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 infi ~ ln_porn c 0.457 0.251 1.822 0.068 -0.032 0.960 ## 2 commit ~ ln_porn a -0.470 0.234 -2.004 0.045 -0.917 0.019 ## 3 infi ~ commit b -0.271 0.071 -3.824 0.000 -0.404 -0.124 ## 4 infi ~~ infi 0.432 0.055 7.908 0.000 0.314 0.531 ## 5 commit ~~ commit 0.531 0.050 10.539 0.000 0.434 0.630 ## 6 ln_porn ~~ ln_porn 0.049 0.000 NA NA 0.049 0.049 ## 7 ab := a*b ab 0.127 0.071 1.798 0.072 -0.005 0.281 ## 8 total := c+(a*b) total 0.585 0.250 2.339 0.019 0.089 1.096 Cool, now we have a SE estimate of .071 instead of .064. Note also that the ci for the ab effect, that is, the indirect effect, ranges from -.005 to .28, we cross zero and can therefore not be certain in the existence of the mediation. The only robust effect is that of commitment on infidelity. Let´s finish of by fitting a normal linear interaction model and compare the outcomes. lm_model &lt;- lm(infi ~ consumption*commit, data) summary.lm(lm_model) ## ## Call: ## lm(formula = infi ~ consumption * commit, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.94415 -0.36188 -0.13989 0.00893 2.00893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.53260 0.42566 3.601 0.000387 *** ## consumption -0.05829 0.19836 -0.294 0.769133 ## commit -0.33807 0.10103 -3.346 0.000954 *** ## consumption:commit 0.04142 0.04864 0.852 0.395292 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6591 on 235 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.1251, Adjusted R-squared: 0.1139 ## F-statistic: 11.2 on 3 and 235 DF, p-value: 6.744e-07 This output seems very nice to me. It very much confirms what our mediation analysis found. The only reliable effect is that of commitment. "],["exploratory-factor-analysis-efa.html", "11 Exploratory factor analysis (EFA)", " 11 Exploratory factor analysis (EFA) "],["confirmatory-factor-analysis.html", "12 Confirmatory factor analysis", " 12 Confirmatory factor analysis "],["multi-level-modelling.html", "13 Multi-level modelling", " 13 Multi-level modelling "],["structural-equation-modelling.html", "14 Structural equation modelling", " 14 Structural equation modelling "],["meta-analysis.html", "15 Meta-analysis", " 15 Meta-analysis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
[["index.html", "Not yet decided 1 Table of contents", " Not yet decided B. Alfons Edmar 2022-05-06 1 Table of contents Introduction Foreword Some useful resources What I expect you to know Descriptive statistics Basic null hypothesis testing Statistical phenomena (regression to the mean, CLT, LOLN) A brief introduction to R programming (with links) Bayesian and Frequentist statistics Likelihood functions Density distributes All hail the almighty Gauss Non-parametric statistics Asking all the question, making statements, and assuming The old guard Bootstrapping Correlation and Covariation in R Welcome to the matrix Regression analysis Univariate regression Multiple regression The general linear model (ANOVA) The generalized linear model(logit) Cross validation LASSO (least absolute shrinkage and selection operator) Ridge regression Fully Bayesian regression Poststratification Path analysis Multiple regression(again) Mediation analysis Exploratory factor analysis (EFA) Confirmatory factor analysis (CFA) Multi-level modelling (MLM) Structural equation modelling (SEM) Meta-analysis "],["introduction.html", "2 Introduction 2.1 Foreword 2.2 Some useful resources", " 2 Introduction 2.1 Foreword I thought It would be fun to create a book covering the things that I have learnt this past year and a half while creating a (hopefully) useful tool for future students. I also really enjoy opensource projects such as R and really think that it is a brilliant workspace. It is not only flexible but creates a very structured environment where all your decisions throughout the analysis is easily documented and shared. In short, I like R and RStudio. The problem is that R is hard, the learning curve is slow, and if you are a psychologist, the opportunities to learn programming are sparse unless you seek them out for yourself. With this book I hope to create an accessible guide to how you can use R for your studies and hopefully your future work. Now is a good time to point out that I am not a statistician, as of writing this I have not even graduated from the master´s program. Thus, this book should not be treated as an authoritative voice on how you should do statistics or interpret your results. The aim here is simple, it is to guide you through some of the statistical analyses that you will encounter and introduce the R programming language in the meantime. There will of course be some explanations of the tools that we use in terms of their statistical properties and interpretations of the results from our analyses, but I cannot stress enough that this is not a book about statistics, it´s a book about doing statistics in R. It´s designed to be brief and easily skimmed though in the sense that you wont have to read so much of my blathering to understand the interesting bits - the code. The general layout of the applied chapters will be analyses of the working examples in Discovering statistics by Andy Field (when applicable) and some workshop exercises. I will also end each applied chapter by finding some real data from an article and working through their analysis. In sum, this is a compilation of the topics covered in the master´s programme in psychological science at the university of Gothenburg in 2021/22 with some additional chapters by yours truly on some of the things that I think are useful and important. It is a supplementary text that simply translates the analyses described in the literature into R-code. It is for students who want to learn R but dont know where to start. It can be hard to learn things on your own without any real incentives, and I hope that this book can coax some people into R through it´s accessibility and relevance to what you will be doing in class anyways. Good luck and happy reading/coding. P.s, If you are a statistician, stop reading here, only pain will follow. 2.2 Some useful resources "],["welcome-to-r.html", "3 Welcome to R 3.1 The R environment 3.2 Basic programming 3.3 Organisation 3.4 Rmarkdowm", " 3 Welcome to R 3.1 The R environment 3.2 Basic programming 3.3 Organisation 3.4 Rmarkdowm "],["what-i-expect-you-to-know.html", "4 What I expect you to know 4.1 Descriptive statistics 4.2 Basic null hypothesis testing 4.3 Statistical phenomena", " 4 What I expect you to know 4.1 Descriptive statistics 4.2 Basic null hypothesis testing 4.3 Statistical phenomena "],["useful-packages-and-functions-that-helped-me-learn.html", "5 Useful packages and functions that helped me learn 5.1 Tidyverse 5.2 Distribution functions - rnorm, dbeta, rbinom etc 5.3 Faux 5.4 lm 5.5 Lavaan 5.6 semPlot 5.7 ?", " 5 Useful packages and functions that helped me learn This is a bit of a strange chapter, I am not entirely sure why I included it, but I think it might be a good idea to have a few words on the packages that we will be using. This Chapter could be returned two If you get stuck with some code and for some really strange reason can´t find a better place to search for answers than here. In sum, there are some useful packages and function in R, and using them might be beneficial for both learning and doing you statistics. 5.1 Tidyverse Like the name implies, tidyverse is an entire universe of connected packages designed to make your coding comfortable and aesthetically pleasing. I neither have the expertise nor the time to go through even the basics of all the tidyverse packages but we will be using them frequently, and when it happens there will be some notation. In the meantime, the first chapter of R for data science is an absolute gem if you are serious about learning R. Very nice introduction, assuming no previous coding experience (and it´s free) I cannot recommend it enough (hence the repeat) https://r4ds.had.co.nz/ 5.2 Distribution functions - rnorm, dbeta, rbinom etc These functions were probably what got me to actually think I understood some things about statistics, which is quite impressive for me. They are essentially functions that describe the shape of certain distributions and gives you the ability to generate random variables from those distributions. The possibilities of this is really endless but I will show some examples of how to generate data below. #Since the data is &quot;random&quot; it will be impossible to reproduce if you re-run the code. This is why we usually set a seed. This makes it so the random data can be reproduced. We do this through the set.seed function before we generate random numbers. Fun fact, the numbers are not truly random since they can be reproduced exactly, they are pseudo-random. #Random data following the normal distribution set.seed(5395) #setting the seed, this can by any combination of integers. norm_data &lt;- rnorm( n = 500 #the number of observations we want , mean = 4 #the mean of the numbers , sd = 1 #the standard deviation of the data ) #Prof of normality through histogram hist(norm_data) #Voila. #We can do the same with the t-distribution set.seed(453) t_data &lt;- rt( n = 50 #number of observations , df= 6 #degrees of freedom ) hist(t_data) #the dame things can be done to the F distribution using rf() #If we want binary outcomes we can use the binomial distribution with r(binom) #lets say we want to simulate 10 tosses of a fair coin for example. set.seed(5325) coin_data &lt;- rbinom( n = 10 #number of observations/tosses , size = 1 #number of trials per observation(one this time) , prob = .5 #the probability of a successful outcome (50/50) ) hist(coin_data) #Cool, If we say 1 is tails we have 6 tails and 4 heads. These might seem like very trivial things, but by manipulating the data and doing analyses on data sets where you actually KNOW what the true population parameters are, can be very useful and enlightening. Simulation is the best kind of preparation; it also highlights very well what you expect from the data. The next package I can recommend handles data generation quite smoothly. 5.3 Faux One of the most powerful properties of programming languages is their ability to generate pseudo random variables. We have seen that we can easily generate independent random numbers following some distribution, but what if we want to simulate relationships between variables? enter faux. There are probably many other packages doing the same things, but this is the one I use, and it has served me quite well.  let´s generate the data that we will work with through this chapter. library(&#39;faux&#39;) set.seed(324) #the seed returns! data &lt;- rnorm_multi(n = 500 #the number of observations we want , vars = 3 #number of variables we want , mu = c(3,5,2)#the three means for our variables. , sd = c(1.3, 2, .6) #the three SDs , r = c(1, .4, .7, #this is where it gets tricky. r is .4, 1, .4, #the correlation matrix. If we have .7, .4, 1) #many variables it can be tedious , varnames = c(&#39;y&#39;, &#39;x&#39;, &#39;z&#39;)) #the names of our variables #lets look at the correlations and the mean of our data cor(data) #correlates all variables in the data set with each other ## y x z ## y 1.0000000 0.3674651 0.6744010 ## x 0.3674651 1.0000000 0.4105545 ## z 0.6744010 0.4105545 1.0000000 summary(data) #summarises the content in the data ## y x z ## Min. :-0.3747 Min. :-0.8381 Min. :0.2554 ## 1st Qu.: 1.9188 1st Qu.: 3.6476 1st Qu.:1.5958 ## Median : 2.9566 Median : 4.9841 Median :2.0288 ## Mean : 2.9204 Mean : 5.0046 Mean :2.0239 ## 3rd Qu.: 3.7996 3rd Qu.: 6.3985 3rd Qu.:2.4164 ## Max. : 6.5359 Max. :12.3798 Max. :3.8473 #Notice how the values are similar, but not exactly the same, why is that? it is because of randomness. We can remedy this if you want to have full control over the parameter estimates in your data. We do this adding the empirical = true to the data generation formula like so: data.2 &lt;- rnorm_multi(n = 500 , vars = 3 , mu = c(3,5,2) , sd = c(1.3, 2, .6) , r = c(1, .4, .7, .4, 1, .4, .7, .4, 1) , varnames = c(&#39;y&#39;, &#39;x&#39;, &#39;z&#39;) , empirical = TRUE) #do we want empirical estimates? #This is now empirical data and not data sample from a population cor(data.2) ## y x z ## y 1.0 0.4 0.7 ## x 0.4 1.0 0.4 ## z 0.7 0.4 1.0 summary(data.2) ## y x z ## Min. :-0.8063 Min. :-0.8758 Min. :0.293 ## 1st Qu.: 2.1405 1st Qu.: 3.7060 1st Qu.:1.590 ## Median : 2.9691 Median : 4.9931 Median :2.015 ## Mean : 3.0000 Mean : 5.0000 Mean :2.000 ## 3rd Qu.: 3.9539 3rd Qu.: 6.2568 3rd Qu.:2.398 ## Max. : 6.5047 Max. :11.9103 Max. :3.605 #And there it is, identical to how we specified it in the rnorm_multi function. Now that we have some data, let´s try to fit it to a model 5.4 lm As we will a little bit more in-depth below, R generally uses strings from specifying models. That is, if we want to regress Y on x we specify that as Y~X. You will be doing a lot of that little squiggly(~), or, as some call it tilde(it´s proper name). Most if not all simple statistics can be done using the linear model function lm(). There are many functions for various types of t-test and chi2 and others but you will get really far with just the lm(), at the end of the day most tests all fall under the general linear model. Let´s fit a linear model with the data we just generated, we can predict y with x and z. fit.lm &lt;- lm( formula = y ~ x + z #the formula of the regression , data = data #the data we want to use ) #When we have fitted a model, we need to summarise the output. There is on specifically for linear models that we can use. summary.lm(object = fit.lm) ## ## Call: ## lm(formula = y ~ x + z, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7678 -0.5999 -0.0311 0.6150 2.9941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.25511 0.16116 -1.583 0.1141 ## x 0.07015 0.02318 3.027 0.0026 ** ## z 1.39556 0.07977 17.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9565 on 497 degrees of freedom ## Multiple R-squared: 0.4647, Adjusted R-squared: 0.4625 ## F-statistic: 215.7 on 2 and 497 DF, p-value: &lt; 2.2e-16 Cool, this is a run of the mill multiple regression. But remember that you cannot only fit our model, you must also summarise it, be that through summary.lm() or by any other means. We´ll get more in to lm() in the regression chapters. Let´s move on to a real big boy, lavaan. 5.5 Lavaan Lavaan, or, latent variable analysis, is an R package that we will use quite a bit. As the name implies it´s main function is to do latent variable analysis. If you do not know what latent variable analysis is, don´t worry, you soon will. The main thing that can be a bit tricky with lavaan is that it uses different kind of model specification than we usually do in simple analyses such as linear models. Contrary to those models, lavaan works in three steps. - first: you specify your model - second: you fit your model - third: you summarise the output of the model fit. If we compare this to the previous two step process of fitting an lm, it is the separation of model specification and model fitting that can take you for a loop. Lavaan models are specified as strings, that is, characters. These characters are then applied to a fitting function. To make things clearer, let´s play around with it a bit. We preciously specified the model of y ~ x + z. This is a character string that communicates that y is regressed on x + z. Lavaan works the same way, so if we wanted to fit a regression in lavaan we can use that exact string, the difference is that we specify the modal as a separate object from the fit. library(lavaan) #Step 1: specify the model model &lt;- (&#39;y ~ x + z&#39;) #Step 2: fit the specifid model with the sem function fit &lt;- sem(model = model, data = data) #Step 3: summarise the output summary(fit) ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 0.070 0.023 3.036 0.002 ## z 1.396 0.080 17.547 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.909 0.058 15.811 0.000 Now, if we look at the regression section in the bottom, we can see that the regression output is exactly the same as we would get if we did a normal lm regression. Let´s print out that regression we fitted earlier. summary.lm(fit.lm) ## ## Call: ## lm(formula = y ~ x + z, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7678 -0.5999 -0.0311 0.6150 2.9941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.25511 0.16116 -1.583 0.1141 ## x 0.07015 0.02318 3.027 0.0026 ** ## z 1.39556 0.07977 17.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9565 on 497 degrees of freedom ## Multiple R-squared: 0.4647, Adjusted R-squared: 0.4625 ## F-statistic: 215.7 on 2 and 497 DF, p-value: &lt; 2.2e-16 And there you go, it´s very much the same. Now, why would you want to do all this other stuff just to fit a regression? well, you wouldnt. But this is not the point of lavaan, lavaan can specify very complex models ranging from latent variable models to multilevel models, and all this is done though the mode character string, also knowns as the lavaan syntax. Below I will give some examples of what type of models you can fit with lavaan. I will plot them out using a package called semPlot - but we will get to that in due time. library(semPlot) #A simple correlation model cor_model &lt;- (&#39; #Correlations are described by &quot;~~&quot; y ~~ x&#39; ) semPaths(sem(cor_model)) #A simple regression model reg_model &lt;- (&#39; #Regressions are described by &quot;~&quot; y ~ x&#39;) semPaths(sem(reg_model), rotation = 2) #A simple mediation model med_model &lt;- (&#39; y ~ x + z z ~ x&#39; ) semPaths(sem(med_model)) #a simple latent variable model sem_model &lt;- (&#39; #Latent variable as describe by &quot;=~&quot; y =~ a + b + c + d x =~ c + d + e + f &#39;) semPaths(sem(sem_model)) This is just a little taste of what lavaan can do. If your data can handle it, you can make incredibly interesting models. We will use lavaan for path analysis, mediation analysis, SEM and multi-level SEM. If you want to go Bayesian, there is even a Bayesian alternative under the name blavaan. Again, this is barely scratching the surface, and if you want to get deeper insight into the workings of lavaan i can recommend going straight to the source: https://lavaan.ugent.be/tutorial/index.html 5.6 semPlot The semPlot package is used to visual/plot SEMs, CFAs and path analyses. We have already used semPlot to graph the sem models above. Now, graphing sems can be quite annoying in the beginning. The documentation is very very extensive, but not super easy to understand (IMO) and before you have a basic understanding of the semPaths function reading the documentation might feel overwhelming. I would recommend going to this very concise video by the author of the package for a nice little overview: https://www.youtube.com/watch?v=rUUzF1_yaXg&amp;ab_channel=SachaEpskamp 5.7 ? Last, but certainly not least, the question mark. This might be the most useful little trick R has to offer. All packages/functions have built in documentation that explains it´s uses and intricacies. Most even have examples of how to use the function. I cannot stress enough how useful this is, not only is it good for solving problems in your code quickly, but all this documentation is a wealth of knowledge. Though brief, most if not all documentation has useful references and information that can broaden your knowledge of what you are actually doing. So how does it work? you simply type ? before a function. Let´s say we want to know more about how to fit lavaan models with the sem function, then we simple write: ?sem #Maybe we have issues generating correct correlations and have to revisit how to use rnorm_multi ?rnorm_multi #Perhaps just the rnorm? ?rnorm Make use of this tool! it can help you a lot. Also, I don´t like leaving a mess. So now that we are done, we should clean up. The fastest way of removing things in R is with the rm() function. This removes singular things from the environment. If we want to remove the coin_data we can simply write: rm(coin_data) And it´s gone. Now you will be glad that you have all your things documented neatly, that way you can load and remove things at the press of a button without having to worry about losing things. But if we want to clean everything out (as I often want) we can write the line: rm(list = ls()) To be completely honest I not know exactly what this line means, but it does a great job cleaning. "],["bayesian-and-frequentists-statistics.html", "6 Bayesian and Frequentists statistics 6.1 Likelihood functions 6.2 Density distributes 6.3 All hail the almighty Gauss", " 6 Bayesian and Frequentists statistics 6.1 Likelihood functions 6.2 Density distributes 6.3 All hail the almighty Gauss "],["non-parametric-statistics.html", "7 Non-parametric statistics 7.1 Asking all the question, making statements, and assuming 7.2 The old guard 7.3 Bootstrapping", " 7 Non-parametric statistics 7.1 Asking all the question, making statements, and assuming 7.2 The old guard 7.3 Bootstrapping "],["correlation-and-covariation-in-r.html", "8 Correlation and Covariation in R 8.1 Welcome to the matrix", " 8 Correlation and Covariation in R 8.1 Welcome to the matrix "],["regression-analysis.html", "9 Regression analysis 9.1 Univariate regression 9.2 Multiple regression 9.3 The general linear model (ANOVA) 9.4 The generalized linear model(logit) 9.5 Cross validation 9.6 LASSO (least absolute shrinkage and selection operator) 9.7 Ridge regression 9.8 Fully Bayesian regression 9.9 Poststratification", " 9 Regression analysis 9.1 Univariate regression 9.2 Multiple regression 9.3 The general linear model (ANOVA) 9.4 The generalized linear model(logit) 9.5 Cross validation 9.6 LASSO (least absolute shrinkage and selection operator) 9.7 Ridge regression 9.8 Fully Bayesian regression 9.9 Poststratification "],["path-analysis.html", "10 Path analysis 10.1 Streiner(2005) 10.2 workshop analysis 10.3 Multiple regression(again) 10.4 Mediation analysis", " 10 Path analysis 10.1 Streiner(2005) Path analysis is not really a type of analysis like OLS regression or logistic regression but rather a mode for communicating and fitting more complex models. The path in path analysis refers to the specification of relationships between variables. What the path describe can vary. For example, a regression analysis where Y is regressed on X we have a path between X and Y that describes a causal effect of X on Y. Note that it is us as researchers that make the call that the relationship is causal, just because X and Y correlate does not mean that either variable causes the other. If you recall the lavaan notations from way back in chapter 4 you should recognise the difference between fitting models in a path context versus a regression context. To monkey what all the smart people say, OLS/ML regression is only a special case of a path analysis/SEM. Hopefully the distinction between path analysis and regression will become a bit fuzzy by the end of this chapter, they are very much the same. One might intuit them as slightly different tools for very similar jobs. In my opinion, the strength of path analysis lies in the visualisation opportunities combined with the ability to fit complex models, restrict those models and lastly compare the fit of those models. The little less cool thing with complex path analysis is that it is, wellcomplex. Another weakness of path analysis compared to normal regression is that it is very easy to fit nested regression models and there is a very natural progression of model building in the multiple regression context - as we saw in the previous chapter. Before getting in to live datasets, we can simulate some data. For this lecture I/we(class of 2021) were assigned an article by Streiner, it is a pretty nice article IMO so even if you havent read it I can recommend it. In the article Streiner analyses data from an unknown disorder called photonumerophobia, he describes it as the fear, that our fears of numbers will come to light. He then defines three predictors for this disorder, namely: HSM &lt;- high school math grade ANX &lt;- overall anxiety TAX &lt;- the difference between predicted tax and actual tax (weird I know, but lets run with it). Note that Streiner does not supply us with any data, he does however give as ample descriptives, enough to simulate the data quite closely, so let´s do that. All numbers are taken from table 1 in Streiner (2005): https://journals.sagepub.com/doi/pdf/10.1177/070674370505000207 #This is a perfect time for some faux. A nice little function is the cormat_from_triangle calls. It is quite simple, if you write in the top right triangle of correlations the function fills out the missing bits in the matrix. Now you dont have to write so much - thanks computer. library(faux) cors &lt;- cormat_from_triangle(c(.509 , -.366 ,.346, -.264 ,.338, .260)) #Now that we have the relationship between the data we can define the rest of the variables with the other descriptives given. set.seed(4543) #setting seed is always good data &lt;- rnorm_multi( n = 200 #let&#39;s take 200 observations , vars = 4 , mu = c(26.79, 20.33, 74.69, 1983.23) , sd = c(7.33 , 5.17 , 5.37 , 525.49) , r = cors #this is our previously defined cor matrix , varnames = c(&#39;pnp&#39;, &#39;anx&#39;, &#39;hsm&#39;, &#39;tax&#39;) , empirical = T)#we want to perfectly reproduce the data #Let´s look at the correlation matrix for the data we just generated cor(data) ## pnp anx hsm tax ## pnp 1.000 0.509 -0.366 0.346 ## anx 0.509 1.000 -0.264 0.338 ## hsm -0.366 -0.264 1.000 0.260 ## tax 0.346 0.338 0.260 1.000 #It´s a thing of beauty is it not? What about the descriptives? pastecs::stat.desc(data, desc = T ,basic = F) ## pnp anx hsm tax ## median 26.4386929 20.0561011 74.67091194 1.954408e+03 ## mean 26.7900000 20.3300000 74.69000000 1.983230e+03 ## SE.mean 0.5183093 0.3655742 0.37971634 3.715775e+01 ## CI.mean.0.95 1.0220834 0.7208964 0.74878412 7.327348e+01 ## var 53.7289000 26.7289000 28.83690000 2.761397e+05 ## std.dev 7.3300000 5.1700000 5.37000000 5.254900e+02 ## coef.var 0.2736096 0.2543040 0.07189717 2.649667e-01 #Lovely. Now we have data that very much resembles that from the article we can fit the path models he describes. Let´s start with table 4. pnp is regressed on all variables and the predictors are allowed to correlate. We will use the package lavaan for this so if you need a refresher go back to chapter 4 and check out the links provided there. #Loading lavaan and semPlot library(lavaan) library(semPlot) #Specifying our model with the lavaan syntax model.1 &lt;- (&#39; #regressions pnp ~ tax + hsm + anx #correlations anx ~~ hsm anx ~~ tax tax ~~ hsm &#39;) #Fitting our model using the sem function fit.1 &lt;- sem(model = model.1, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #The warning here is due to the fact that the tax variable is much larger than the others. Let´s plot the model. semPaths( object = fit.1 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , curvePivot = TRUE , edge.label.cex = 1.5 , rotation = 2 , sizeMan = 10) #And there it is, pretty as a picture. Now, Streiner actually specifies two other models, those seen in figure 5. Lets create and plot those two. After that we can check how they fit. #lets start with model a model.a &lt;- (&#39; #regressions pnp ~ tax tax ~ hsm hsm ~ anx &#39;) #Fitting the model fit.a &lt;- sem(model = model.a, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #Ploting the model semPaths( object = fit.a , what = &#39;est, std&#39; , layout = &#39;spring&#39; , style = &#39;lisrel&#39; , curvePivot = T , edge.label.cex = 1.5 , rotation = 1 , residuals = T , sizeMan = 10) #The control you have over the layout of these plots are vast, but the require tinkering that is not often worth it. This looks good enough. Let´s do the next model. model.b &lt;- (&#39; #regressions pnp ~ tax + hsm tax ~ anx hsm ~ anx &#39;) #Fitting the model fit.b &lt;- sem(model = model.b, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #Ploting the model semPaths( object = fit.b , what = &#39;est, std&#39; , layout = &#39;tree2&#39; , style = &#39;lisrel&#39; , curvePivot = T , edge.label.cex = 1.5 , rotation = 2 , residuals = T , sizeMan = 10) Cool, this is the exact models he fits. Now, are these models any good? we can explore that question by summarising the fits (fit.1, fit.a and fit.b), but first, how many parameters can we estimate? This is one of the trickier parts of path analysis and SEM. We cannot simply think about the number of variables we have; we must think about our data in terms of a variance/co-variance matrix. For each variable we have one variance, in this case 4, each variable pair has a covariation, thus we have ([k^2  k] / 2) covariances where k is the number of variables we have. Let´s calculate how many pieces of information we have k &lt;- 4 co &lt;- ((k^2-k)/2) #We have 4 variances and 6 covariances, this totals out at 10. This means that we can make a total 10 parameter estimates before we exhaust our degrees of freedom. Read the article more closely for a better understanding of degrees of freedom, they are fascinating. So, how many parameters are we estimating? in the first model(fit.1) we are actually estimating 10 parameters (three regressions, tree covariations, and 4 variances). This means that we have 0 degrees of freedom and a perfect fit. In second model(fit.a) we estimate 6 parameters(three regressions and tree variances) and in the last model(fit.b) we estimate 7 parameters(4 regressions and 3 variances). This can be tricky to wrap your mind around in the beginning but once you play around with it a bit it becomes more straight forward. Since we have two overidentified models we can compare the fit of them(the identified model fits perfectly), we can do this with the anova function. anova(fit.a, fit.b) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.b 2 5565.9 5589.0 52.613 ## fit.a 3 5632.6 5652.4 121.295 68.681 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #note that this if not an F test but a chi-squared test(LRT) The significance of the test indicates that model.b fits significantly better than model.a. However, the information difference in terms of AIC and BIC is very small. But this is an issue that doesnt really apply itself well to entirely imaginary data. We don´t really have a good frame of reference in terms of theory. 10.2 workshop analysis Now that we have gotten familiar with path analysis, we can try out another example. The data is still very much simulated, but this time we have to deal with some actual data points, not just generation by ourselves. The following analysis is based of the workshop on path analysis at GU. Hopefully i can take the data from the workshop without any issues. Let´s load the data (it should be available on the repository, hopefully.) library(haven) data &lt;- read_sav(&quot;data path analysis spring 22.sav&quot;) #Note. this is my path to the data and to access the file you need to import it yourself These are the main packages we will be using. We have the usual suspects + MVN for analysis of multivariate normality. library(tidyverse) ## -- Attaching packages ------------------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## -- Conflicts ---------------------------------------------------- tidyverse_conflicts() -- ## x purrr::%||%() masks faux::%||%() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(lavaan) library(semPlot) library(MVN) Let´s take a peek at what we are working with. summary(data) ## stress satisfaction turnover_intent demands support ## Min. :0.600 Min. :0.000 Min. :0.000 Min. :1.000 Min. :1.000 ## 1st Qu.:1.500 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:2.000 1st Qu.:2.333 ## Median :2.000 Median :2.000 Median :1.500 Median :2.000 Median :2.667 ## Mean :1.993 Mean :2.578 Mean :1.896 Mean :2.363 Mean :2.732 ## 3rd Qu.:2.333 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:3.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 All variables are numeric and seems to range between 0 and 5. Note that some variables are ordinal, that is, they are whole numbers that are not continuous, that is, the distance between 1 and 2 cannot be assumed to be the same as between 3 and 4. If we want to assume this, the data needs to normally distributed. So let´s take a look at the distributions. Note however that this is NOT the normality assumption of the linear regression but rather an assumption of using ordinal data as if they were continuous. ggplot(data, aes(stress))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #It has a normal look, and it seems to be continuous measure, this is good. ggplot(data, aes(satisfaction))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Again, quite normal looking - this is ordinal but could be treated as if continuous with some mental gymnastics ggplot(data, aes(turnover_intent))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Interesting, perhaps its measured in half steps? but that begs the question why none of the higher valus such as 4.5 exists, it seems to stop after 2. Strange. ggplot(data, aes(demands))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Like satisfaction, looks pretty good. ggplot(data, aes(support))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Now this is a strange looking distribution...it looks to be a continuous measure and the density of the curve should be quite normal. #However, we need to check the normality of our predicted variables when we have them. Before that we can check the normality tests. One common measure is mardia, so let´s use that. We can also look at some of the qq-plots and the outlier measures. Let´s save the scores in a list called normality_diagnostics normality_diagnostics &lt;- mvn(data , mvnTest = &#39;mardia&#39; #takes maridas test , multivariatePlot = &#39;qq&#39; #normal chi2 qq-plot , univariateTest = &#39;AD&#39; #anderson-darling test for univariate normality , showOutliers = TRUE , showNewData = TRUE #stores a tibble without the outliers , multivariateOutlierMethod = &#39;adj&#39;) #takes the adjusted mahalanobis distance #Checking the normality descriptives normality_diagnostics$Descriptives ## n Mean Std.Dev Median Min Max 25th 75th Skew ## stress 322 1.992847 0.7447653 2.000000 0.6 5 1.500000 2.3325 1.5307216 ## satisfaction 322 2.577640 1.0715218 2.000000 0.0 5 2.000000 3.0000 0.6089998 ## turnover_intent 322 1.895963 1.2288900 1.500000 0.0 5 1.000000 3.0000 0.7192168 ## demands 322 2.363354 0.9145545 2.000000 1.0 5 2.000000 3.0000 0.4177021 ## support 322 2.731884 0.7106893 2.666667 1.0 5 2.333333 3.0000 0.2551844 ## Kurtosis ## stress 4.593582827 ## satisfaction 0.130539733 ## turnover_intent -0.169774465 ## demands -0.002784417 ## support 0.298869065 #Checking univariate normality normality_diagnostics$univariateNormality ## Test Variable Statistic p value Normality ## 1 Anderson-Darling stress 8.7723 &lt;0.001 NO ## 2 Anderson-Darling satisfaction 17.5045 &lt;0.001 NO ## 3 Anderson-Darling turnover_intent 9.3350 &lt;0.001 NO ## 4 Anderson-Darling demands 16.5444 &lt;0.001 NO ## 5 Anderson-Darling support 3.2055 &lt;0.001 NO #Checking multivariate normality normality_diagnostics$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 460.776024749078 1.05636187815355e-75 NO ## 2 Mardia Kurtosis 8.96312473441598 0 NO ## 3 MVN &lt;NA&gt; &lt;NA&gt; NO So, nothing is normal and we have quite a few outliers according to Mahalanobis. If we want an outlier free data set the mvn function saves it for us. Let´s try to extract the clean data from the normality diagnostics. clean_data &lt;- normality_diagnostics$newData Here we have the data without the 27 outliers indicated by the normality diagnostics. I´m not a fan of this type of diagnostics, but we can use the raw data as a type of sensitivity analysis later on, for now lets move on using the clean data. Since we don´t really know what we are doing it´s always good to look at the correlations to inform us about what connections lie within our sample. So let´s do that and then try to find a good model for our data. #This gives a normal correlation matrix cor(clean_data) ## stress satisfaction turnover_intent demands support ## stress 1.00000000 -0.4892677 0.22641261 -0.04095583 -0.3617246 ## satisfaction -0.48926773 1.0000000 -0.59110506 0.12206989 0.3145087 ## turnover_intent 0.22641261 -0.5911051 1.00000000 0.03633829 -0.2111387 ## demands -0.04095583 0.1220699 0.03633829 1.00000000 0.1464965 ## support -0.36172462 0.3145087 -0.21113872 0.14649650 1.0000000 #we can however plot the data out using the corplot function from the psych package without loading it by using the :: command like so: psych::corPlot(cor(clean_data)) #Note. the omitted variable in the bottom is turnover_intent Stress, support and satisfaction seems to be correlated just fine. Demands seem to be a quite superfluous variable having low correlations to all other variables. So let´s build a model where stress and support are allowed to correlate and that both have a causal impact on the satisfaction you have in the workplace. Let´s also say that support and satisfaction have a causal impact on your turnover intent. Let´s specify this model using the lavaan syntax. model.1 &lt;- (&#39; #regressions satisfaction ~ stress + support turnover_intent ~ satisfaction turnover_intent ~ support #covariates stress ~~ support &#39;) Now that we have our model - specified in text - we can fit that model using lavaans sem function just like we did before. fit.1 &lt;- sem( model = model.1 #the model we want to fit , data = clean_data #the data we want to fit the model on , estimator = &#39;MLM&#39; #the estimator we use (robust maximum likelihood) ) Now that we have our fit, we can summarise the results summary(fit.1 #the fit we want to summarize , standardize = TRUE #we want the standardised estimates , fit.measures = TRUE #we also want the fit measures(CFI, TLI) , rsquare = TRUE #if we want to r2 for the model ) ## lavaan 0.6-9 ended normally after 18 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 295 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 3.014 2.867 ## Degrees of freedom 1 1 ## P-value (Chi-square) 0.083 0.090 ## Scaling correction factor 1.051 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 260.797 208.698 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.250 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.992 0.991 ## Tucker-Lewis Index (TLI) 0.953 0.945 ## ## Robust Comparative Fit Index (CFI) 0.992 ## Robust Tucker-Lewis Index (TLI) 0.954 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1420.264 -1420.264 ## Loglikelihood unrestricted model (H1) -1418.757 -1418.757 ## ## Akaike (AIC) 2858.528 2858.528 ## Bayesian (BIC) 2891.711 2891.711 ## Sample-size adjusted Bayesian (BIC) 2863.169 2863.169 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.083 0.080 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.197 0.191 ## P-value RMSEA &lt;= 0.05 0.195 0.207 ## ## Robust RMSEA 0.082 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.199 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.022 0.022 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## stress -0.633 0.107 -5.902 0.000 -0.633 -0.432 ## support 0.252 0.087 2.909 0.004 0.252 0.158 ## turnover_intent ~ ## satisfaction -0.672 0.051 -13.158 0.000 -0.672 -0.582 ## support -0.051 0.084 -0.610 0.542 -0.051 -0.028 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~~ ## support -0.178 0.044 -4.093 0.000 -0.178 -0.362 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .satisfaction 0.850 0.066 12.878 0.000 0.850 0.739 ## .turnover_intnt 0.994 0.074 13.440 0.000 0.994 0.650 ## stress 0.536 0.081 6.649 0.000 0.536 1.000 ## support 0.454 0.043 10.580 0.000 0.454 1.000 ## ## R-Square: ## Estimate ## satisfaction 0.261 ## turnover_intnt 0.350 So our model fits remarkably well. Not even the chi2 is significant. Also, not how much better our model is than the baseline model. One bad thing is the fact that we are not very courageous, we only have one degree of freedom. Let´s plot this model out using semPlot and then try improving our fit even more. #This is a more detailed notation of the semPlot alternatives semPaths( object = fit.1 #the fit we want to plot(our model) , what = &#39;est, std&#39; #what we want to plot(the z-coefficients) , style = &#39;lisrel&#39; #the style of the plot , curvePivot = TRUE #manipulates how the lines look , esize = 4 #the size of our lines , nCharNodes = 0 #takes all the characters in our manifest vars , edge.label.cex = 1.5#how big the estimates are(indicated in &quot;what =&quot;) , residuals = FALSE #removes the residual loops , sizeMan = 15 #indicates the width of manifest variables , rotation = 2 #how we want to rotate the plot(wich direction) , theme = &#39;Borkulo&#39;) #the theme we want to use, i like this one #Looks pretty good right? but notice the very weak trace between support and turnover intent and satisfaction! Let´s interpret this. We can see that the correlation between support and stress is -.36 (as we saw in the correlation plot). We can also see that for every unit increase in stress satisfaction decreased with .43 standard deviations. We also see that for every unit increase in satisfaction we see a decrease in turnover_intent at .58 standard deviations. All these estimates, together with the unstandardized counterparts are available in the summary above. In the summary we also see an r2 of .26 in satisfaction and .35 in turnover intent, meaning that we can explain roughly 26% of the variation in satisfaction through stress and support and .35% of the variation in turnover intent through support and satisfaction. Pretty neat right? So where do we go from here? we have a model and its fits the data very well. We want to be brave scientists though, so we should refine our model and make some bolder claims. For example, let´s make the claim that support does NOT influence turnover intent, support does not even influence satisfaction. No, support actually has a one directional relationship with stress, that is the degree of support we have impacts our degree of stress, but stress does not influence how much support we have. Let´s specify this model and fit it. model.2 &lt;- (&#39; #regressions stress ~ support satisfaction ~ stress turnover_intent ~ satisfaction &#39;) fit.2 &lt;- sem( model = model.2 , data = clean_data , estimator = &#39;MLM&#39;) summary(fit.2 , standardize = TRUE , fit.measures = TRUE , rsquare = TRUE ) ## lavaan 0.6-9 ended normally after 14 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 295 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 11.898 12.009 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.008 0.007 ## Scaling correction factor 0.991 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 260.797 241.478 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.080 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.965 0.962 ## Tucker-Lewis Index (TLI) 0.930 0.923 ## ## Robust Comparative Fit Index (CFI) 0.965 ## Robust Tucker-Lewis Index (TLI) 0.930 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1122.743 -1122.743 ## Loglikelihood unrestricted model (H1) -1116.794 -1116.794 ## ## Akaike (AIC) 2257.486 2257.486 ## Bayesian (BIC) 2279.608 2279.608 ## Sample-size adjusted Bayesian (BIC) 2260.580 2260.580 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.100 0.101 ## 90 Percent confidence interval - lower 0.045 0.046 ## 90 Percent confidence interval - upper 0.163 0.164 ## P-value RMSEA &lt;= 0.05 0.064 0.063 ## ## Robust RMSEA 0.100 ## 90 Percent confidence interval - lower 0.046 ## 90 Percent confidence interval - upper 0.163 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.058 0.058 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~ ## support -0.393 0.083 -4.756 0.000 -0.393 -0.362 ## satisfaction ~ ## stress -0.717 0.104 -6.920 0.000 -0.717 -0.489 ## turnover_intent ~ ## satisfaction -0.682 0.049 -13.783 0.000 -0.682 -0.591 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .stress 0.466 0.057 8.237 0.000 0.466 0.869 ## .satisfaction 0.875 0.067 12.995 0.000 0.875 0.761 ## .turnover_intnt 0.996 0.074 13.531 0.000 0.996 0.651 ## ## R-Square: ## Estimate ## stress 0.131 ## satisfaction 0.239 ## turnover_intnt 0.349 By looking at the fit measures we can see that this model fits very well. But notice that we now have 3 degrees of freedom, that is, by further constraining the model (not allowing correlations between stress and support and removing the regression between support and turnover intent) we have expressed a more theoretically parsimonious model - very cool. Let´s make the path diagram and interpret the coefficients. semPaths( object = fit.2 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , curvePivot = TRUE , esize = 10 , asize = 5 #arrowhead size , sizeMan2 = 3 #height of the manifest variables , nCharNodes = 0 , edge.label.cex = 1.5 , residuals = FALSE , sizeMan = 15 , rotation = 2 , theme = &#39;Borkulo&#39;) #Note. The new comments are just arguments we didn´t use in the previous plot Cool, notice how we rephrased the connection between support and stress to a one-sided arrow. This can now be interpreted as a regression weight, that is, for every unit increase in support we se a .36 unit decrease in stress. We also see a bigger coefficient for the relationship between stress and satisfaction. This is likely due to the fact that we presume that support does not impact satisfaction but influences stress and thus, some of the variation we can explain in stress by support also influences satisfaction - i.e., stress mediates the relationship between support and satisfaction. The relationship between satisfaction and turnover intent is also different, but only by .01, that is, super small and an unimportant difference. Now that we have two overidentified models we can compare the fit of them. This is surprisingly easy; we only use the anova function and our to model fits. Note that this is not a GLM anova but an LRT test, that is, a likelihood ratio test. anova(fit.1, fit.2) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.1 1 2858.5 2891.7 3.0141 ## fit.2 3 2257.5 2279.6 11.8977 9.2498 2 0.009805 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So what does this tell us? It tells us that the baseline model(fit.1) fits the data significantly better then the other model(fit.2). However, one should not that the AIC and BIC is lower for the fit.2 model, low scores on AIC and BIC is indicative of good fit, and models with low values are preferable to those with higher BIC and AIC. I do not feel particularly strongly about any of these models, but if I had to choose one I would choose the second one since I like parsimony, but the first model technically fits better (though its almost identified so do with that what you will). For more on the LRT you can see: http://econ.upf.edu/~satorra/dades/BryantSatorraPaperInPressSEM.pdf One last thing we can do before we move on to other things is the check the modification indices. This is one cool thing that you can do with a path analysis that normal regression can´t really do. We will not use them for anything, but it is interesting to look at them. modificationindices(fit.2) %&gt;% arrange(-mi) %&gt;% head(20) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 1 support ~ satisfaction 8.440 0.131 0.131 0.208 0.208 ## 2 satisfaction ~ support 8.440 0.252 0.252 0.158 0.235 ## 3 stress ~~ satisfaction 8.440 0.299 0.299 0.468 0.468 ## 4 stress ~ satisfaction 8.440 0.341 0.341 0.500 0.500 ## 5 stress ~ turnover_intent 6.336 -0.109 -0.109 -0.184 -0.184 ## 6 support ~ turnover_intent 3.887 -0.067 -0.067 -0.124 -0.124 ## 7 stress ~~ turnover_intent 3.408 -0.082 -0.082 -0.121 -0.121 ## 8 satisfaction ~ turnover_intent 2.351 -0.171 -0.171 -0.197 -0.197 ## 9 turnover_intent ~ stress 2.351 -0.139 -0.139 -0.083 -0.083 ## 10 satisfaction ~~ turnover_intent 2.351 -0.170 -0.170 -0.182 -0.182 ## 11 turnover_intent ~ support 0.298 -0.048 -0.048 -0.026 -0.039 #Note. the %&gt;% is a pipe function, they make code simpler to write so look into them! See the provided programming resources in the beginning of the book. The modification indices (mi column) shows how much the chi2 of the model would drop if the parameter was included. Thus, indices over 3.84 will significantly increase the fit of the model. We have a few modifications we can make to our model, some of which improves the fit quite a bit. Lastly, let´s do a sensitivity check by running the second model through the raw data. That is, the data with the outliers included. sensitivity.fit &lt;- sem( model = model.2 , data = data , estimator = &#39;MLM&#39;) summary(sensitivity.fit, standardize = TRUE, fit.measures = TRUE , rsquare = TRUE) ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 322 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 10.159 10.615 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.017 0.014 ## Scaling correction factor 0.957 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 267.940 254.344 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.053 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.973 0.969 ## Tucker-Lewis Index (TLI) 0.945 0.939 ## ## Robust Comparative Fit Index (CFI) 0.972 ## Robust Tucker-Lewis Index (TLI) 0.944 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1234.025 -1234.025 ## Loglikelihood unrestricted model (H1) -1228.945 -1228.945 ## ## Akaike (AIC) 2480.049 2480.049 ## Bayesian (BIC) 2502.697 2502.697 ## Sample-size adjusted Bayesian (BIC) 2483.665 2483.665 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.086 0.089 ## 90 Percent confidence interval - lower 0.032 0.034 ## 90 Percent confidence interval - upper 0.147 0.151 ## P-value RMSEA &lt;= 0.05 0.119 0.108 ## ## Robust RMSEA 0.087 ## 90 Percent confidence interval - lower 0.034 ## 90 Percent confidence interval - upper 0.146 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.054 0.054 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~ ## support -0.346 0.073 -4.730 0.000 -0.346 -0.330 ## satisfaction ~ ## stress -0.694 0.099 -7.007 0.000 -0.694 -0.482 ## turnover_intent ~ ## satisfaction -0.672 0.048 -14.120 0.000 -0.672 -0.586 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .stress 0.493 0.060 8.155 0.000 0.493 0.891 ## .satisfaction 0.878 0.065 13.536 0.000 0.878 0.767 ## .turnover_intnt 0.989 0.074 13.405 0.000 0.989 0.657 ## ## R-Square: ## Estimate ## stress 0.109 ## satisfaction 0.233 ## turnover_intnt 0.343 anova(sensitivity.fit, fit.2) ## Warning in lavTestLRT(object = object, ..., model.names = NAMES): lavaan WARNING: some ## models have the same degrees of freedom ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## sensitivity.fit 3 2480.1 2502.7 10.159 ## fit.2 3 2257.5 2279.6 11.898 1.7384 0 #Note. We are comparing two models with the same degrees of freedom, this is not very appropriate since they are not nested (and on different data) but its nice to see the measures anyways. So, the model based on the raw data is not that different from we get when we have cleaned the data. This becomes even more apparent if we were to check the parameter estimates for the two models like so: parameterestimates(fit.2) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 stress ~ support -0.393 0.083 -4.756 0 -0.555 -0.231 ## 2 satisfaction ~ stress -0.717 0.104 -6.920 0 -0.920 -0.514 ## 3 turnover_intent ~ satisfaction -0.682 0.049 -13.783 0 -0.779 -0.585 ## 4 stress ~~ stress 0.466 0.057 8.237 0 0.355 0.577 ## 5 satisfaction ~~ satisfaction 0.875 0.067 12.995 0 0.743 1.007 ## 6 turnover_intent ~~ turnover_intent 0.996 0.074 13.531 0 0.851 1.140 ## 7 support ~~ support 0.454 0.000 NA NA 0.454 0.454 parameterestimates(sensitivity.fit) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 stress ~ support -0.346 0.073 -4.730 0 -0.489 -0.202 ## 2 satisfaction ~ stress -0.694 0.099 -7.007 0 -0.888 -0.500 ## 3 turnover_intent ~ satisfaction -0.672 0.048 -14.120 0 -0.765 -0.579 ## 4 stress ~~ stress 0.493 0.060 8.155 0 0.374 0.611 ## 5 satisfaction ~~ satisfaction 0.878 0.065 13.536 0 0.751 1.005 ## 6 turnover_intent ~~ turnover_intent 0.989 0.074 13.405 0 0.844 1.133 ## 7 support ~~ support 0.504 0.000 NA NA 0.504 0.504 They are pretty much the same parameter estimates, certainly not significantly different. So, what more can we do for this fun little analysis? we can do some bootstrapping. Lavaan has a really nice bootstrapping function that gives a lot of freedom to choose what to do with the estimates. Let´s bootstrap all estimates from the second model(fit.2) and take a percentile intervall for them. Note that this can take a few seconds. boot_sample &lt;- bootstrapLavaan( object = fit.2 #the fit we want to sample , R = 1000 #the number of iterations we want ) Notice that boot_sample is a matrix of many estimates. Let´s put the regression coefficients in a dataframe and make our intervals. boot_sample &lt;- data.frame(boot_sample) boot_coef &lt;- select(boot_sample #the data we want to take variables/cols from , c(1,2,3,)) #a list of the cols we want to take(1,2,3) quantile(boot_coef$stress.support #the estimate we want to take intervals on , probs = c(.025, .5, .975)) #the percentile points we want to see ## 2.5% 50% 97.5% ## -0.5489199 -0.3825642 -0.2386760 #We see that the regression weight ranges between -.55 and -.24 quantile(boot_coef$satisfaction.stress , probs = c(.025, .5, .975)) ## 2.5% 50% 97.5% ## -0.9758261 -0.7247085 -0.5454487 #We see that the regression weight ranges between -.98 and -.55 quantile(boot_coef$turnover_intent.satisfaction , probs = c(.025, .5, .975)) ## 2.5% 50% 97.5% ## -0.7719642 -0.6811282 -0.5928512 #We see that the regression weight ranges between -.77 and -.59 This is not super interesting, but it is pretty cool and also kind of illustrates what you can do with bootstrapping in a more manual way. There are of course much more you can do than to take percentile intervals of the regression weights but that lies beyond the specific realm of path analysis. 10.3 Multiple regression(again) By now you should feel good, both with lavaan semPlot and the general procedure for path analysis. So, lets take a step back and rework the good ´ol Field example for multiple regression, but with a path analysis. And don´t worry, this will be short and sweet :) 10.4 Mediation analysis In the Field book he gives an example of mediation with the data set from Lambert et al.,(2012). Since we are very comfortable with working with paths, mediation becomes a simple thing. In the regression example in the previous chapter we only estimated direct effects, that is, effects of a on c (or x on y if you prefer those letters). Mediation can estimate both direct and indirect effects. The indirect effect is the multiplication of path a and path b. Let´s load the data. While the process of a moderation analysis is quite complicated in terms of functions and packages - as we saw in the regression part of the book, with mediation we can rely on good ´ol lavaan. #Packages we need library(lavaan) library(semPlot) #Loading the data data &lt;- read_sav(&quot;Lambert et al. (2012).sav&quot;) names(data) &lt;- c(&#39;consumption&#39;, &#39;ln_porn&#39;, &#39;commit&#39;, &#39;infi&#39;, &#39;hook_ups&#39;) Let´s specify the model from the Field book. med_model.1 &lt;- (&#39; # Direct effect infi ~ c*consumption # Mediator commit ~ a*consumption infi ~ b*commit # Indirect effect (a*b) ab := a*b # Total effect total := c + (a*b) &#39;) #Note. The defined variables a, b and c are needed for the mediation analysis to work. These are the paths of our model, without them we cannot calculate our main and indirect effects! the lavaan symbol &#39;:=&#39; describes a new parameter dependent on our specified model. In this case it´s the regression weights (I believe) Let´s fit and summarise the result of this model. med_fit.1 &lt;- sem(med_model.1, data) #Summarising a lavaan can take a lot of commands, I will note what they do here as I have done before, but note that there are many other alternatives you can use! summary(med_fit.1 #the model we want to use(med_fit.1) , standardize = T #do we want a standardized estimate? T mean true/yes , fit.measures = T #do we want fit measures? , rsquare = T) #do we want r2 ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 35.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -501.418 ## Loglikelihood unrestricted model (H1) -501.418 ## ## Akaike (AIC) 1012.836 ## Bayesian (BIC) 1030.218 ## Sample-size adjusted Bayesian (BIC) 1014.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## consumptin (c) 0.107 0.038 2.797 0.005 0.107 0.171 ## commit ~ ## consumptin (a) -0.092 0.042 -2.175 0.030 -0.092 -0.139 ## infi ~ ## commit (b) -0.268 0.058 -4.612 0.000 -0.268 -0.282 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.429 0.039 10.932 0.000 0.429 0.878 ## .commit 0.531 0.049 10.932 0.000 0.531 0.981 ## ## R-Square: ## Estimate ## infi 0.122 ## commit 0.019 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.025 0.013 1.967 0.049 0.025 0.039 ## total 0.132 0.040 3.329 0.001 0.132 0.211 Nice, this is exactly what we want. Notice also that our model is saturated/identified, that is, it has 0 degrees of freedom. This means that we can´t really assess the fit of this model. Let´s make a quick interpretation. Infidelity increases with .107 units for every unit increase in consumption. Commitment decreases by .092 units for every increase in consumption and infidelity decreases with .268 for every unit increase in commitment. ´ab´ is our indirect effect, that is, .025. Which can be interpreted as a kind of r2. semPaths(med_fit.1 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , layout = &#39;tree&#39; , nCharNodes = 0 , edge.label.cex = 1.5 , sizeMan = 20 , residuals = F , rotation = 4 , theme = &#39;Borkulo&#39;) #Note that this is NOT the same output as in the field book. He uses the log transformed variable for consumption. So let´s recreate his model. med_model.2 &lt;- (&#39; # direct effect infi ~ c*ln_porn # mediator commit ~ a*ln_porn infi ~ b*commit # indirect effect (a*b) ab := a*b # total effect total := c + (a*b) &#39;) med_fit.2 &lt;- sem(med_model.2, data) summary(med_fit.2, standardize = T, fit.measures = T, rsquare = T) ## lavaan 0.6-9 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 33.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -502.418 ## Loglikelihood unrestricted model (H1) -502.418 ## ## Akaike (AIC) 1014.835 ## Bayesian (BIC) 1032.218 ## Sample-size adjusted Bayesian (BIC) 1016.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## ln_porn (c) 0.457 0.193 2.365 0.018 0.457 0.145 ## commit ~ ## ln_porn (a) -0.470 0.212 -2.215 0.027 -0.470 -0.142 ## infi ~ ## commit (b) -0.271 0.058 -4.642 0.000 -0.271 -0.285 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.432 0.040 10.932 0.000 0.432 0.886 ## .commit 0.531 0.049 10.932 0.000 0.531 0.980 ## ## R-Square: ## Estimate ## infi 0.114 ## commit 0.020 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.127 0.064 1.999 0.046 0.127 0.040 ## total 0.585 0.200 2.925 0.003 0.585 0.186 semPaths(med_fit.2 , what = &#39;est&#39; , style = &#39;lisrel&#39; , layout = &#39;tree&#39; , nCharNodes = 0 , edge.label.cex = 1.5 , sizeMan = 20 , residuals = F , theme = &#39;Borkulo&#39; , rotation = 4) #Very nice, we have recreated the findings from Field. These are the same models, but with the log transformation. The reason why I didn´t use that in the first model is because it´s harder to interpret. I have a hard time understanding what this indirect effect of .127 means since its a combination of the influence of a log variable, through a variable that is not logged. A smarter person than I will have to describe what this means. Field also uses bootstrapped standard errors; this can be specified in the fit portion of our workflow. Let´s refit our second model but with bootstrapped SEs. #it´s going to take a few seconds so do not fret if you dont get an output set.seed(234) #setting seed so that we can recreate the random values med_fit.2 &lt;- sem(med_model.2 #what model we want to fit , se = &#39;bootstrap&#39; #how do we want to estimate the std.err? , data) #indicates the data we want to fit the model on. summary(med_fit.2, standardize = T, fit.measures = T, rsquare = T) ## lavaan 0.6-9 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 33.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -502.418 ## Loglikelihood unrestricted model (H1) -502.418 ## ## Akaike (AIC) 1014.835 ## Bayesian (BIC) 1032.218 ## Sample-size adjusted Bayesian (BIC) 1016.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## ln_porn (c) 0.457 0.251 1.822 0.068 0.457 0.145 ## commit ~ ## ln_porn (a) -0.470 0.234 -2.004 0.045 -0.470 -0.142 ## infi ~ ## commit (b) -0.271 0.071 -3.824 0.000 -0.271 -0.285 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.432 0.055 7.908 0.000 0.432 0.886 ## .commit 0.531 0.050 10.539 0.000 0.531 0.980 ## ## R-Square: ## Estimate ## infi 0.114 ## commit 0.020 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.127 0.071 1.798 0.072 0.127 0.040 ## total 0.585 0.250 2.339 0.019 0.585 0.186 parameterestimates(med_fit.2) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 infi ~ ln_porn c 0.457 0.251 1.822 0.068 -0.032 0.960 ## 2 commit ~ ln_porn a -0.470 0.234 -2.004 0.045 -0.917 0.019 ## 3 infi ~ commit b -0.271 0.071 -3.824 0.000 -0.404 -0.124 ## 4 infi ~~ infi 0.432 0.055 7.908 0.000 0.314 0.531 ## 5 commit ~~ commit 0.531 0.050 10.539 0.000 0.434 0.630 ## 6 ln_porn ~~ ln_porn 0.049 0.000 NA NA 0.049 0.049 ## 7 ab := a*b ab 0.127 0.071 1.798 0.072 -0.005 0.281 ## 8 total := c+(a*b) total 0.585 0.250 2.339 0.019 0.089 1.096 Cool, now we have a SE estimate of .071 instead of .064. Note also that the ci for the ab effect, that is, the indirect effect, ranges from -.005 to .28, we cross zero and can therefore not be certain in the existence of the mediation. The only robust effect is that of commitment on infidelity. Let´s finish of by fitting a normal linear interaction model and compare the outcomes. lm_model &lt;- lm(infi ~ consumption*commit, data) summary.lm(lm_model) ## ## Call: ## lm(formula = infi ~ consumption * commit, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.94415 -0.36188 -0.13989 0.00893 2.00893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.53260 0.42566 3.601 0.000387 *** ## consumption -0.05829 0.19836 -0.294 0.769133 ## commit -0.33807 0.10103 -3.346 0.000954 *** ## consumption:commit 0.04142 0.04864 0.852 0.395292 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6591 on 235 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.1251, Adjusted R-squared: 0.1139 ## F-statistic: 11.2 on 3 and 235 DF, p-value: 6.744e-07 This output seems very nice to me. It very much confirms what our mediation analysis found. The only reliable effect is that of commitment. "],["exploratory-factor-analysis-efa.html", "11 Exploratory factor analysis (EFA)", " 11 Exploratory factor analysis (EFA) "],["confirmatory-factor-analysis.html", "12 Confirmatory factor analysis", " 12 Confirmatory factor analysis "],["multi-level-modelling.html", "13 Multi-level modelling", " 13 Multi-level modelling "],["structural-equation-modelling.html", "14 Structural equation modelling", " 14 Structural equation modelling "],["meta-analysis.html", "15 Meta-analysis", " 15 Meta-analysis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> f91ef1f7cdf80f4cef0afd775ea99b40c089c8c7
