[["index.html", "Not yet decided 1 Table of contents", " Not yet decided B. Alfons Edmar 2022-07-18 1 Table of contents Introduction Foreword Some useful resources Welcome to R The R environment Basic programming Organisation Rmarkdown What I expect you to know Descriptive statistics Basic null hypothesis testing Statistical phenomena (regression to the mean, CLT, LOLN) Useful R packages and functions (with links) Bayesian and Frequentist statistics Non-parametric statistics Asking all the question, making statements, and assuming The old guard Bootstrapping Correlation and Covariation in R Welcome to the matrix Regression analysis Univariate regression and Multiple regression The general linear model (ANOVA) The generalized linear model(logit) Cross validation LASSO (least absolute shrinkage and selection operator) Ridge regression Fully Bayesian regression Poststratification Path analysis Multiple regression(again) Mediation analysis Exploratory factor analysis (EFA) Confirmatory factor analysis (CFA) Multi-level modelling (MLM) Structural equation modelling (SEM) Meta-analysis Real data for real scientists "],["introduction.html", "2 Introduction 2.1 Foreword 2.2 The grain of salt 2.3 Some useful resources", " 2 Introduction 2.1 Foreword I thought It would be fun to create a book covering the things that I have learnt this past year and a half while creating a (hopefully) useful tool for future students. I also really enjoy opensource projects such as R and really think that it is a brilliant workspace. It is not only flexible but creates a very structured environment where all your decisions throughout the analysis is easily documented and shared. In short, I like R and RStudio. The problem is that R is hard, the learning curve is slow, and if you are a psychologist, the opportunities to learn programming are sparse unless you seek them out for yourself. With this book I hope to create an accessible guide to how you can use R for your studies and hopefully your future work. Now is a good time to point out that I am not a statistician, as of writing this I have not even graduated from the master´s program. Thus, this book should not be treated as an authoritative voice on how you should do statistics or interpret your results. The aim here is simple, it is to guide you through some of the statistical analyses that you will encounter and introduce the R programming language in the meantime. There will of course be some explanations of the tools that we use in terms of their statistical properties and interpretations of the results from our analyses, but I cannot stress enough that this is not a book about statistics, it´s a book about doing statistics in R. It´s designed to be brief and easily skimmed though in the sense that you wont have to read so much of my blathering to understand the interesting bits - the code. The general layout of the applied chapters will be analyses of the working examples in Discovering statistics by Andy Field (when applicable) and some workshop exercises. I will also end each applied chapter by finding some real data from an article and working through their analysis. In sum, this is a compilation of the topics covered in the master´s programme in psychological science at the university of Gothenburg in 2021/22 with some additional chapters by yours truly on some of the things that I think are useful and important. It is a supplementary text that simply translates the analyses described in the literature into R-code. It is for students who want to learn R but dont know where to start. It can be hard to learn things on your own without any real incentives, and I hope that this book can coax some people into R through it´s accessibility and relevance to what you will be doing in class anyways. Good luck and happy reading/coding. P.s, If you are a statistician, stop reading here, only pain will follow. 2.2 The grain of salt If it wasnt painfully clear from the foreword, I do not presume myself to have any sort of authority in matters of statistics (or software for that matter). This text is as much for my own education as for anyone who wishes to use it as a guide. With that said, there are some disclaimers needed. In general, I prefer Bayesian solutions to data analytic problems compared to frequentist solutions. Since I will mostly cover non-Bayesian analyses, this clarification has value, since much of what we will be doing in this book is not my preferred way of doing statistics. This does not mean that the modelling that we will be doing is wrong or misleading, Im certainly not a Bayesian fundamentalist, but I generally prefer full posterior estimations and bayes factors to p-values and confidence intervals. I cannot stress enough that this is preference, there are many cases where frequent and Bayesian statistics come to similar conclusions, but in some cases, Bayes can help you. And if Bayes does not help you, then what is the reason for doing it? I would again like to stress that I am neither a psychologist, statistician, compute scientists or a teacher, so I am very much out of my depth here, but if I were to pretend that I were any of those things there would be three things that I would like to say before you start going through this book: 1: Be aware of survival bias. All of the code/models (online or in this book) are victims of survival bias. Before those successful models or codes were written, there was numerus unsuccessful attempts to specify that model or code. So, when your model does not work or your code returns am error, be aware that it is a part of the process. And remember, all the successful things you see rests atop a pile of discarded alternatives! 2: Horses for courses. This old adage is very much applied to statistics. Individuals have different preferences in how they wish to solve certain problems. Annoyingly, there are often many ways to solve similar problems, but it is important to remember to use the right horse for the course. This is not meant to be interpreted in a cook-book kind of way but rather that certain analytical solutions are suited for different things. So remember what YOUR question is and try to find the right analytical tool for that specific problem. 3: Embrace uncertainty. It is quite rare, in my experience, that scientists talk about what they do not know, or report statistics in a way that emphasises their uncertainty about a topic. Don´t think that you need to have an answer to everything, it is very hard to know anything  thats why science is hard. This does not mean that you should be afraid to make predictions, but it does mean that you should be very open to being wrong and to be conservative with what you are certain about. Hopefully this section can cover my back to the people who might take extreme issue with me writing this little guide and to give the readers of this a little bit of insight in how I reason about science. 2.3 Some useful resources In this section I wish to direct you to some of great resources that exists on R-programming and statistics in general. Note that not all of these are free, but some of them are. Firstly I must recommend Hadley Wickhams book R for data science. It gives a wonderful introduction to how you can use some of the most convenient R functions. It is an essential of R programming and can be found here: https://r4ds.had.co.nz/. In terms of statistics, I have a few books that I have thoroughly enjoyed. My favourite is statistical rethinking by Richard McElreath. It is splendid book on Bayesian inference using R and STAN (a language for Bayesian modelling and sampling). In 2022 lectures by McElreath was published to youtube giving the book even more accessibility than it already had. Another book (again on Bayesian inference) is doing Bayesian data analysis by John Kruschke. While I prefere McElreath´s book this is a very nice introduction to statistics and Bayesian inference. A book which only makes subtle Bayesian hints is regression and other stories by Gelman, Hill and Vehtari. This is a good book for getting into regression analysis  which is probably a good thing to get in to as a scientist. Another book on statistics that has been of great use to me is Understanding the New Statistics by Geoff Cumming. This is not a book on Bayesian statistics but covers some of the important aspects of statistics in the social sciences. Note that I have not mentioned discovering statistics by Andy Field, this is not because that I dislike that book (I dont), but if I had recommendations I needed to make, I wouldnt include that particular book  in comparison to it´s competitors it does not cut it. However, it admittedly has pedagogical value, thus, we will use much of the data files from Field. This is not because I have any unrequited love for Field, but because the book will probably be familiar to you (the reader) and covered in the courses in the programme. There are of course many other books that are great but that I will not link here, I say, whichever book that makes you do good statistics is a good book for you (but if you want inspiration feel free to check out the links below). Links: McElreath: https://github.com/rmcelreath/stat_rethinking_2022 Kruschke: https://www.sciencedirect.com/book/9780124058880/doing-bayesian-data-analysis Gelman, Hill and Vehtari: https://avehtari.github.io/ROS-Examples/ Cumming: https://thenewstatistics.com/itns/ "],["welcome-to-r.html", "3 Welcome to R 3.1 The R environment 3.2 Basic programming 3.3 Organisation", " 3 Welcome to R If you have not downloaded R yet, now is the time to do so. Simply googling r download should get you there, if not, go to: r-project.org or find one of the many tutorials of how to download r and the interface rstudio. Rstudio is very nice and can be customized to suite your preferred layout. 3.1 The R environment Rstudio has 4 primary panes where you can work from. These are the global environment, plots/files, source, and console. The source and the console are the panes/windows where you can write and execute code. I recommend using the source pane for all your programming. That way you can edit you code easily and execute it whenever you want  and most importantly, save it for later so you work never disappears. 3.2 Basic programming R is more or less a very sophisticated calculator. You can use the source as a simple calculator. For example, executing the code 1+1 returns 2 in the consol. Try It out yourself or copy the code below. 1+1 ## [1] 2 You can also save objects to the global environment and give them certain values. For example, if a and b are equal to 1, a + b will also equal 2. The only thing you need to do is define the characters as numbers like so: a = 1 b = 1 a+b ## [1] 2 You can also ask questions from r. However, r can only answer in a binary fashion, that is, TRUE(T) or FALSE(F). To ask r if a = b we use double equal marks. The == notation is a logical phrasing while the = is an assignment of value. 1 == 1 ## [1] TRUE 1+1 == 3 ## [1] FALSE a==b ## [1] TRUE a+b == 3 ## [1] FALSE I find this quite annoying, therefore I rarely use the single equal mark for anything outside what is required by functions. Instead, I use &lt;-. In many settings the &lt;- is synonymous with =, but whenever I assign a function or a value to something in the global environment I use &lt;- instead of = since I find the arrow more readable and less confusing. #We assign with arrows a &lt;- 2 b &lt;- 2 a + b ## [1] 4 #And do logic with equal a == b ## [1] TRUE #Arrows don´t to logic - try writing a &lt;- &lt;- b This is not even close to scratching the surface of what r programming is, but this is not a text about programming. I encourage you to looking into some of the resources on R I provide if you are interested in more then the absolute bare-bones. 3.3 Organisation Something that will make you time with r infinitely more enjoyable is keeping your house in order and organising your files so that they are easily accessible in your computer. You NEED to have a folder dedicated to r. This folder should in turn be divided up in folders containing scripts data files and other potential folders. How you organise is of course very much up to you, but having files that are in the same place is very important, otherwise things will disappear in the bottomless pit that is default hard drive storage. This is particularly important for when you want to load datafiles fast and consistently and avoid errors in your scripts. This leads us to projects. 3.3.1 Projects R projects are awesome. They function as a kind of working space whare you can store various files associated with what you are working on. For example, you might want to create an r project for following this text. You simply go up to files -&gt; new projects -&gt; new directory -&gt; new project the fill in the name and where you want to store the project  I have a folder within my r folder containing all projects. Opening data files and scripts is very accessible through the files pane when working within a project. 3.3.2 Rmarkdown The easiest way to work in r is with normal scripts, the annoying part with normal scripts is that writing notes is a bit cumbersome. They never look good, and you have to put a # mark before writing so that your code doesnt disturb the other lines of code. One way to take notes and structure an analysis better is with r markdown. There are many things that goes into using markdown, but with a bit practice it becomes natural  I encourage you to look up a tutorial on using markdown :) "],["what-i-expect-you-to-know.html", "4 What I expect you to know 4.1 Descriptive statistics 4.2 Basic null hypothesis testing 4.3 Statistical phenomena", " 4 What I expect you to know This book could (and probably should) be confused with my own notetaking in certain if not all chapters. That means that I will probably omit many things that are obvious to me or things I deem to much of a sidebar to explain. That is, things that will make the reading a drag  this should be a quick read! Therefore, it might be good to mention a few things that will be good to know before you read this (although they are not necessary). 4.1 Descriptive statistics Descriptive statistics such as measures of centrality (mean, mode, median) and spread (variance, deviance etc) will not be explained in this text. This extends to other statistical values such as residuals and various errors. An understanding of the basics is required since interpretations of more complicated statistics such as root mean squared error of approximation (RMSEA) is not something I can or will spend much time at. Again, this is not a text about statistics, its a text about doing statistics. Therefore, and understanding of the basics is good to have since I won´t cover it. 4.2 Basic null hypothesis testing Null hypothesis testing (NHT) is, for better or worse, the most common way of doing statistics currently. At a graduate level the theory behind NHT should be something you are familiar with. While we will do some test interpretation, the logic underlying hypothesis testing will not be covered directly. It might be mentioned in some chapters where we play around with distributions and simulation. Other than that, NHT will be shown rather then explained. 4.3 Statistical phenomena Basic knowledge of statistical phenomena such as regression to the mean, law of large numbers, independence of events, correlation != causation and central limit theorem is very good to have. It will make the logic behind some procedures clearer. It is also useful in the sense that it will make it easier to diagnose errors and weird results. Since some of the techniques we will cover are a bit advanced, having a basic understanding of some of the underlying principles of statistics and how data behaves is good. If everything I have said up until this point sounds like gibberish, do not fret. While it´s good to know the basics, you will be able to read the code and use it yourself without issue, but it is important to remember that most things that are good to know about statistics wont be covered here! "],["useful-packages-and-functions-that-helped-me-learn.html", "5 Useful packages and functions that helped me learn 5.1 Tidyverse 5.2 Distribution functions - rnorm, dbeta, rbinom etc 5.3 Faux 5.4 lm 5.5 Lavaan 5.6 semPlot 5.7 ?", " 5 Useful packages and functions that helped me learn This is a bit of a strange chapter, I am not entirely sure why I included it, but I think it might be a good idea to have a few words on the packages that we will be using. This Chapter could be returned two If you get stuck with some code and for some really strange reason can´t find a better place to search for answers than here. In sum, there are some useful packages and function in R, and using them might be beneficial for both learning and doing you statistics. 5.1 Tidyverse Like the name implies, tidyverse is an entire universe of connected packages designed to make your coding comfortable and aesthetically pleasing. I neither have the expertise nor the time to go through even the basics of all the tidyverse packages but we will be using them frequently, and when it happens there will be some notation. In the meantime, the first chapter of R for data science is an absolute gem if you are serious about learning R. Very nice introduction, assuming no previous coding experience (and it´s free) I cannot recommend it enough (hence the repeat) https://r4ds.had.co.nz/ 5.2 Distribution functions - rnorm, dbeta, rbinom etc These functions were probably what got me to actually think I understood some things about statistics, which is quite impressive for me. They are essentially functions that describe the shape of certain distributions and gives you the ability to generate random variables from those distributions. The possibilities of this is really endless but I will show some examples of how to generate data below. #Since the data is &quot;random&quot; it will be impossible to reproduce if you re-run the code. This is why we usually set a seed. This makes it so the random data can be reproduced. We do this through the set.seed function before we generate random numbers. Fun fact, the numbers are not truly random since they can be reproduced exactly, they are pseudo-random. #Random data following the normal distribution set.seed(5395) #setting the seed, this can by any combination of integers. norm_data &lt;- rnorm( n = 500 #the number of observations we want , mean = 4 #the mean of the numbers , sd = 1 #the standard deviation of the data ) #Prof of normality through histogram hist(norm_data) #Voila. #We can do the same with the t-distribution set.seed(453) t_data &lt;- rt( n = 50 #number of observations , df= 6 #degrees of freedom ) hist(t_data) #the dame things can be done to the F distribution using rf() #If we want binary outcomes we can use the binomial distribution with r(binom) #lets say we want to simulate 10 tosses of a fair coin for example. set.seed(5325) coin_data &lt;- rbinom( n = 10 #number of observations/tosses , size = 1 #number of trials per observation(one this time) , prob = .5 #the probability of a successful outcome (50/50) ) hist(coin_data) #Cool, If we say 1 is tails we have 6 tails and 4 heads. These might seem like very trivial things, but by manipulating the data and doing analyses on data sets where you actually KNOW what the true population parameters are, can be very useful and enlightening. Simulation is the best kind of preparation; it also highlights very well what you expect from the data. The next package I can recommend handles data generation quite smoothly. 5.3 Faux One of the most powerful properties of programming languages is their ability to generate pseudo random variables. We have seen that we can easily generate independent random numbers following some distribution, but what if we want to simulate relationships between variables? enter faux. There are probably many other packages doing the same things, but this is the one I use, and it has served me quite well.  let´s generate the data that we will work with through this chapter. library(&#39;faux&#39;) set.seed(324) #the seed returns! data &lt;- rnorm_multi(n = 500 #the number of observations we want , vars = 3 #number of variables we want , mu = c(3,5,2)#the three means for our variables. , sd = c(1.3, 2, .6) #the three SDs , r = c(1, .4, .7, #this is where it gets tricky. r is .4, 1, .4, #the correlation matrix. If we have .7, .4, 1) #many variables it can be tedious , varnames = c(&#39;y&#39;, &#39;x&#39;, &#39;z&#39;)) #the names of our variables #lets look at the correlations and the mean of our data cor(data) #correlates all variables in the data set with each other ## y x z ## y 1.0000000 0.3674651 0.6744010 ## x 0.3674651 1.0000000 0.4105545 ## z 0.6744010 0.4105545 1.0000000 summary(data) #summarises the content in the data ## y x z ## Min. :-0.3747 Min. :-0.8381 Min. :0.2554 ## 1st Qu.: 1.9188 1st Qu.: 3.6476 1st Qu.:1.5958 ## Median : 2.9566 Median : 4.9841 Median :2.0288 ## Mean : 2.9204 Mean : 5.0046 Mean :2.0239 ## 3rd Qu.: 3.7996 3rd Qu.: 6.3985 3rd Qu.:2.4164 ## Max. : 6.5359 Max. :12.3798 Max. :3.8473 #Notice how the values are similar, but not exactly the same, why is that? it is because of randomness. We can remedy this if you want to have full control over the parameter estimates in your data. We do this adding the empirical = true to the data generation formula like so: data.2 &lt;- rnorm_multi(n = 500 , vars = 3 , mu = c(3,5,2) , sd = c(1.3, 2, .6) , r = c(1, .4, .7, .4, 1, .4, .7, .4, 1) , varnames = c(&#39;y&#39;, &#39;x&#39;, &#39;z&#39;) , empirical = TRUE) #do we want empirical estimates? #This is now empirical data and not data sample from a population cor(data.2) ## y x z ## y 1.0 0.4 0.7 ## x 0.4 1.0 0.4 ## z 0.7 0.4 1.0 summary(data.2) ## y x z ## Min. :-0.8063 Min. :-0.8758 Min. :0.293 ## 1st Qu.: 2.1405 1st Qu.: 3.7060 1st Qu.:1.590 ## Median : 2.9691 Median : 4.9931 Median :2.015 ## Mean : 3.0000 Mean : 5.0000 Mean :2.000 ## 3rd Qu.: 3.9539 3rd Qu.: 6.2568 3rd Qu.:2.398 ## Max. : 6.5047 Max. :11.9103 Max. :3.605 #And there it is, identical to how we specified it in the rnorm_multi function. Now that we have some data, let´s try to fit it to a model 5.4 lm As we will a little bit more in-depth below, R generally uses strings from specifying models. That is, if we want to regress Y on x we specify that as Y~X. You will be doing a lot of that little squiggly(~), or, as some call it tilde(it´s proper name). Most if not all simple statistics can be done using the linear model function lm(). There are many functions for various types of t-test and chi2 and others but you will get really far with just the lm(), at the end of the day most tests all fall under the general linear model. Let´s fit a linear model with the data we just generated, we can predict y with x and z. fit.lm &lt;- lm( formula = y ~ x + z #the formula of the regression , data = data #the data we want to use ) #When we have fitted a model, we need to summarise the output. There is on specifically for linear models that we can use. summary.lm(object = fit.lm) ## ## Call: ## lm(formula = y ~ x + z, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7678 -0.5999 -0.0311 0.6150 2.9941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.25511 0.16116 -1.583 0.1141 ## x 0.07015 0.02318 3.027 0.0026 ** ## z 1.39556 0.07977 17.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9565 on 497 degrees of freedom ## Multiple R-squared: 0.4647, Adjusted R-squared: 0.4625 ## F-statistic: 215.7 on 2 and 497 DF, p-value: &lt; 2.2e-16 Cool, this is a run of the mill multiple regression. But remember that you cannot only fit our model, you must also summarise it, be that through summary.lm() or by any other means. We´ll get more in to lm() in the regression chapters. Let´s move on to a real big boy, lavaan. 5.5 Lavaan Lavaan, or, latent variable analysis, is an R package that we will use quite a bit. As the name implies it´s main function is to do latent variable analysis. If you do not know what latent variable analysis is, don´t worry, you soon will. The main thing that can be a bit tricky with lavaan is that it uses different kind of model specification than we usually do in simple analyses such as linear models. Contrary to those models, lavaan works in three steps. - first: you specify your model - second: you fit your model - third: you summarise the output of the model fit. If we compare this to the previous two step process of fitting an lm, it is the separation of model specification and model fitting that can take you for a loop. Lavaan models are specified as strings, that is, characters. These characters are then applied to a fitting function. To make things clearer, let´s play around with it a bit. We preciously specified the model of y ~ x + z. This is a character string that communicates that y is regressed on x + z. Lavaan works the same way, so if we wanted to fit a regression in lavaan we can use that exact string, the difference is that we specify the modal as a separate object from the fit. library(lavaan) #Step 1: specify the model model &lt;- (&#39;y ~ x + z&#39;) #Step 2: fit the specifid model with the sem function fit &lt;- sem(model = model, data = data) #Step 3: summarise the output summary(fit) ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 0.070 0.023 3.036 0.002 ## z 1.396 0.080 17.547 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.909 0.058 15.811 0.000 Now, if we look at the regression section in the bottom, we can see that the regression output is exactly the same as we would get if we did a normal lm regression. Let´s print out that regression we fitted earlier. summary.lm(fit.lm) ## ## Call: ## lm(formula = y ~ x + z, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7678 -0.5999 -0.0311 0.6150 2.9941 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.25511 0.16116 -1.583 0.1141 ## x 0.07015 0.02318 3.027 0.0026 ** ## z 1.39556 0.07977 17.495 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9565 on 497 degrees of freedom ## Multiple R-squared: 0.4647, Adjusted R-squared: 0.4625 ## F-statistic: 215.7 on 2 and 497 DF, p-value: &lt; 2.2e-16 And there you go, it´s very much the same. Now, why would you want to do all this other stuff just to fit a regression? well, you wouldnt. But this is not the point of lavaan, lavaan can specify very complex models ranging from latent variable models to multilevel models, and all this is done though the mode character string, also knowns as the lavaan syntax. Below I will give some examples of what type of models you can fit with lavaan. I will plot them out using a package called semPlot - but we will get to that in due time. library(semPlot) #A simple correlation model cor_model &lt;- (&#39; #Correlations are described by &quot;~~&quot; y ~~ x&#39; ) semPaths(sem(cor_model)) #A simple regression model reg_model &lt;- (&#39; #Regressions are described by &quot;~&quot; y ~ x&#39;) semPaths(sem(reg_model), rotation = 2) #A simple mediation model med_model &lt;- (&#39; y ~ x + z z ~ x&#39; ) semPaths(sem(med_model)) #a simple latent variable model sem_model &lt;- (&#39; #Latent variable as describe by &quot;=~&quot; y =~ a + b + c + d x =~ c + d + e + f &#39;) semPaths(sem(sem_model)) This is just a little taste of what lavaan can do. If your data can handle it, you can make incredibly interesting models. We will use lavaan for path analysis, mediation analysis, SEM and multi-level SEM. If you want to go Bayesian, there is even a Bayesian alternative under the name blavaan. Again, this is barely scratching the surface, and if you want to get deeper insight into the workings of lavaan i can recommend going straight to the source: https://lavaan.ugent.be/tutorial/index.html 5.6 semPlot The semPlot package is used to visual/plot SEMs, CFAs and path analyses. We have already used semPlot to graph the sem models above. Now, graphing sems can be quite annoying in the beginning. The documentation is very very extensive, but not super easy to understand (IMO) and before you have a basic understanding of the semPaths function reading the documentation might feel overwhelming. I would recommend going to this very concise video by the author of the package for a nice little overview: https://www.youtube.com/watch?v=rUUzF1_yaXg&amp;ab_channel=SachaEpskamp 5.7 ? Last, but certainly not least, the question mark. This might be the most useful little trick R has to offer. All packages/functions have built in documentation that explains it´s uses and intricacies. Most even have examples of how to use the function. I cannot stress enough how useful this is, not only is it good for solving problems in your code quickly, but all this documentation is a wealth of knowledge. Though brief, most if not all documentation has useful references and information that can broaden your knowledge of what you are actually doing. So how does it work? you simply type ? before a function. Let´s say we want to know more about how to fit lavaan models with the sem function, then we simple write: ?sem #Maybe we have issues generating correct correlations and have to revisit how to use rnorm_multi ?rnorm_multi #Perhaps just the rnorm? ?rnorm Make use of this tool! it can help you a lot. Also, I don´t like leaving a mess. So now that we are done, we should clean up. The fastest way of removing things in R is with the rm() function. This removes singular things from the environment. If we want to remove the coin_data we can simply write: rm(coin_data) And it´s gone. Now you will be glad that you have all your things documented neatly, that way you can load and remove things at the press of a button without having to worry about losing things. But if we want to clean everything out (as I often want) we can write the line: rm(list = ls()) To be completely honest I not know exactly what this line means, but it does a great job cleaning. "],["bayesian-and-frequentists-statistics.html", "6 Bayesian and Frequentists statistics 6.1 Likelihood functions 6.2 Density distributes 6.3 All hail the almighty Gauss", " 6 Bayesian and Frequentists statistics 6.1 Likelihood functions 6.2 Density distributes 6.3 All hail the almighty Gauss "],["non-parametric-statistics.html", "7 Non-parametric statistics 7.1 Asking all the question, making statements, and assuming 7.2 The old guard 7.3 Bootstrapping", " 7 Non-parametric statistics 7.1 Asking all the question, making statements, and assuming 7.2 The old guard 7.3 Bootstrapping "],["correlation-and-covariation-in-r.html", "8 Correlation and Covariation in R 8.1 Welcome to the matrix", " 8 Correlation and Covariation in R 8.1 Welcome to the matrix "],["regression-analysis.html", "9 Regression analysis 9.1 Univariate regression and multiple regression 9.2 Comparing means 9.3 The generalized linear model(logit) 9.4 Cross validation 9.5 LASSO (least absolute shrinkage and selection operator) 9.6 Ridge regression 9.7 Fully Bayesian regression 9.8 Poststratification", " 9 Regression analysis 9.1 Univariate regression and multiple regression Packages need to follow along with the code, special packages will be loaded when and if needed. library(tidyverse) library(car) library(haven) library(here) Regression is not always easily visualised. This here is a function that will visualise a univariate slope along with some of the most important measurements. This is a slight adaption from a function I found here: https://sejohnston.com/2012/08/09/a-quick-and-easy-function-to-plot-lm-results-in-r/ regplot &lt;- function (fit, name) { ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) + geom_jitter( alpha = .5, size = 2) + stat_smooth(method = &quot;lm&quot;) + labs(title = name, subtitle = paste(&quot;Adj R2 = &quot;,signif(summary(fit)$adj.r.squared, 5), &quot;Intercept =&quot;,signif(fit$coef[[1]],5 ), &quot; Slope =&quot;,signif(fit$coef[[2]], 5), &quot; P =&quot;,signif(summary(fit)$coef[4], 5)))+ theme_bw() } Field uses data from album sales. When loading data in r the best way is to put your data files in an easy to access place - note that this is my path, the data file is album sales from chapter 8 or 9 depending on edition so load that. data &lt;- read_sav(&quot;Album Sales.sav&quot;) #Let´s change the names to low caps so we can type easily. names(data) &lt;- c(&#39;adverts&#39;, &#39;sales&#39;, &#39;airplay&#39;, &#39;image&#39;) #Taking a peek at the data head(data) ## # A tibble: 6 x 4 ## adverts sales airplay image ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.3 330 43 10 ## 2 986. 120 28 7 ## 3 1446. 360 35 7 ## 4 1188. 270 33 7 ## 5 575. 220 44 5 ## 6 569. 170 19 5 summary(data) ## adverts sales airplay image ## Min. : 9.104 Min. : 10.0 Min. : 0.00 Min. : 1.00 ## 1st Qu.: 215.918 1st Qu.:137.5 1st Qu.:19.75 1st Qu.: 6.00 ## Median : 531.916 Median :200.0 Median :28.00 Median : 7.00 ## Mean : 614.412 Mean :193.2 Mean :27.50 Mean : 6.77 ## 3rd Qu.: 911.226 3rd Qu.:250.0 3rd Qu.:36.00 3rd Qu.: 8.00 ## Max. :2271.860 Max. :360.0 Max. :63.00 Max. :10.00 All variables are treated as numerical. Before any formal analysis can be pursued, we should first visually inspect the variables. #I usually prefer histograms without density curves but lets use them both to visualise the distribution of our data. ggplot(data, aes(adverts))+ geom_histogram(aes(y = ..density..), color = &#39;steelblue&#39;, size = 1)+ geom_density() ggplot(data, aes(sales))+ geom_histogram(aes(y = ..density..), color = &#39;steelblue&#39;, size = 1)+ geom_density() ggplot(data, aes(airplay))+ geom_histogram(aes(y = ..density..), color = &#39;steelblue&#39;, size = 1)+ geom_density() ggplot(data, aes(image))+ geom_histogram(aes(y = ..density..), color = &#39;steelblue&#39;, size = 1)+ geom_density() #looks pretty good, the ..density.. call is what gives the curve. So this is what we are working with, our IVs dont need to be normal, the residuals need to be normal however. We dont want heteroskedasticity. The neat thing with having multiple continuous variables is that it opens up plotting possibilities. Lets make some graphs ggplot(data = data, aes(y = sales, x = adverts , size = image, colour = airplay))+ geom_point(alpha = .8)+ theme_bw() #It is a bit messy, lets clean our plot by colouring by image. Lets first colour by images thats less than 5 ggplot(data = data, aes(y = sales, x = adverts , size = airplay, colour = image &lt; 5))+ geom_point(alpha = .5)+ theme_bw() #interesting, most bands have an image at 5 or above, and those that dont do not sell well regardless of their advert budget or image (the size of the points are roughly the same). Lets colour by image &lt;8 ggplot(data = data, aes(y = sales, x = adverts , size = airplay, colour = image &lt; 8))+ geom_point(alpha = .5)+ theme_bw() #Pretty graphs right? This is a better spread. But we can see that image does not seem to impact sales that much. There seems to be an effect, however. The most interesting thing with this graph however is that a high advert budget more or less promises good sales. No band with a budget less than 1 000 000 pounds have less than 100 000 album sales. A crude heuristic for bands wanting to fund their adverts with album sales could be to not sell their album for less than 10£. But this is of course bad advice. We have no information on the cost of the albums and sales are most likely affected by the cost of the album. Nevertheless, this is a fun graph. Lets make image into a factor so we can split our graphs better using the facet_wrap function plotdata &lt;- data plotdata$image &lt;- as.factor(plotdata$image) #Now image is a factor with to levels. Lets use the image of the band as colour ggplot(data = plotdata, aes(y = sales, x = adverts , size = airplay, colour = image))+ geom_point(alpha = .8)+ facet_wrap(~image)+ theme_bw() #Or ggplot(data = plotdata, aes(y = sales, x = adverts , colour = airplay))+ geom_point(size = 3, alpha = .8)+ facet_wrap(~image)+ theme_bw() #Note that one graph colours airplay while the other image Another interesting graph. Interestingly we can see that the band with the highest image has no advert budget. However, very few bands with that high image exists. The main idea this graph gives is that the slope between sales and advert budget does not seem to depend heavily on the image of the band. However, bands with low image often have less advert budget - and less sales. we can also see a relationship between airplay and sales, bigger points are usually a bit higher up on sales than small points. So what are our expectations given the graphs? we should expect an effect of image airplay and adverts on sales, but there will probably not be any meaningful interactions between them since no relationship was observable in the graphs. lets see if this holds up. our first model is a simple regression of adverts on album sales. We would expect more sales with more advert budget take a look at the lm function for syntax and code reference. ?lm #fitting the model m1 &lt;- lm(sales ~ adverts, data = data) One of the most important things we have to do when analysing data using statistical methods is checking that our assumptions for our method holds. For regression that: normality of residuals, independent errors, no multicollinearity, and homogeneity of variance in the residuals. Lets check these things out. #Checking the residuals and normality assumption ggplot(data = data, aes(y = predict.lm(m1), x = m1$residuals))+ geom_point(size = 3, alpha = .4)+ geom_smooth(method = &#39;lm&#39;) #Nice, the predicted values of our model is distributed quite normally across the residuals. This indicates that the variation in our model is roughly equal on low, medium, and high point estimates. We can also check the density of the #Residuals - they should be normal around 0. ggplot(data = data, aes(x = m1$residuals))+ geom_density() ggplot(data = data, aes(x = m1$residuals))+ geom_histogram(colour = &#39;steelblue&#39;, size = 1) #Nice, this means that our error in measurement isnt systematically lower or higher than our fitted model. #Checking error independence with Durbin Watson test durbinWatsonTest(m1) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.04394305 2.032324 0.772 ## Alternative hypothesis: rho != 0 #Nice, the p-value is insignificant and the test-statistic is close to 2. Now that we have checked our assumptions and have a good ground to stand on, we can check the fit of our model. #Checking the fit of the model summary.lm(m1) ## ## Call: ## lm(formula = sales ~ adverts, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.949 -43.796 -0.393 37.040 211.866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.341e+02 7.537e+00 17.799 &lt;2e-16 *** ## adverts 9.612e-02 9.632e-03 9.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 65.99 on 198 degrees of freedom ## Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 ## F-statistic: 99.59 on 1 and 198 DF, p-value: &lt; 2.2e-16 #Lets plot this manually in ggplot using geom_smooth, on our model ggplot(data, aes(y = sales, x = adverts))+ geom_point(alpha = .5, size = 2)+ geom_smooth(method = &#39;lm&#39;)+ theme_bw() #Geom smooth fits a linear model to the data, method = &#39;lm&#39; tells it that its a linear model. And there we have it, but by using the regplot function we created above we can put some of the info from the model on the plot. regplot(m1, &#39;m1&#39;) summary.lm(m1) ## ## Call: ## lm(formula = sales ~ adverts, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.949 -43.796 -0.393 37.040 211.866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.341e+02 7.537e+00 17.799 &lt;2e-16 *** ## adverts 9.612e-02 9.632e-03 9.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 65.99 on 198 degrees of freedom ## Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 ## F-statistic: 99.59 on 1 and 198 DF, p-value: &lt; 2.2e-16 #Nice graph with good info :) Pretty neat right? now, what can we make of this? the slope estimates of ca 0.1 indicates that sales increase by .1 for every unit of adverts. The equates to roughly 100 sales per thousand pounds in advertising budget. Also, note the shaded line. This is our marginal 95% confidence interval. We can see that our uncertainty in the parameter estimate increases at the mode extreme values since we dont have a lot of data regarding the dispersion on those levels of adverts. We should also check for influential cases analytically, not just visually. we can do this with the function lm.influence. summary(influence.measures(m1)) ## Potentially influential observations of ## lm(formula = sales ~ adverts, data = data) : ## ## dfb.1_ dfb.advr dffit cov.r cook.d hat ## 1 0.35 -0.27 0.35_* 0.93_* 0.06 0.01 ## 10 0.22 -0.15 0.22 0.97_* 0.02 0.01 ## 11 0.01 -0.02 -0.03 1.04_* 0.00 0.03_* ## 23 -0.02 0.04 0.04 1.05_* 0.00 0.03_* ## 42 0.29 -0.22 0.29 0.95_* 0.04 0.01 ## 43 -0.02 0.04 0.05 1.06_* 0.00 0.05_* ## 75 -0.02 0.04 0.05 1.03_* 0.00 0.02 ## 87 -0.06 0.11 0.12 1.05_* 0.01 0.05_* ## 88 0.02 -0.03 -0.03 1.05_* 0.00 0.04_* ## 92 -0.01 0.01 0.01 1.03_* 0.00 0.02 ## 93 -0.01 0.03 0.03 1.04_* 0.00 0.03 ## 113 -0.13 0.04 -0.17 0.96_* 0.01 0.01 ## 124 0.09 0.02 0.18 0.96_* 0.02 0.01 ## 126 0.05 -0.10 -0.11 1.03_* 0.01 0.02 ## 169 0.32 -0.23 0.33_* 0.92_* 0.05 0.01 ## 175 0.03 -0.07 -0.08 1.03_* 0.00 0.02 ## 184 0.08 -0.13 -0.13 1.08_* 0.01 0.06_* summary(cooks.distance(m1)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.000e-08 3.380e-04 1.576e-03 4.416e-03 6.314e-03 5.716e-02 #lets look at the hat values manually summary(hatvalues(m1)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.005000 0.005732 0.008030 0.010000 0.011305 0.063529 hatvalues(m1)%&gt;% sort.default(decreasing = TRUE) %&gt;% head(20) ## 184 43 87 88 23 11 93 126 ## 0.06352892 0.04590335 0.04502948 0.03687259 0.03442720 0.03108016 0.02569576 0.02435532 ## 175 28 55 46 62 75 199 92 ## 0.02353610 0.02344638 0.02334463 0.02201132 0.02170914 0.02170914 0.02170914 0.01997151 ## 3 128 20 102 ## 0.01971805 0.01894939 0.01751014 0.01656749 Going by cooks distance we dont seem to have any very influential values, that is, values exceeding 1. However, we have some values that exceeds three times the leverage or hatvalues. if we go by hoaglin and welch guides of looking at values exceeding three times the average, that is values exceeding .03. that is, observation 11, 23, 88, 87, 43 and 184. What happens if we remove them? clean_data &lt;- slice(data, -c(11, 23, 88, 87, 43, 184)) clean_m1 &lt;- lm(sales~adverts, clean_data) regplot(clean_m1, &#39;clean m1&#39;) summary.lm(clean_m1) ## ## Call: ## lm(formula = sales ~ adverts, data = clean_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.910 -44.484 -0.291 38.368 211.845 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 134.18426 8.00113 16.771 &lt; 2e-16 *** ## adverts 0.09596 0.01116 8.602 2.76e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 66.9 on 192 degrees of freedom ## Multiple R-squared: 0.2782, Adjusted R-squared: 0.2744 ## F-statistic: 73.99 on 1 and 192 DF, p-value: 2.763e-15 #. Does not change much in terms of estimation. We can compare the graphs using the grid arrange function from the gridExtra package library(gridExtra) m1_plot &lt;- regplot(m1, &#39;original model&#39;) clean_m1_plot &lt;- regplot(clean_m1, &#39;influential observations removed&#39;) grid.arrange(m1_plot, clean_m1_plot, name = c(&#39;original&#39;, &#39;clean&#39;)) #. Here we can easily see that the influential observations were the cases with very high advertising budgets. it might be prudent to use the updated model, but no model is better than the other. We should use the model that helps us answer our research question best - horses for courses A fast way, but less pleasing and not as hands on, is to simply use the plot function. It gives much of the same information plot(m1) plot(clean_m1) #. Note that the spread of fitted to residuals is much nicer when we exclude the influential observations. lets kick this up a notch and run a multiple regression on a split sample so that we can cross-validate our model. lets take all the variables in our data #Splitting our sample randomly using rbinom set.seed(123) data_partition &lt;- rbinom(200, 1, prob = .5) data &lt;- cbind(data, data_partition) rm(data_partition) #The binary variable &quot;data partition&quot; is our random divide of the sample in to 2. We will run the regression on one part of the sample and see how well it predicts the data in the second sample. lets call it training data and test data. training_data &lt;- filter(data, data_partition == 1) glimpse(training_data$data_partition) ## int [1:97] 1 1 1 1 1 1 1 1 1 1 ... test_data &lt;- filter(data, data_partition == 0) glimpse(test_data$data_partition) ## int [1:103] 0 0 0 0 0 0 0 0 0 0 ... Now we have our two samples, lets fit our training model. it works the same way as the simple regression, but we just add the additional IVs like you would in a normal regression formula. but first lets standardize the data so its more easily comparable #Standardising, or, &quot;scaling&quot; our data z_training_data &lt;- data.frame(scale(training_data)) z_test_data &lt;- data.frame(scale(test_data)) #Fitting the multiple regression training_model &lt;- lm(sales ~ adverts + image + airplay, data = z_training_data) Like for the simple regression we need to check our assumptions #Checking the residuals and normality assumtions ggplot(data = training_data, aes( y = predict.lm(training_model) , x = training_model$residuals))+ geom_point(size = 3, alpha = .4)+ geom_smooth(method = &#39;lm&#39;) #Looks good ggplot(data = training_data, aes(x = training_model$residuals))+ geom_density() ggplot(data = training_data, aes(x = training_model$residuals))+ geom_histogram(colour = &#39;steelblue&#39;, size = 1) #Nice #We need to check for multicollinearity when we do multiple regression. A cool way to do this is with GGally, but we can also just calculate VIF and tolerance(1/vif) vif(training_model) ## adverts image airplay ## 1.063294 1.066953 1.053305 1/vif(training_model) ## adverts image airplay ## 0.9404740 0.9372484 0.9493924 #Tolerance is simply 1 divided by the VIF. Our vif should be &lt;10 and our tolerance # &gt;.2. library(GGally) ggpairs(training_model) #Don´t worry about this, this is just a cool diagnostic tool with loads of info in a single plot with a very simple call. #Checking error independence with Durbin Watson test durbinWatsonTest(training_model) ## lag Autocorrelation D-W Statistic p-value ## 1 0.00282891 1.887567 0.586 ## Alternative hypothesis: rho != 0 #Still looks good, but notice that the test statistic is less close to 2. #Small sample = bad Now, lets check the fit of out model summary.lm(training_model) ## ## Call: ## lm(formula = sales ~ adverts + image + airplay, data = z_training_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4006 -0.2931 -0.0123 0.3958 1.3417 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.215e-16 6.004e-02 0.000 1.000000 ## adverts 4.901e-01 6.223e-02 7.876 6.16e-12 *** ## image 2.161e-01 6.234e-02 3.466 0.000802 *** ## airplay 4.666e-01 6.194e-02 7.532 3.18e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5913 on 93 degrees of freedom ## Multiple R-squared: 0.6613, Adjusted R-squared: 0.6503 ## F-statistic: 60.51 on 3 and 93 DF, p-value: &lt; 2.2e-16 regplot(training_model, &#39;training model&#39;) #. All estimates are significant, indicating that all variables contribute to the number of sales, we have also doubled the R2, we can explain much more of the variation with this model. Note also how the visuals suck. its hard to visualize multiple regression. But lets try some ways of visualising. One way is to use the broom package like so: library(broom) #We need to make image into a factor training_data$image &lt;- as.factor(training_data$image) #We then fit the model that we want to plot plot1 &lt;- lm(sales ~ adverts + image, data = training_data) #Then, using the augment function we can fit a slope to each level of &quot;image&quot; ggplot(augment(plot1), aes(y = sales, x = adverts, col = image))+ geom_point()+ geom_smooth(method = &#39;lm&#39;, se = FALSE)+ theme_bw() #. This is a visualization of the sales across advert budget grouped by the band image, we can see that bands with a bad image (1), have fever sales, but they also have less budget. But we also see differences in slope depending on image. the band with the best image has a flatter slope then the bands with slightly worse image. Note that we already know this from our previous graphs, and they are way less messy, this is not a very good graph I think. Again, visualizing multiple regression is hard, since the dimensions become hard to grasp after two or more variables, given that we care about interactions. One other way is though plotting marginal predictions or coefficients. This illustrates the regression output quite nicely I think, this is sometimes referred to as a simple slopes graph. #Lets extract the coefficients from the model cf &lt;- training_model$coef intercept &lt;- cf[1] adverts &lt;- cf[2] adverts &lt;- round(adverts, 4) image &lt;- cf[3] image &lt;- round(image, 4) airplay &lt;- cf[4] airplay &lt;- round(airplay, 4) #Here they are paste(adverts, image, airplay) ## [1] &quot;0.4901 0.2161 0.4666&quot; #This is a grid of values so that the plot has structure - this will be invisible in the graph statgrid &lt;- data.frame( y = seq(from = 0, to = adverts, length = 100) ,x = seq(from = 0, to = 1, length = 100)) #Plotting the slopes ggplot(data = statgrid, aes(x = x, y = y))+ geom_jitter(alpha = 0)+ geom_abline(intercept = intercept, slope = adverts , col = &#39;blue&#39;, size = 1)+ geom_abline(intercept = intercept, slope = image , col = &#39;red&#39;, size = 1)+ geom_abline(intercept = intercept, slope = airplay , col = &#39;green&#39;, size = 1)+ ylab(label = &#39;standardised sale value&#39;)+ xlab(label = &#39;dependent variable slope&#39;)+ theme_bw()+ labs(title = &#39;Adverts = blue | Image = red | Airplay = green&#39;) #Our simple slopes plot This visualization shows the slopes from the model. That is, the increase in units of sales when the dependent variable increases with one independently. I do not see this type of visualisation often but i quite like it. I think its a neat way of visualising a regression output. It is quite abstract though, since it requires that you scale you DVs so they can fit the same plot. Lastly, lets see if we have any significant interaction effects, this is a little teaser for what we will do later on in the moderation chapter but I think it might be a good idea to see if we have an interaction here. Going by the graphs earlier we should not expect anything big. # A third model that uses the interaction between image and adverts m3 &lt;- lm(sales ~ adverts*image + airplay, data = z_training_data) summary.lm(m3) ## ## Call: ## lm(formula = sales ~ adverts * image + airplay, data = z_training_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4197 -0.2856 0.0063 0.4046 1.3321 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.01604 0.06137 0.261 0.79438 ## adverts 0.49317 0.06214 7.937 4.87e-12 *** ## image 0.18976 0.06594 2.878 0.00498 ** ## airplay 0.45808 0.06220 7.365 7.36e-11 *** ## adverts:image -0.07987 0.06652 -1.201 0.23294 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5899 on 92 degrees of freedom ## Multiple R-squared: 0.6665, Adjusted R-squared: 0.652 ## F-statistic: 45.96 on 4 and 92 DF, p-value: &lt; 2.2e-16 #The interaction is not significant. lets go all out and fit a full interaction model - our fourth model m4 &lt;- lm(sales ~ adverts*image*airplay, data = z_training_data) summary.lm(m4) ## ## Call: ## lm(formula = sales ~ adverts * image * airplay, data = z_training_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.39859 -0.27268 -0.00666 0.38561 1.33742 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.011502 0.068353 0.168 0.8668 ## adverts 0.505923 0.070951 7.131 2.54e-10 *** ## image 0.185909 0.081013 2.295 0.0241 * ## airplay 0.445473 0.068210 6.531 3.93e-09 *** ## adverts:image -0.102109 0.086212 -1.184 0.2394 ## adverts:airplay 0.002962 0.084294 0.035 0.9720 ## image:airplay 0.074178 0.100145 0.741 0.4608 ## adverts:image:airplay 0.029568 0.131518 0.225 0.8226 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5975 on 89 degrees of freedom ## Multiple R-squared: 0.6691, Adjusted R-squared: 0.643 ## F-statistic: 25.71 on 7 and 89 DF, p-value: &lt; 2.2e-16 So, simply going by this the interaction terms does not do much for us. lets just stick with our original training model and see how well it predicts our test data. Lets extract our predictions #We need to load in the caret package for this (more on this later in the cross validation chapter) library(caret) # Make predictions and compute the R2, RMSE and MAE predictions &lt;- training_model %&gt;% predict(test_data) data.frame( R2 = R2(predictions, test_data$sales), RMSE = RMSE(predictions, test_data$sales), MAE = MAE(predictions, test_data$sales)) ## R2 RMSE MAE ## 1 0.318754 242.4139 189.1693 #We have an RMSE of 242, if the training model has a lower one, it fits better #We need to make image numeric for prediction purposes training_data$image &lt;- as.numeric(training_data$image) predictions.2 &lt;- training_model %&gt;% predict(training_data) data.frame( R2 = R2(predictions.2, training_data$sales), RMSE = RMSE(predictions.2, training_data$sales), MAE = MAE(predictions.2, training_data$sales)) ## R2 RMSE MAE ## 1 0.390846 227.6169 167.243 #So, not very unexpectedly, we have better fit on this model. #Let´s store some predictions for plotting #Prediction on the training data pred_train &lt;- training_model %&gt;% predict(z_training_data) #Prediction on the test data pred_test &lt;- training_model %&gt;% predict(z_test_data) Lets plot the predictions #For reference, this is what a perfect prediction would look like ggplot(data = data.frame(data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =sales, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;sales&#39;)+ labs(title = &#39;perfect prediction&#39;)+ theme_bw() #Training data prediction ggplot(data = data.frame(z_training_data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =pred_train, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;model predicted sales&#39;)+ theme_bw() #. This shows how well our model performs, the red line indicating perfect predictions that is, if we could estimate the number of sales perfectly. Which we clearly cant, let´s look at our out of sample prediction. ggplot(data = data.frame(z_test_data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =pred_test, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;model predicted sales&#39;)+ theme_bw() #. Lets compare all the plots using the grid arrange function again perfect_pred &lt;- ggplot(data = data.frame(scale(data)), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =sales, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;sales&#39;)+ labs(title = &#39;perfect prediction&#39;)+ theme_bw() insample_pred &lt;- ggplot(data = data.frame(z_training_data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =pred_train, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;predicted sales&#39;)+ labs(title = &#39;insample prediction&#39;)+ theme_bw() outsample_pred &lt;- ggplot(data = data.frame(z_test_data), aes(sales))+ geom_abline(intercept = 0, slope = 1, size = 2, col = &#39;red&#39;)+ geom_point(aes(y =pred_test, x = sales), size = 4, alpha = .5)+ ylab(label = &#39;predicted sales&#39;)+ labs(title = &#39;out of sample prediction&#39;)+ theme_bw() grid.arrange(perfect_pred, insample_pred, outsample_pred) #This last piece of code compiles the graphs And there we have it, this model seems pretty good. Nothing amazing, but pretty good. In reality out of sample predictions are usually worse, but since we have such a simple model with reasonable dependent variables, this is not that unexpected. We certainly dont have issues with over-fitting our model, which can cause it to make poor out of sample predictions. In this chapter we have gone through a little bit of everything, we have some prediction some moderation some cross validation and of course the simple regression analysis. In future chapters we will take a closer look at all of these methods, if you want to have a more real example of regression check out the last chapter where we use real messy data for various analyses. 9.2 Comparing means 9.2.1 A primer on means and effect sizes with simulation We should all be familiar with means, spreads, distributions by now. Lets use an example from Field to get a good look at how this comes together when we compare means. We will be using the invisibility cloak data since I think that´s funny. The question is, will an invisibility cloak make you more mischievous? library(tidyverse) library(here) library(faux) library(haven) invisibility &lt;- read_sav(here(&quot;data sets/Invisibility.sav&quot;)) #Checking the data structure str(invisibility) ## tibble [24 x 2] (S3: tbl_df/tbl/data.frame) ## $ Cloak : dbl+lbl [1:24] 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## ..@ label : chr &quot;Cloak of invisibility&quot; ## ..@ format.spss: chr &quot;F8.0&quot; ## ..@ labels : Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;No Cloak&quot; &quot;Cloak&quot; ## $ Mischief: num [1:24] 3 1 5 4 6 4 6 2 0 5 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Mischievous Acts&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.0&quot; #Renaming to lower caps names(invisibility) &lt;- c(&#39;cloak&#39;, &#39;mischief&#39;) #making invisibility a factor invisibility$cloak &lt;- as.factor(invisibility$cloak) #Taking some descriptive statistics using pastecs by(invisibility$mischief, invisibility$cloak, pastecs::stat.desc) ## invisibility$cloak: 0 ## nbr.val nbr.null nbr.na min max range ## 12.0000000 1.0000000 0.0000000 0.0000000 6.0000000 6.0000000 ## sum median mean SE.mean CI.mean.0.95 var ## 45.0000000 4.0000000 3.7500000 0.5521995 1.2153828 3.6590909 ## std.dev coef.var ## 1.9128750 0.5101000 ## ------------------------------------------------------------------- ## invisibility$cloak: 1 ## nbr.val nbr.null nbr.na min max range ## 12.0000000 0.0000000 0.0000000 2.0000000 8.0000000 6.0000000 ## sum median mean SE.mean CI.mean.0.95 var ## 60.0000000 5.0000000 5.0000000 0.4767313 1.0492785 2.7272727 ## std.dev coef.var ## 1.6514456 0.3302891 #Let´s plot it also so we can get a better look. #First, we look at the histograms for mischief for cloak and no cloak ggplot(data = filter(invisibility, cloak == 0))+ geom_histogram(aes(mischief), fill = &#39;darkred&#39;)+ labs(title = &#39;No cloak&#39;)+ theme_bw() ggplot(data = filter(invisibility, cloak == 1))+ geom_histogram(aes(mischief), fill = &#39;steelblue&#39;)+ labs(title = &#39;Cloak&#39;)+ theme_bw() invisibility%&gt;% ggplot(aes(x = mischief, colour = cloak))+ geom_density(size = 2, aes(fill = cloak), alpha = .3)+ theme_bw() #They are not distributed very differently; we should be able to compare these two groups alright. Let´s plot the median difference with violins and boxes. invisibility%&gt;% ggplot(aes(y = mischief, x = cloak, colour = cloak))+ geom_violin(aes(fill = cloak), alpha = .4)+ geom_boxplot(width = .3, colour = &#39;grey&#39;, size = 1)+ theme_bw() #Posessing a cloak seems to make someone slightly more mischievous. Let´s check if the difference is statistically significant using the t-test t &lt;- t.test(mischief ~ cloak, data = invisibility) t ## ## Welch Two Sample t-test ## ## data: mischief by cloak ## t = -1.7135, df = 21.541, p-value = 0.101 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -2.764798 0.264798 ## sample estimates: ## mean in group 0 mean in group 1 ## 3.75 5.00 library(effectsize) effectsize(t) ## Warning in effectsize.htest(t): Unable to retrieve data from htest object. Using t_to_d() ## approximation. ## d | 95% CI ## --------------------- ## -0.74 | [-1.60, 0.14] Seems like we dont have a very robust difference, lets see if we can simulate the distribution of the populations using the rnorm function and the mean and spread of mischief for cloak and no cloak. no_cloak &lt;- rnorm(n = 1e4, mean = 3.75, sd = 1.91) #Since we are working with ordinal scales, we need to truncate our values no_cloak &lt;- norm2trunc(no_cloak, min = 0) #We also need to round our values. no_cloak &lt;- round(no_cloak, 0) cloak &lt;- rnorm(n = 1e4, mean = 5, sd = 1.65) cloak &lt;- norm2trunc(cloak, min = 0) cloak &lt;- round(cloak, 0) #Creating a data set with our simulated values data &lt;- data.frame(no_cloak, cloak) names(data) = c(&#39;no_cloak&#39;, &#39;cloak&#39;) summary(data) ## no_cloak cloak ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 3.000 1st Qu.: 4.000 ## Median : 4.000 Median : 5.000 ## Mean : 3.864 Mean : 5.005 ## 3rd Qu.: 5.000 3rd Qu.: 6.000 ## Max. :11.000 Max. :11.000 #Plotting our simulated values ggplot(data)+ geom_histogram(aes(cloak), fill = &#39;red&#39;, alpha = .5)+ geom_histogram(aes(no_cloak), fill = &#39;blue&#39;, alpha = .5)+ xlab(&#39;Blue = no cloak. Red = cloak&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Looks pretty nice to me. Let´s run the t-test on the simulated values sim_t &lt;- t.test(x= no_cloak, y = cloak, data = data) sim_t ## ## Welch Two Sample t-test ## ## data: no_cloak and cloak ## t = -46.15, df = 19819, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.189252 -1.092348 ## sample estimates: ## mean of x mean of y ## 3.8638 5.0046 effectsize(t) ## d | 95% CI ## --------------------- ## -0.74 | [-1.60, 0.14] Notice how small the p-value is, this is due to our big sample. But the effect size is not significant, that is, we are not certain that an effect actually exists. If we have a large number of observations, the p-value is generally significant. This simulation is therefore quite bad. Let´s make a type of bootstrap simulation instead, but instead of using sampling from the data we make estimations using the rnorm function with the same sample size as our data, that is n = 12(per group) #We can use a for-loop for this. First, we create an object for storing our t-values. resamp_t &lt;- data.frame(t = NA) #Then we specify our loop. We will make 1000 t-tests and store the t-value in resamp_t set.seed(69) for (i in 1:1000){ no_cloak &lt;- rnorm(n = 12, mean = 3.75, sd = 1.91) no_cloak &lt;- norm2trunc(no_cloak, min = 0) no_cloak &lt;- round(no_cloak, 0) cloak &lt;- rnorm(n = 12, mean = 5, sd = 1.65) cloak &lt;- norm2trunc(cloak, min = 0) cloak &lt;- round(cloak, 0) t &lt;- t.test(x= no_cloak, y = cloak, data = data) resamp_t[i,] &lt;- c(t$statistic) } #Epic, let&#39;s plot out our t-values and put some lines at the critical values of t for our specific sample size. ggplot(resamp_t)+ geom_density(aes(t))+ geom_vline(xintercept = -2.048, linetype = &#39;dashed&#39;, size = 1)+ theme_bw() #Pretty cool right? The dashed line indicates all significant t-tests. We can get somethings thats similar to a power calculation from this by counting the number of significant test and dividing it by the number of tests we have done. What we get is the ratio of significant results. We can then say that &quot;assuming the effect exists, there is an x change to find it given our sample size&quot;. power &lt;- sum(resamp_t$t&lt;=-2.048)/length(resamp_t$t) #Note that this is NOT an a priori power calculation but given the sample size of the data we can only find the effect around 33% of the time, this is really bad. Even if we assume that such a big effect size that is observed exists, we are in big trubble. #Let´s compare our results with a normal power analysis pwr::pwr.t.test(n = 12, d = .74, sig.level = .05, type = &#39;two.sample&#39;, alternative = &#39;two.sided&#39;) ## ## Two-sample t test power calculation ## ## n = 12 ## d = 0.74 ## sig.level = 0.05 ## power = 0.4105285 ## alternative = two.sided ## ## NOTE: n is number in *each* group #The results are not the same but the certainly send the same message! In conclusion we can say that a cloak wont make you more mischievous, or at least we need some more evidence to come to that conclusion. I quite like using simulation as a means of approximating power. It puts the abstract concept of power on a more tangible level, that is, at what quantile does our presumed distribution of t-values reject the null hypothesis? even though this is not true power, i think it does a nice job of providing a practical visual. While we are on the topic of power, we can look at a common practice that can reduce our power quite a bit: median splits. I believe Field calls the devils work? lets find out if they are using simulation. #First, we create some data with faux, you can use my numbers or choose your own, they are arbitrary. set.seed(234) data &lt;- rnorm_multi( n = 400 #the number of obs , vars = 2 #the number of variables , mu = c(44, 23) #the mean of our variables , sd = c(8, 3) #the sd of our variables , r = .68 #the correlation of our variables , varnames = c(&#39;x&#39;, &#39;y&#39;))#the name of our vars cor(data) ## x y ## x 1.0000000 0.6897844 ## y 0.6897844 1.0000000 # Here we have a correlation of .689 sampled from a population correlation of .68 #lets see what happens when we split the data in to quantile splits. That is, divide them in 4 equal parts. data &lt;- data %&gt;% mutate(x_quantilegroup = ntile(x, 4)) %&gt;% mutate(y_quantilegroup = ntile(y, 4)) %&gt;% head() cor(data$x_quantilegroup, data$y_quantilegroup) ## [1] 0.8443566 #This is a very big correlation, probably spurious, lets simulate this to find out. We follow the same procedure as with the t-tests but lets do 5000 simulations instead. Let´s start with continuous r. r1 &lt;- data.frame(r1 = NA) set.seed(3452) for ( i in 1:5000){ data &lt;- rnorm_multi( n = 400 , vars = 2 , mu = c(44, 23) , sd = c(8, 3) , r = .68 , varnames = c(&#39;x&#39;, &#39;y&#39;)) r &lt;- data.frame(cor(data$x, data$y) ) r1[i,] &lt;- c(r) } #Now lets split the sample into quantiles - this will take a bit more time r2 &lt;- data.frame(r2 = NA) set.seed(435) for ( i in 1:5000){ data &lt;- rnorm_multi( n = 400 , vars = 2 , mu = c(44, 23) , sd = c(8, 3) , r = .68 , varnames = c(&#39;x&#39;, &#39;y&#39;)) data &lt;- data %&gt;% mutate(x_quantilegroup = ntile(x, 4)) %&gt;% mutate(y_quantilegroup = ntile(y, 4)) r &lt;- cor(data$x_quantilegroup,data$y_quantilegroup) r2[i,] &lt;- c(r) } r &lt;- data.frame(r1,r2) summary(r) ## r1 r2 ## Min. :0.5558 Min. :0.4660 ## 1st Qu.:0.6621 1st Qu.:0.5840 ## Median :0.6811 Median :0.6080 ## Mean :0.6801 Mean :0.6065 ## 3rd Qu.:0.6990 3rd Qu.:0.6300 ## Max. :0.7602 Max. :0.7180 #Lets visualise this. ggplot(r)+ geom_density(aes(r1), fill = &#39;blue&#39;, alpha = .5)+ geom_density(aes(r2), fill = &#39;red&#39;, alpha = .5)+ geom_vline(aes(xintercept = .68), size = 2, linetype = &#39;dashed&#39;)+ xlab(&#39;Red = corelation from quantlies. Blue = continus correlation. Dashed line = true effect size&#39;)+ theme_bw() #. They truly are the devil. We get the wrong effect size. This is something to think about. We should probably not reduce our sample with median/quantile splits if at all possible. A nice thing with simulations is that we have all the information. That is, we know the true effect size since we decide it ourselves. Another thing that Field harps on about is that the effect size of eta is less preferable than omega. I agree, but it´s not super easy to understand why. We can use simulations again to see how eta and omega behaves in different settings. Let´s do a play example and say we are testing school classes (on something), we dont expect a difference between all of them, but we do expect differences between some classes. Here is an example of a comparison between three classes. Note that this part will be a bit code heavy, use the ? function for the commands you want to take a closer look at :) #Let´s clean the environment before we start rm(list = ls()) k &lt;- 3 #number of classes n1 &lt;- 23 #student in class one n2 &lt;- 22 #students in class two n3 &lt;- 25 #students in class three N &lt;- n1 + n2 + n3 df_b &lt;- k - 1 df_e &lt;- N - k sst &lt;- 4000 #total sum of squares ssb &lt;- 500 #sum of squares between sse &lt;- sst-ssb #sum of squares error mse &lt;- sse/df_e #mean squared error ssb/sst #eta ## [1] 0.125 (ssb-(df_b*mse))/(sst+mse) #omega ## [1] 0.09760589 Pretty simple right? Let´s change the setting and make a data frame comparison of 14 classes where the sst and ssb increases by 5% for every new class included in the comparison. That is, the explained variance and the total variance increases the same amount - i.e., equally #Again, I like a clean environment, so i start with a clean slate rm(list = ls()) k &lt;- c(1:15) N &lt;- cumsum(rep(25, 15)) df_b &lt;- k - 1 df_e &lt;- N - k sst &lt;- cumprod(c(4000,rep(1.05, 14))) ssb &lt;- cumprod(c(500,rep(1.05, 14))) sse &lt;- sst-ssb mse &lt;- sse/df_e eta &lt;- ssb/sst omega &lt;- (ssb-(df_b*mse))/(sst+mse) effect_diff &lt;- eta-omega class_data_equal &lt;- data.frame(k, N, df_b, df_e, sst, ssb, sse, mse, eta, omega, effect_diff) class_data_equal &lt;- slice(class_data_equal, -1) #removing the first row View(class_data_equal) Lets plot this so we can see more clearly #Plotting both effect sizes ggplot(class_data_equal)+ geom_line(aes(y = omega, x = k), size = 1, colour = &#39;red&#39;)+ geom_line((aes(y = eta , x = k)),size = 1, colour = &#39;blue&#39;)+ scale_x_continuous(breaks = 1:14)+ ylab(&#39;Effect size&#39;)+ ggtitle(&#39;Omega = Red. Eta = Blue&#39;)+ theme_bw() #Plotting the difference between the effect sizes ggplot(class_data_equal)+ geom_smooth(aes(y = effect_diff, x = k))+ scale_x_continuous(breaks = 1:14)+ ylab(&#39;Effect size difference&#39;)+ theme_bw() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Pretty cool right? lets make it even worse and look at an example where the explained variance increases less than the increase in total variance. Lets say that with each additional class, sst increases with 5% and ssb with 2% sst &lt;- cumprod(c(4000,rep(1.05, 14))) ssb &lt;- cumprod(c(500,rep(1.02, 14))) sse &lt;- sst-ssb mse &lt;- sse/df_e eta &lt;- ssb/sst omega &lt;- (ssb-(df_b*mse))/(sst+mse) effect_diff &lt;- eta-omega class_data_unequal &lt;- data.frame(k, N, df_b, df_e, sst, ssb, sse, mse, eta, omega, effect_diff) class_data_unequal &lt;- slice(class_data_unequal, -1) #removing the first row View(class_data_unequal) lets plot this too ggplot(class_data_unequal)+ geom_line(aes(y = omega, x = k), size = 1, colour = &#39;red&#39;)+ geom_line((aes(y = eta , x = k)),size = 1, colour = &#39;blue&#39;)+ scale_x_continuous(breaks = 1:14)+ ylab(&#39;Effect size&#39;)+ ggtitle(&#39;Omega = Red. Eta = Blue&#39;)+ theme_bw() ggplot(class_data_unequal)+ geom_smooth(aes(y = effect_diff, x = k))+ scale_x_continuous(breaks = 1:14)+ ylab(&#39;Effect size&#39;)+ theme_bw() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Observe the slight difference in slope, lets look closer at this difference equal_unequal_diff &lt;- class_data_unequal$effect_diff - class_data_equal$effect_diff x.seq &lt;- seq(1:14) diffplot &lt;- data.frame(equal_unequal_diff, x.seq) ggplot(diffplot)+ geom_point(aes(y = equal_unequal_diff, x=x.seq) , size = 4, alpha = .5, colour = &#39;red&#39;)+ scale_x_continuous(breaks = 1:14)+ ylab(&#39;Effect size difference&#39;)+ xlab(&#39;df between&#39;)+ theme_bw() we see here that the difference between equal and unequal ssb/sst ratio increases as the number of comparisons increase. Another way of thinking about this could be through repeated measures. If we exchange class for time in the example above, we would make the same conclusion. That is because Omega is dependent on the sample size since it uses MSE for its estimation. We can simulate a correlation between two variables in various sample sizes to examine this closer. here we have 30 observations per group with a correlation of .6 data &lt;- rnorm_multi( n = 30 , vars = 2 , mu = c(33, 35) , sd = c(2 , 5 ) , r = .6 , varnames = c(&#39;x&#39;, &#39;y&#39;) , empirical = TRUE) cor(data) ## x y ## x 1.0 0.6 ## y 0.6 1.0 lm1 &lt;- aov(y ~ x, data = data) summary.aov(lm1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 261 261.00 15.75 0.000457 *** ## Residuals 28 464 16.57 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary.lm(lm1) ## ## Call: ## aov(formula = y ~ x, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.6671 -1.7540 0.1118 1.6817 9.8603 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.500 12.495 -1.160 0.255657 ## x 1.500 0.378 3.969 0.000457 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.071 on 28 degrees of freedom ## Multiple R-squared: 0.36, Adjusted R-squared: 0.3371 ## F-statistic: 15.75 on 1 and 28 DF, p-value: 0.0004571 effectsize(lm1, type = &#39;eta&#39;) ## # Effect Size for ANOVA ## ## Parameter | Eta2 | 95% CI ## ------------------------------- ## x | 0.36 | [0.13, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). effectsize(lm1, type = &#39;omega&#39;) ## # Effect Size for ANOVA ## ## Parameter | Omega2 | 95% CI ## --------------------------------- ## x | 0.33 | [0.11, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). If we drastically increase the sample size omega and eta will be equal data2 &lt;- rnorm_multi(n = 1e5 , vars = 2 , mu = c(33, 35) , sd = c(2 , 5 ) , r = .6 , varnames = c(&#39;x&#39;, &#39;y&#39;) , empirical = TRUE) cor(data2) ## x y ## x 1.0 0.6 ## y 0.6 1.0 lm2 &lt;- aov(y ~ x, data = data2) summary.aov(lm2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 899991 899991 56249 &lt;2e-16 *** ## Residuals 99998 1599984 16 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary.lm(lm2) ## ## Call: ## aov(formula = y ~ x, data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.1101 -2.6919 -0.0065 2.6867 17.0662 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.500000 0.209095 -69.35 &lt;2e-16 *** ## x 1.500000 0.006325 237.17 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4 on 99998 degrees of freedom ## Multiple R-squared: 0.36, Adjusted R-squared: 0.36 ## F-statistic: 5.625e+04 on 1 and 99998 DF, p-value: &lt; 2.2e-16 effectsize(lm2, type = &#39;eta&#39;) ## # Effect Size for ANOVA ## ## Parameter | Eta2 | 95% CI ## ------------------------------- ## x | 0.36 | [0.38, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). effectsize(lm2, type = &#39;omega&#39;) ## # Effect Size for ANOVA ## ## Parameter | Omega2 | 95% CI ## --------------------------------- ## x | 0.36 | [0.38, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). And if we reduce it to nothing they will differ greatly data3 &lt;- rnorm_multi(n = 5 , vars = 2 , mu = c(33, 35) , sd = c(2 , 5 ) , r = .6 , varnames = c(&#39;x&#39;, &#39;y&#39;) , empirical = TRUE) cor(data3) ## x y ## x 1.0 0.6 ## y 0.6 1.0 lm3 &lt;- aov(y ~ x, data = data3) summary.aov(lm3) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 36 36.00 1.687 0.285 ## Residuals 3 64 21.33 summary.lm(lm3) ## ## Call: ## aov(formula = y ~ x, data = data3) ## ## Residuals: ## 1 2 3 4 5 ## -5.0432 5.3418 -1.8607 -0.8544 2.4166 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.500 38.161 -0.380 0.729 ## x 1.500 1.155 1.299 0.285 ## ## Residual standard error: 4.619 on 3 degrees of freedom ## Multiple R-squared: 0.36, Adjusted R-squared: 0.1467 ## F-statistic: 1.687 on 1 and 3 DF, p-value: 0.2848 effectsize(lm3, type = &#39;eta&#39;) ## # Effect Size for ANOVA ## ## Parameter | Eta2 | 95% CI ## ------------------------------- ## x | 0.36 | [0.00, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). effectsize(lm3, type = &#39;omega&#39;) ## # Effect Size for ANOVA ## ## Parameter | Omega2 | 95% CI ## --------------------------------- ## x | 0.12 | [0.00, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). There we have it. Since omega uses the mean sum of squares it varies across sample sizes. Eta does not do this; therefore it only varies depending on the ratio of ssb and sst. This is why the eta is the same across data1 data2 and data3, while omega is very different Lets make a plot of eta and omega values on sample sizes from 5 to 200 per group. This might be labour intensive for your computer - but it should go fast :) #let´s clean up our environment first - we have a lot of stuff there now rm(list = ls()) omega &lt;- data.frame(&#39;parameter&#39; = NA, &#39;omega2&#39; = NA , &#39;ci&#39; = NA, &#39;ci_lo&#39; = NA, &#39;ci_hi&#39; = NA) for( i in 1:200){ data &lt;- data.frame(rnorm_multi(n = i+5 , vars = 2 , mu = c(0, 1) , sd = c(1 ,1 ) , r = .6 , empirical = TRUE , varnames = c(&#39;x&#39;, &#39;y&#39;))) x &lt;- data$x y &lt;- data$y lm1 &lt;- aov(x~y) o &lt;- effectsize(lm1, type = &#39;omega&#39;) omega[i,] &lt;- o } eta &lt;- data.frame(&#39;parameter&#39; = NA, &#39;eta&#39; = NA , &#39;ci&#39; = NA, &#39;ci_lo&#39; = NA, &#39;ci_hi&#39; = NA) for( i in 1:200){ data &lt;- data.frame(rnorm_multi(n = i+4 , vars = 2 , mu = c(0, 1) , sd = c(1 ,1 ) , r = .6 , empirical = TRUE , varnames = c(&#39;x&#39;, &#39;y&#39;))) x &lt;- data$x y &lt;- data$y lm1 &lt;- aov(x~y) e &lt;- effectsize(lm1, type = &#39;eta&#39;) eta[i,] &lt;- e } omega_eta &lt;- data.frame(omega$omega2, eta$eta, (5:204)) names(omega_eta) &lt;- c(&#39;omega&#39;, &#39;eta&#39;, &#39;sample_size&#39;) ggplot(omega_eta)+ geom_smooth(aes(x = sample_size, y = omega), colour = &#39;red&#39;)+ geom_smooth(aes(x = sample_size, y = eta))+ scale_x_continuous(breaks = c(5, 20, 30, 50, 75, 100, 150, 200))+ ylab(&#39;Effect size&#39;)+ xlab(&#39;n per group&#39;)+ ggtitle(&#39;Omega = Red. Eta = Blue&#39;)+ theme_bw() This shows the relationship between eta and omega quite nicely I think. It also shows how Eta can be quite a problematic effect size if the sample is small. A disclaimer to this section is that I am not a very mathy guy and I might be doing some mistakes in these simulations. I would have liked to cross reference it somewhere, but I have not seen anyone doing anything similar. 9.2.2 ANOVA The Field book places a lot of focus on the glm or the general linear model. Confusingly, the function glm() in R does not refer to the general linear model but the generalised linear model, which as a broader mode of model specification that we will get to later. When Field talks about the general linear model he usually talks about analysis of variance. I do not see any major advantages of using ANOVA (or ANVCOVA) instead of multiple regression. Due to this bias of mine, I will leave this chapter quite bare bones, since i think that you could just use regression instead or formulate some more sophisticated analysis (that we will get to later). This is not due to some long-standing grudge against ANOVA (I used ANOVA in my BA thesis) but simply because I think regression is a better mode of analysis most of the time. With that said, ANOVA output should be familiar to you since we got an ANOVA output in the previous chapter on regression. If we only want the ANOVA and not coefficients like in regression, we use the aov() function instead of the lm() function. Quite simple right? The one thing with ANOVA that I absolutely adore is the plotting. Since we are using factors as dependent variables, we can make some lovely plots including my favourite: violin plots. So, let´s load some data from the Field book and make some plots. #I believe Field uses the beer goggle data, so let´s load that along with the packages we need. library(tidyverse) library(haven) goggles &lt;- read_sav(&quot;data sets/Goggles.sav&quot;) library(MVN) #for diagnostics and descriptives library(multcomp) #for multiple comparisons library(effectsize) #for effectsize calculation Nice, let´s check the structure of the data and take some descriptives str(goggles) ## tibble [48 x 3] (S3: tbl_df/tbl/data.frame) ## $ FaceType : dbl+lbl [1:48] 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## ..@ label : chr &quot;Attractivenss of facial stimuli&quot; ## ..@ format.spss : chr &quot;F8.0&quot; ## ..@ display_width: int 11 ## ..@ labels : Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Unattractive&quot; &quot;Attractive&quot; ## $ Alcohol : dbl+lbl [1:48] 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,... ## ..@ label : chr &quot;Alcohol consumption&quot; ## ..@ format.spss: chr &quot;F8.0&quot; ## ..@ labels : Named num [1:3] 0 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Placebo&quot; &quot;Low dose&quot; &quot;High dose&quot; ## $ Attractiveness: num [1:48] 6 7 6 7 6 5 8 6 7 6 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Median attractiveness rating&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 13 diagnostics &lt;- mvn(data = goggles #the data , mvnTest = &#39;mardia&#39; #what mvn test to use , univariateTest = &#39;SW&#39; #what univariate test(shapiro/wilks) , univariatePlot = &#39;qq&#39; #what plot: QQ plot , multivariatePlot = &#39;qq&#39;# what plot: QQ plot , multivariateOutlierMethod = &#39;quan&#39; #what method - mahalanobis , showNewData = T) #generating new outlier free data #We have some issues with normality, but we never care about that anyways. Let´s check the descriptives diagnostics$Descriptives ## n Mean Std.Dev Median Min Max 25th 75th Skew Kurtosis ## FaceType 48 0.500000 0.5052912 0.5 0 1 0 1 0.000000 -2.0412326 ## Alcohol 48 1.000000 0.8251370 1.0 0 2 0 2 0.000000 -1.5618490 ## Attractiveness 48 5.666667 1.5889691 6.0 1 8 5 7 -0.735088 0.3320222 #Note that they look very bad, it´s not what we are interested in at all. We need to use grouping coding as well as changing the structure of the data to factors. Cool, we have numeric values on attractiveness signifying that it is our dependent variable, while alcohol and facetype is our independent variables. However, they are not coded as factors, we need to change this, so lets do that and make some plots to examine the relationship between our variables. fac.alc &lt;- as.factor(goggles$Alcohol) #Factor of the alcohol variable fac.face &lt;- as.factor(goggles$FaceType) #Factor of the facetype variable goggles &lt;- cbind(goggles, fac.alc, fac.face) #Binding the factors to the data rm(fac.alc, fac.face) #Removing the factors from env goggles %&gt;% ggplot(aes(y = Attractiveness, x = fac.alc))+ geom_violin(aes(fill = fac.alc, alpha = .3))+ geom_boxplot(width = .3, fill = &#39;grey&#39;, alpha = .6, size = 1)+ theme_bw() #We see a clear difference in variation, while the median is similar across values of alcohol, the spread gets smaller as alcohol increases. goggles %&gt;% ggplot(aes(y = Attractiveness, x = fac.alc, colour = fac.face))+ geom_boxplot(width = .3, fill = &#39;grey&#39;, alpha = .6, size = 1)+ theme_bw() #Right, here we see that facetype influences the relationship between alcohol and attractiveness. Face type 1 does not differ across alcohol treatments. Cool, let´s take some better descriptive statistics and make some group comparisons, we can use the pastecs package for this. by(goggles$Attractiveness, goggles$fac.face, pastecs::stat.desc) ## goggles$fac.face: 0 ## nbr.val nbr.null nbr.na min max range ## 24.0000000 0.0000000 0.0000000 1.0000000 8.0000000 7.0000000 ## sum median mean SE.mean CI.mean.0.95 var ## 120.0000000 5.0000000 5.0000000 0.3710541 0.7675839 3.3043478 ## std.dev coef.var ## 1.8177865 0.3635573 ## ------------------------------------------------------------------- ## goggles$fac.face: 1 ## nbr.val nbr.null nbr.na min max range ## 24.0000000 0.0000000 0.0000000 5.0000000 8.0000000 3.0000000 ## sum median mean SE.mean CI.mean.0.95 var ## 152.0000000 6.0000000 6.3333333 0.1965893 0.4066759 0.9275362 ## std.dev coef.var ## 0.9630868 0.1520663 #This is a pretty good way of getting descriptives. For example, we see that face type 0 has a mean attractiveness of 5 and sd of 1.8 while facetype 2 has mean attractiveness of 6.3 and sd of .9. by(goggles$Attractiveness, goggles$fac.alc, pastecs::stat.desc) ## goggles$fac.alc: 0 ## nbr.val nbr.null nbr.na min max range ## 16.0000000 0.0000000 0.0000000 1.0000000 8.0000000 7.0000000 ## sum median mean SE.mean CI.mean.0.95 var ## 79.0000000 5.5000000 4.9375000 0.4870725 1.0381704 3.7958333 ## std.dev coef.var ## 1.9482898 0.3945903 ## ------------------------------------------------------------------- ## goggles$fac.alc: 1 ## nbr.val nbr.null nbr.na min max range ## 16.0000000 0.0000000 0.0000000 3.0000000 8.0000000 5.0000000 ## sum median mean SE.mean CI.mean.0.95 var ## 91.0000000 6.0000000 5.6875000 0.3381167 0.7206787 1.8291667 ## std.dev coef.var ## 1.3524669 0.2377964 ## ------------------------------------------------------------------- ## goggles$fac.alc: 2 ## nbr.val nbr.null nbr.na min max range ## 16.0000000 0.0000000 0.0000000 5.0000000 8.0000000 3.0000000 ## sum median mean SE.mean CI.mean.0.95 var ## 102.0000000 6.0000000 6.3750000 0.2719528 0.5796537 1.1833333 ## std.dev coef.var ## 1.0878113 0.1706371 #Note that we dont learn much more from the descriptives than we do from the plots. Before doing our ANOVA we should check the homogeneity of variance. We do this with the levensTest from the car package like we did with the regression. car::leveneTest(y = goggles$Attractiveness , interaction(goggles$fac.alc, goggles$fac.face)) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 5 0.7171 0.6142 ## 42 #Looks good. the interaction function makes the levens tast compare all possible groups to check if the variance is different across the groups. I expected it to be bigger since the plots showed discrepancy in variation. We can check the variation difference for the factors singularly as well, lets do that. car::leveneTest(y = goggles$Attractivenes, group = goggles$fac.alc) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 2.1847 0.1243 ## 45 car::leveneTest(y = goggles$Attractivenes, group = goggles$fac.face) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 6.4 0.0149 * ## 46 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #This is more what I expected, since the sd/variance was quite different we sould expect levens to be significant. Let´s fit our model. Since we want to fit a full factorial ANOVA we can assume to have met our assumption of homogeneity of variance. model.1 &lt;- aov(Attractiveness ~ fac.face*fac.alc, data = goggles) Good, now we have our ANOVA, we can examine it by using the summar.aov function or the ANOVA function. They have some different properties to them but are more or less the same. summary.aov(model.1, intercept = T) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## (Intercept) 1 1541.3 1541.3 1125.843 &lt; 2e-16 *** ## fac.face 1 21.3 21.3 15.583 0.000295 *** ## fac.alc 2 16.5 8.3 6.041 0.004943 ** ## fac.face:fac.alc 2 23.3 11.6 8.507 0.000791 *** ## Residuals 42 57.5 1.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(model.1) ## Analysis of Variance Table ## ## Response: Attractiveness ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fac.face 1 21.333 21.3333 15.5826 0.0002952 *** ## fac.alc 2 16.542 8.2708 6.0413 0.0049434 ** ## fac.face:fac.alc 2 23.292 11.6458 8.5065 0.0007913 *** ## Residuals 42 57.500 1.3690 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #see ?summary.aov and ?anova for different uses. I personally prefer summary.aov Right, we see significant differences between all variables along with a significant interaction term, indicating that the relationship between the IV and DVs are dependent on the values of the other DV. We cannot say which variable that causes the difference. Let´s take a closer look at what levels of the variables that drives the significance and the effect sizes. TukeyHSD(model.1) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Attractiveness ~ fac.face * fac.alc, data = goggles) ## ## $fac.face ## diff lwr upr p adj ## 1-0 1.333333 0.6516897 2.014977 0.0002952 ## ## $fac.alc ## diff lwr upr p adj ## 1-0 0.7500 -0.2550324 1.755032 0.1777260 ## 2-0 1.4375 0.4324676 2.442532 0.0033704 ## 2-1 0.6875 -0.3175324 1.692532 0.2317125 ## ## $`fac.face:fac.alc` ## diff lwr upr p adj ## 1:0-0:0 2.875 1.128535277 4.6214647 0.0001939 ## 0:1-0:0 1.375 -0.371464723 3.1214647 0.1974738 ## 1:1-0:0 3.000 1.253535277 4.7464647 0.0000979 ## 0:2-0:0 3.125 1.378535277 4.8714647 0.0000491 ## 1:2-0:0 2.625 0.878535277 4.3714647 0.0007414 ## 0:1-1:0 -1.500 -3.246464723 0.2464647 0.1291660 ## 1:1-1:0 0.125 -1.621464723 1.8714647 0.9999339 ## 0:2-1:0 0.250 -1.496464723 1.9964647 0.9980556 ## 1:2-1:0 -0.250 -1.996464723 1.4964647 0.9980556 ## 1:1-0:1 1.625 -0.121464723 3.3714647 0.0812069 ## 0:2-0:1 1.750 0.003535277 3.4964647 0.0492763 ## 1:2-0:1 1.250 -0.496464723 2.9964647 0.2889504 ## 0:2-1:1 0.125 -1.621464723 1.8714647 0.9999339 ## 1:2-1:1 -0.375 -2.121464723 1.3714647 0.9871315 ## 1:2-0:2 -0.500 -2.246464723 1.2464647 0.9550187 effectsize(model = model.1, type = &#39;omega&#39;) ## # Effect Size for ANOVA (Type I) ## ## Parameter | Omega2 (partial) | 95% CI ## -------------------------------------------------- ## fac.face | 0.23 | [0.07, 1.00] ## fac.alc | 0.17 | [0.02, 1.00] ## fac.face:fac.alc | 0.24 | [0.06, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). effectsize(model = model.1, type = &#39;eta&#39;) ## # Effect Size for ANOVA (Type I) ## ## Parameter | Eta2 (partial) | 95% CI ## ------------------------------------------------ ## fac.face | 0.27 | [0.10, 1.00] ## fac.alc | 0.22 | [0.05, 1.00] ## fac.face:fac.alc | 0.29 | [0.10, 1.00] ## ## - One-sided CIs: upper bound fixed at (1). #To get a multiple r2 we can fit a regression model like so: summary.lm(lm(Attractiveness ~ fac.face * fac.alc, data = goggles)) ## ## Call: ## lm(formula = Attractiveness ~ fac.face * fac.alc, data = goggles) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.500 -0.625 -0.125 0.625 2.500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.5000 0.4137 8.461 1.29e-10 *** ## fac.face1 2.8750 0.5850 4.914 1.41e-05 *** ## fac.alc1 1.3750 0.5850 2.350 0.023531 * ## fac.alc2 3.1250 0.5850 5.342 3.49e-06 *** ## fac.face1:fac.alc1 -1.2500 0.8274 -1.511 0.138319 ## fac.face1:fac.alc2 -3.3750 0.8274 -4.079 0.000197 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.17 on 42 degrees of freedom ## Multiple R-squared: 0.5154, Adjusted R-squared: 0.4578 ## F-statistic: 8.936 on 5 and 42 DF, p-value: 7.677e-06 #Note also that we get coefficients - I think this is a more intuitive way to model. The Tukey function is a very neat function for ANOVA. There is significant difference between placebo conditions and high alcohol doses, as well as on various levels of interactions between the factors. We can explain quite a bit of variation as well which is nice. This is all the ANOVA I have in me to give. If it is not evident by now, I do not like ANOVA (on its own) very much. There are many more things you can do with the ANOVA analysis along with contrasts, robust measures, and interaction plots. I advise you to look for these things elsewhere since I don´t find them very fun - especially since we can use other tools that are more fun, like multiple regression. 9.3 The generalized linear model(logit) Packages we need to do some logits library(tidyverse) library(haven) library(here) library(car) Let´s load the data. We use the data from field again - that is, the eel data. data &lt;- read_sav(here(&quot;data sets/Eel.sav&quot;)) names(data) &lt;- c(&#39;cured&#39;, &#39;intervention&#39;, &#39;duration&#39;) Let´s have a look at our variables str(data) ## tibble [113 x 3] (S3: tbl_df/tbl/data.frame) ## $ cured : dbl+lbl [1:113] 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1... ## ..@ label : chr &quot;Cured?&quot; ## ..@ format.spss : chr &quot;F8.0&quot; ## ..@ display_width: int 13 ## ..@ labels : Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Not Cured&quot; &quot;Cured&quot; ## $ intervention: dbl+lbl [1:113] 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1... ## ..@ label : chr &quot;Intervention&quot; ## ..@ format.spss : chr &quot;F8.0&quot; ## ..@ display_width: int 14 ## ..@ labels : Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;No Treatment&quot; &quot;Intervention&quot; ## $ duration : num [1:113] 7 7 6 8 7 6 7 7 8 7 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Number of Days with Problem before Treatment&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## - attr(*, &quot;label&quot;)= chr &quot;File created by MATRIX&quot; data %&gt;% ggplot(aes(x = duration))+ geom_histogram()+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Looks very nice, first we will do a univariate logistic regression and then add a the dummy variable of intervention. We formulate our model the same way as in the normal regression but use the glm function instead of the lm function. Since we are using a glm, we also need to specify what link function we will use, we will use logit since, wellthis is a chapter on logit. m1 &lt;- glm( formula = cured ~ duration , data = data , family = binomial(link = &#39;logit&#39;)) summary.glm(m1) ## ## Call: ## glm(formula = cured ~ duration, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4747 -1.3049 0.9548 1.0551 1.1603 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.5951 1.1680 -0.510 0.610 ## duration 0.1271 0.1634 0.778 0.436 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 154.08 on 112 degrees of freedom ## Residual deviance: 153.47 on 111 degrees of freedom ## AIC: 157.47 ## ## Number of Fisher Scoring iterations: 4 Interesting, the duration does not seem to increase the probability of being cured. Let´s check if the intervention alone increases the probability of being cured. m2 &lt;- glm( formula = cured ~ intervention , data = data , family = binomial(link = &#39;logit&#39;)) summary.glm(m2) ## ## Call: ## glm(formula = cured ~ intervention, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5940 -1.0579 0.8118 0.8118 1.3018 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.2877 0.2700 -1.065 0.28671 ## intervention 1.2287 0.3998 3.074 0.00212 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 154.08 on 112 degrees of freedom ## Residual deviance: 144.16 on 111 degrees of freedom ## AIC: 148.16 ## ## Number of Fisher Scoring iterations: 4 It certainly does, let´s fit a multivariate logistic regression using both variables. m3 &lt;- glm( formula = cured ~ duration + intervention , data = data , family = binomial(link = &#39;logit&#39;)) summary.glm(m3) ## ## Call: ## glm(formula = cured ~ duration + intervention, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6025 -1.0572 0.8107 0.8161 1.3095 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.234660 1.220563 -0.192 0.84754 ## duration -0.007835 0.175913 -0.045 0.96447 ## intervention 1.233532 0.414565 2.975 0.00293 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 154.08 on 112 degrees of freedom ## Residual deviance: 144.16 on 110 degrees of freedom ## AIC: 150.16 ## ## Number of Fisher Scoring iterations: 4 No, duration does not matter even when we adjust for the intervention. We might have an interaction though, so let´s fit a model with interactions m4 &lt;- glm( formula = cured ~ duration*intervention , data = data , family = binomial(link = &#39;logit&#39;)) summary.glm(m4) ## ## Call: ## glm(formula = cured ~ duration * intervention, family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6510 -1.0611 0.8045 0.8412 1.3285 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.52033 1.68409 -0.309 0.757 ## duration 0.03436 0.24540 0.140 0.889 ## intervention 1.85049 2.53509 0.730 0.465 ## duration:intervention -0.08694 0.35200 -0.247 0.805 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 154.08 on 112 degrees of freedom ## Residual deviance: 144.09 on 109 degrees of freedom ## AIC: 152.09 ## ## Number of Fisher Scoring iterations: 4 Huh, if we include an interaction term no variables are significant predictors. Let´s stick with the model without the interaction and check for any influential observations summary(influence.measures(m3)) ## Potentially influential observations of ## glm(formula = cured ~ duration + intervention, family = binomial(link = &quot;logit&quot;), data = data) : ## ## dfb.1_ dfb.drtn dfb.intr dffit cov.r cook.d hat ## 47 -0.27 0.24 0.02 -0.28 1.08_* 0.02 0.08 ## 111 -0.27 0.24 0.02 -0.28 1.08_* 0.02 0.08 mean(hatvalues(m3))*3 ## [1] 0.07964602 #observation 47 and 111 are highly impactfull. Lets have a look at the residuals summary(scale(m3$residuals)) ## V1 ## Min. :-1.6907 ## 1st Qu.:-0.8188 ## Median : 0.6503 ## Mean : 0.0000 ## 3rd Qu.: 0.6532 ## Max. : 1.1034 res &lt;- scale(m3$residuals) head(arrange(tibble(res), desc(res)), 10) ## # A tibble: 10 x 1 ## res[,1] ## &lt;dbl&gt; ## 1 1.10 ## 2 1.10 ## 3 1.10 ## 4 1.10 ## 5 1.10 ## 6 1.09 ## 7 1.09 ## 8 1.09 ## 9 1.09 ## 10 1.09 #They are quite small = good. I don´t think we need to exclude any variables, so let´s check our assumptions and then make you model interpretable. #Checking independence durbinWatsonTest(m3) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.1881671 2.363961 0.062 ## Alternative hypothesis: rho != 0 This is not very good. The statistic is approaching significance, it is close to two however. Lets check the VIF and tolerance. vif(m3) ## duration intervention ## 1.075431 1.075431 1/vif(m3) ## duration intervention ## 0.9298601 0.9298601 Very solid, no issues here. Lets finally go through the linearity assumption. We can use the box-tidwell test for this. Or just check visually. Lets do the latter  I like that better. logit &lt;- logit(fitted.values(m3)) df &lt;- data.frame(data$cured, data$duration, data$intervention) %&gt;% mutate(logit = logit(fitted.values(m3))) %&gt;% gather(key = &quot;predictors&quot;, value = &quot;predictor.value&quot;, -logit) ## Warning: attributes are not identical across measure variables; ## they will be dropped ggplot(df, aes(logit, predictor.value))+ geom_point(size = 0.5, alpha = 0.5) + geom_smooth(method = &quot;loess&quot;) + theme_bw() + facet_wrap(~predictors, scales = &quot;free_y&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; No good, duration is not linear to the logit, we should drop it. We won´t be doing much harm to our model anyways. Thus, let´s continue with model 2(m2). Now we want to interpret the model, I like to use absolute probabilities. This is not appropriate in this scenario since we dont have a continuous variable. prob &lt;- predict.glm(m3, type = &#39;response&#39;) data &lt;- cbind(data, prob) ggplot(data = data, aes(x=duration, y = prob))+ stat_smooth(method=&quot;glm&quot; , se=T , method.args = list(gaussian(link = &#39;logit&#39;)) , colour = &#39;pink&#39; , size = 2)+ geom_jitter(alpha=.5, height = 0) + ylim(c(0,1))+ theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; #. Coolio, here we see the absolute probability of being cured as the duration after the treatment increases given that we adjust for the treatment effect. Note that the effect is not significant, the shaded 95% confidence interval is overlapping across values of duration. If we do the same plot on intervention, we get a graph with two values on the x-axis like so: ggplot(data = data, aes(x=intervention, y = prob))+ stat_smooth(method=&quot;glm&quot; , se=T , method.args = list(gaussian(link = &#39;logit&#39;)) , colour = &#39;pink&#39; , size = 2)+ geom_jitter(alpha=.5, height = 0) + ylim(c(0,1))+ theme_bw() ## Don&#39;t know how to automatically pick scale for object of type haven_labelled/vctrs_vctr/double. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; #Note. The dots/jitters on the graphs are just the observations. I keep them there because they emphasize that the outcome is binary. We can also get the odds ratios taking the exponent of the logit like so: exp(coef(glm(formula = cured ~ duration + intervention, family = binomial(link = &quot;logit&quot;), data = data))) ## (Intercept) duration intervention ## 0.7908401 0.9921954 3.4333349 #Or simply accessing the coefficients from the fitted model, I prefer this latter way :) exp(m3$coefficients) ## (Intercept) duration intervention ## 0.7908401 0.9921954 3.4333349 Here we see that the likelihood of being cured is three times as likely for those partaking in the treatment. The duration does not matter. We can also make a classification table - how many times our model makes correct classifications on the dependent variable. classification &lt;- data.frame( respons = data$cured #what is the actual outcome? , predicted = round(fitted.values(m3),0)) #what is our guess? xtabs(~ predicted + respons, data = classification) ## respons ## predicted 0 1 ## 0 32 24 ## 1 16 41 #How many predictions were correct? correct_prediction.1 &lt;- 32+41 #What percentage is correct? correct_prediction.1/length(data$cured) ## [1] 0.6460177 This is not a very good model. This model only makes correct classification 65% of the time. This is not much better than chance. This marks the end for logistic regression for now. Hopefully there is a good example of logistic regression in the wild in the chapter on real data (which don´t exist as of now). This example is quite boring in the sense that not much model building is available - we dont have a lot of variables. 9.4 Cross validation Logistic regression is the first machine learning algorithm that we have employed looking past the normal univariate and multivariate regression. A common way to cross validate is to split a sample in two and try to predict out-of-sample data with a model specified within sample. This is probably the best way, but it is resource intensive in the sense that you cant train your model on the entire data set. Another way is using k-folds cross validation. This is a machine learning technique where you run multiple analyses on a k-number of partitions on your data. It is very interesting and something to look further into if you want to produce replicable results - which is important. Lets make a k-folds cross validation on the logit model we produced in the previous chapter. We need to formulate our training model again for it to run smoothly. We can use the caret package for this. library(caret) lets respecify our data as our training data, that is, the data that we train our model on in order to make out of sample predictions. train_data &lt;- data train_data$cured &lt;- as.factor(train_data$cured) Let´s specify the model again train_control &lt;- trainControl(method = &quot;cv&quot; #cv stands for cross validation , number = 10) #this is the number of particions # This is the same model as m2. But specified to do a k-folds cross validation. m3_train &lt;- train(cured ~ duration + intervention, data = train_data, trControl = train_control, method = &quot;glm&quot;, family=binomial(link = &#39;logit&#39;)) summary(m3_train) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6025 -1.0572 0.8107 0.8161 1.3095 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.234660 1.220563 -0.192 0.84754 ## duration -0.007835 0.175913 -0.045 0.96447 ## intervention 1.233532 0.414565 2.975 0.00293 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 154.08 on 112 degrees of freedom ## Residual deviance: 144.16 on 110 degrees of freedom ## AIC: 150.16 ## ## Number of Fisher Scoring iterations: 4 summary(m3_train$results) ## parameter Accuracy Kappa AccuracySD KappaSD ## Length:1 Min. :0.6465 Min. :0.2873 Min. :0.1336 Min. :0.265 ## Class :character 1st Qu.:0.6465 1st Qu.:0.2873 1st Qu.:0.1336 1st Qu.:0.265 ## Mode :character Median :0.6465 Median :0.2873 Median :0.1336 Median :0.265 ## Mean :0.6465 Mean :0.2873 Mean :0.1336 Mean :0.265 ## 3rd Qu.:0.6465 3rd Qu.:0.2873 3rd Qu.:0.1336 3rd Qu.:0.265 ## Max. :0.6465 Max. :0.2873 Max. :0.1336 Max. :0.265 The main thing to look at here is accuracy. We can see that the model performs quite poorly, making roughly 64,7% correct classifications. That is, we identify people being cured/not being cured correctly ca 65 times of 100. This is very similar to the result of our within sample classification - but it does not have to be similar. This is a very straight forward example of cross validation, for other techniques you can check out the documentation in the carat package or any other machine learning package. In psychology cross validation is, sadly, rare. But getting a grasp on the reproducibility of your results is paramount for your analysis so I encourage a deeper dive into cross-validation. I need to learn more about it myself. 9.5 LASSO (least absolute shrinkage and selection operator) 9.6 Ridge regression 9.7 Fully Bayesian regression 9.8 Poststratification "],["path-analysis.html", "10 Path analysis 10.1 Streiner(2005) 10.2 workshop analysis 10.3 Multiple regression(again) 10.4 Mediation analysis", " 10 Path analysis 10.1 Streiner(2005) Path analysis is not really a type of analysis like OLS regression or logistic regression but rather a mode for communicating and fitting more complex models. The path in path analysis refers to the specification of relationships between variables. What the path describe can vary. For example, a regression analysis where Y is regressed on X we have a path between X and Y that describes a causal effect of X on Y. Note that it is us as researchers that make the call that the relationship is causal, just because X and Y correlate does not mean that either variable causes the other. If you recall the lavaan notations from way back in chapter 4 you should recognise the difference between fitting models in a path context versus a regression context. To monkey what all the smart people say, OLS/ML regression is only a special case of a path analysis/SEM. Hopefully the distinction between path analysis and regression will become a bit fuzzy by the end of this chapter, they are very much the same. One might intuit them as slightly different tools for very similar jobs. In my opinion, the strength of path analysis lies in the visualisation opportunities combined with the ability to fit complex models, restrict those models and lastly compare the fit of those models. The little less cool thing with complex path analysis is that it is, wellcomplex. Another weakness of path analysis compared to normal regression is that it is very easy to fit nested regression models and there is a very natural progression of model building in the multiple regression context - as we saw in the previous chapter. Before getting in to live datasets, we can simulate some data. For this lecture I/we(class of 2021) were assigned an article by Streiner, it is a pretty nice article IMO so even if you havent read it I can recommend it. In the article Streiner analyses data from an unknown disorder called photonumerophobia, he describes it as the fear, that our fears of numbers will come to light. He then defines three predictors for this disorder, namely: HSM &lt;- high school math grade ANX &lt;- overall anxiety TAX &lt;- the difference between predicted tax and actual tax (weird I know, but lets run with it). Note that Streiner does not supply us with any data, he does however give as ample descriptives, enough to simulate the data quite closely, so let´s do that. All numbers are taken from table 1 in Streiner (2005): https://journals.sagepub.com/doi/pdf/10.1177/070674370505000207 #This is a perfect time for some faux. A nice little function is the cormat_from_triangle calls. It is quite simple, if you write in the top right triangle of correlations the function fills out the missing bits in the matrix. Now you dont have to write so much - thanks computer. library(faux) cors &lt;- cormat_from_triangle(c(.509 , -.366 ,.346, -.264 ,.338, .260)) #Now that we have the relationship between the data we can define the rest of the variables with the other descriptives given. set.seed(4543) #setting seed is always good data &lt;- rnorm_multi( n = 200 #let&#39;s take 200 observations , vars = 4 , mu = c(26.79, 20.33, 74.69, 1983.23) , sd = c(7.33 , 5.17 , 5.37 , 525.49) , r = cors #this is our previously defined cor matrix , varnames = c(&#39;pnp&#39;, &#39;anx&#39;, &#39;hsm&#39;, &#39;tax&#39;) , empirical = T)#we want to perfectly reproduce the data #Let´s look at the correlation matrix for the data we just generated cor(data) ## pnp anx hsm tax ## pnp 1.000 0.509 -0.366 0.346 ## anx 0.509 1.000 -0.264 0.338 ## hsm -0.366 -0.264 1.000 0.260 ## tax 0.346 0.338 0.260 1.000 #It´s a thing of beauty is it not? What about the descriptives? pastecs::stat.desc(data, desc = T ,basic = F) ## pnp anx hsm tax ## median 26.4386929 20.0561011 74.67091194 1.954408e+03 ## mean 26.7900000 20.3300000 74.69000000 1.983230e+03 ## SE.mean 0.5183093 0.3655742 0.37971634 3.715775e+01 ## CI.mean.0.95 1.0220834 0.7208964 0.74878412 7.327348e+01 ## var 53.7289000 26.7289000 28.83690000 2.761397e+05 ## std.dev 7.3300000 5.1700000 5.37000000 5.254900e+02 ## coef.var 0.2736096 0.2543040 0.07189717 2.649667e-01 #Lovely. Now we have data that very much resembles that from the article we can fit the path models he describes. Let´s start with table 4. pnp is regressed on all variables and the predictors are allowed to correlate. We will use the package lavaan for this so if you need a refresher go back to chapter 4 and check out the links provided there. #Loading lavaan and semPlot library(lavaan) library(semPlot) #Specifying our model with the lavaan syntax model.1 &lt;- (&#39; #regressions pnp ~ tax + hsm + anx #correlations anx ~~ hsm anx ~~ tax tax ~~ hsm &#39;) #Fitting our model using the sem function fit.1 &lt;- sem(model = model.1, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #The warning here is due to the fact that the tax variable is much larger than the others. Let´s plot the model. semPaths( object = fit.1 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , curvePivot = TRUE , edge.label.cex = 1.5 , rotation = 2 , sizeMan = 10) #And there it is, pretty as a picture. Now, Streiner actually specifies two other models, those seen in figure 5. Lets create and plot those two. After that we can check how they fit. #lets start with model a model.a &lt;- (&#39; #regressions pnp ~ tax tax ~ hsm hsm ~ anx &#39;) #Fitting the model fit.a &lt;- sem(model = model.a, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #Ploting the model semPaths( object = fit.a , what = &#39;est, std&#39; , layout = &#39;spring&#39; , style = &#39;lisrel&#39; , curvePivot = T , edge.label.cex = 1.5 , rotation = 1 , residuals = T , sizeMan = 10) #The control you have over the layout of these plots are vast, but the require tinkering that is not often worth it. This looks good enough. Let´s do the next model. model.b &lt;- (&#39; #regressions pnp ~ tax + hsm tax ~ anx hsm ~ anx &#39;) #Fitting the model fit.b &lt;- sem(model = model.b, data = data) ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: ## some observed variances are (at least) a factor 1000 times larger than others; use ## varTable(fit) to investigate #Ploting the model semPaths( object = fit.b , what = &#39;est, std&#39; , layout = &#39;tree2&#39; , style = &#39;lisrel&#39; , curvePivot = T , edge.label.cex = 1.5 , rotation = 2 , residuals = T , sizeMan = 10) Cool, this is the exact models he fits. Now, are these models any good? we can explore that question by summarising the fits (fit.1, fit.a and fit.b), but first, how many parameters can we estimate? This is one of the trickier parts of path analysis and SEM. We cannot simply think about the number of variables we have; we must think about our data in terms of a variance/co-variance matrix. For each variable we have one variance, in this case 4, each variable pair has a covariation, thus we have ([k^2  k] / 2) covariances where k is the number of variables we have. Let´s calculate how many pieces of information we have k &lt;- 4 co &lt;- ((k^2-k)/2) #We have 4 variances and 6 covariances, this totals out at 10. This means that we can make a total 10 parameter estimates before we exhaust our degrees of freedom. Read the article more closely for a better understanding of degrees of freedom, they are fascinating. So, how many parameters are we estimating? in the first model(fit.1) we are actually estimating 10 parameters (three regressions, tree covariations, and 4 variances). This means that we have 0 degrees of freedom and a perfect fit. In second model(fit.a) we estimate 6 parameters(three regressions and tree variances) and in the last model(fit.b) we estimate 7 parameters(4 regressions and 3 variances). This can be tricky to wrap your mind around in the beginning but once you play around with it a bit it becomes more straight forward. Since we have two overidentified models we can compare the fit of them(the identified model fits perfectly), we can do this with the anova function. anova(fit.a, fit.b) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.b 2 5565.9 5589.0 52.613 ## fit.a 3 5632.6 5652.4 121.295 68.681 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #note that this if not an F test but a chi-squared test(LRT) The significance of the test indicates that model.b fits significantly better than model.a. However, the information difference in terms of AIC and BIC is very small. But this is an issue that doesnt really apply itself well to entirely imaginary data. We don´t really have a good frame of reference in terms of theory. 10.2 workshop analysis Now that we have gotten familiar with path analysis, we can try out another example. The data is still very much simulated, but this time we have to deal with some actual data points, not just generation by ourselves. The following analysis is based of the workshop on path analysis at GU. Hopefully i can take the data from the workshop without any issues. Let´s load the data (it should be available on the repository, hopefully.) library(haven) data &lt;- read_sav(&quot;data sets/data path analysis spring 22.sav&quot;) #Note. this is my path to the data and to access the file you need to import it yourself These are the main packages we will be using. We have the usual suspects + MVN for analysis of multivariate normality. library(tidyverse) library(lavaan) library(semPlot) library(MVN) Let´s take a peek at what we are working with. summary(data) ## stress satisfaction turnover_intent demands support ## Min. :0.600 Min. :0.000 Min. :0.000 Min. :1.000 Min. :1.000 ## 1st Qu.:1.500 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:2.000 1st Qu.:2.333 ## Median :2.000 Median :2.000 Median :1.500 Median :2.000 Median :2.667 ## Mean :1.993 Mean :2.578 Mean :1.896 Mean :2.363 Mean :2.732 ## 3rd Qu.:2.333 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:3.000 3rd Qu.:3.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 All variables are numeric and seems to range between 0 and 5. Note that some variables are ordinal, that is, they are whole numbers that are not continuous, that is, the distance between 1 and 2 cannot be assumed to be the same as between 3 and 4. If we want to assume this, the data needs to normally distributed. So let´s take a look at the distributions. Note however that this is NOT the normality assumption of the linear regression but rather an assumption of using ordinal data as if they were continuous. ggplot(data, aes(stress))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #It has a normal look, and it seems to be continuous measure, this is good. ggplot(data, aes(satisfaction))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Again, quite normal looking - this is ordinal but could be treated as if continuous with some mental gymnastics ggplot(data, aes(turnover_intent))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Interesting, perhaps its measured in half steps? but that begs the question why none of the higher valus such as 4.5 exists, it seems to stop after 2. Strange. ggplot(data, aes(demands))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Like satisfaction, looks pretty good. ggplot(data, aes(support))+ geom_histogram(colour = &#39;blue&#39;)+ theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #Now this is a strange looking distribution...it looks to be a continuous measure and the density of the curve should be quite normal. #However, we need to check the normality of our predicted variables when we have them. Before that we can check the normality tests. One common measure is mardia, so let´s use that. We can also look at some of the qq-plots and the outlier measures. Let´s save the scores in a list called normality_diagnostics normality_diagnostics &lt;- mvn(data , mvnTest = &#39;mardia&#39; #takes maridas test , multivariatePlot = &#39;qq&#39; #normal chi2 qq-plot , univariateTest = &#39;AD&#39; #anderson-darling test for univariate normality , showOutliers = TRUE , showNewData = TRUE #stores a tibble without the outliers , multivariateOutlierMethod = &#39;adj&#39;) #takes the adjusted mahalanobis distance #Checking the normality descriptives normality_diagnostics$Descriptives ## n Mean Std.Dev Median Min Max 25th 75th Skew ## stress 322 1.992847 0.7447653 2.000000 0.6 5 1.500000 2.3325 1.5307216 ## satisfaction 322 2.577640 1.0715218 2.000000 0.0 5 2.000000 3.0000 0.6089998 ## turnover_intent 322 1.895963 1.2288900 1.500000 0.0 5 1.000000 3.0000 0.7192168 ## demands 322 2.363354 0.9145545 2.000000 1.0 5 2.000000 3.0000 0.4177021 ## support 322 2.731884 0.7106893 2.666667 1.0 5 2.333333 3.0000 0.2551844 ## Kurtosis ## stress 4.593582827 ## satisfaction 0.130539733 ## turnover_intent -0.169774465 ## demands -0.002784417 ## support 0.298869065 #Checking univariate normality normality_diagnostics$univariateNormality ## Test Variable Statistic p value Normality ## 1 Anderson-Darling stress 8.7723 &lt;0.001 NO ## 2 Anderson-Darling satisfaction 17.5045 &lt;0.001 NO ## 3 Anderson-Darling turnover_intent 9.3350 &lt;0.001 NO ## 4 Anderson-Darling demands 16.5444 &lt;0.001 NO ## 5 Anderson-Darling support 3.2055 &lt;0.001 NO #Checking multivariate normality normality_diagnostics$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 460.776024749078 1.05636187815355e-75 NO ## 2 Mardia Kurtosis 8.96312473441598 0 NO ## 3 MVN &lt;NA&gt; &lt;NA&gt; NO So, nothing is normal and we have quite a few outliers according to Mahalanobis. If we want an outlier free data set the mvn function saves it for us. Let´s try to extract the clean data from the normality diagnostics. clean_data &lt;- normality_diagnostics$newData Here we have the data without the 27 outliers indicated by the normality diagnostics. I´m not a fan of this type of diagnostics, but we can use the raw data as a type of sensitivity analysis later on, for now lets move on using the clean data. Since we don´t really know what we are doing it´s always good to look at the correlations to inform us about what connections lie within our sample. So let´s do that and then try to find a good model for our data. #This gives a normal correlation matrix cor(clean_data) ## stress satisfaction turnover_intent demands support ## stress 1.00000000 -0.4892677 0.22641261 -0.04095583 -0.3617246 ## satisfaction -0.48926773 1.0000000 -0.59110506 0.12206989 0.3145087 ## turnover_intent 0.22641261 -0.5911051 1.00000000 0.03633829 -0.2111387 ## demands -0.04095583 0.1220699 0.03633829 1.00000000 0.1464965 ## support -0.36172462 0.3145087 -0.21113872 0.14649650 1.0000000 #we can however plot the data out using the corplot function from the psych package without loading it by using the :: command like so: psych::corPlot(cor(clean_data)) #Note. the omitted variable in the bottom is turnover_intent Stress, support and satisfaction seems to be correlated just fine. Demands seem to be a quite superfluous variable having low correlations to all other variables. So let´s build a model where stress and support are allowed to correlate and that both have a causal impact on the satisfaction you have in the workplace. Let´s also say that support and satisfaction have a causal impact on your turnover intent. Let´s specify this model using the lavaan syntax. model.1 &lt;- (&#39; #regressions satisfaction ~ stress + support turnover_intent ~ satisfaction turnover_intent ~ support #covariates stress ~~ support &#39;) Now that we have our model - specified in text - we can fit that model using lavaans sem function just like we did before. fit.1 &lt;- sem( model = model.1 #the model we want to fit , data = clean_data #the data we want to fit the model on , estimator = &#39;MLM&#39; #the estimator we use (robust maximum likelihood) ) Now that we have our fit, we can summarise the results summary(fit.1 #the fit we want to summarize , standardize = TRUE #we want the standardised estimates , fit.measures = TRUE #we also want the fit measures(CFI, TLI) , rsquare = TRUE #if we want to r2 for the model ) ## lavaan 0.6-9 ended normally after 18 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 295 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 3.014 2.867 ## Degrees of freedom 1 1 ## P-value (Chi-square) 0.083 0.090 ## Scaling correction factor 1.051 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 260.797 208.698 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.250 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.992 0.991 ## Tucker-Lewis Index (TLI) 0.953 0.945 ## ## Robust Comparative Fit Index (CFI) 0.992 ## Robust Tucker-Lewis Index (TLI) 0.954 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1420.264 -1420.264 ## Loglikelihood unrestricted model (H1) -1418.757 -1418.757 ## ## Akaike (AIC) 2858.528 2858.528 ## Bayesian (BIC) 2891.711 2891.711 ## Sample-size adjusted Bayesian (BIC) 2863.169 2863.169 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.083 0.080 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.197 0.191 ## P-value RMSEA &lt;= 0.05 0.195 0.207 ## ## Robust RMSEA 0.082 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.199 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.022 0.022 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## stress -0.633 0.107 -5.902 0.000 -0.633 -0.432 ## support 0.252 0.087 2.909 0.004 0.252 0.158 ## turnover_intent ~ ## satisfaction -0.672 0.051 -13.158 0.000 -0.672 -0.582 ## support -0.051 0.084 -0.610 0.542 -0.051 -0.028 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~~ ## support -0.178 0.044 -4.093 0.000 -0.178 -0.362 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .satisfaction 0.850 0.066 12.878 0.000 0.850 0.739 ## .turnover_intnt 0.994 0.074 13.440 0.000 0.994 0.650 ## stress 0.536 0.081 6.649 0.000 0.536 1.000 ## support 0.454 0.043 10.580 0.000 0.454 1.000 ## ## R-Square: ## Estimate ## satisfaction 0.261 ## turnover_intnt 0.350 So our model fits remarkably well. Not even the chi2 is significant. Also, not how much better our model is than the baseline model. One bad thing is the fact that we are not very courageous, we only have one degree of freedom. Let´s plot this model out using semPlot and then try improving our fit even more. #This is a more detailed notation of the semPlot alternatives semPaths( object = fit.1 #the fit we want to plot(our model) , what = &#39;est, std&#39; #what we want to plot(the z-coefficients) , style = &#39;lisrel&#39; #the style of the plot , curvePivot = TRUE #manipulates how the lines look , esize = 4 #the size of our lines , nCharNodes = 0 #takes all the characters in our manifest vars , edge.label.cex = 1.5#how big the estimates are(indicated in &quot;what =&quot;) , residuals = FALSE #removes the residual loops , sizeMan = 15 #indicates the width of manifest variables , rotation = 2 #how we want to rotate the plot(wich direction) , theme = &#39;Borkulo&#39;) #the theme we want to use, i like this one #Looks pretty good right? but notice the very weak trace between support and turnover intent and satisfaction! Let´s interpret this. We can see that the correlation between support and stress is -.36 (as we saw in the correlation plot). We can also see that for every unit increase in stress satisfaction decreased with .43 standard deviations. We also see that for every unit increase in satisfaction we see a decrease in turnover_intent at .58 standard deviations. All these estimates, together with the unstandardized counterparts are available in the summary above. In the summary we also see an r2 of .26 in satisfaction and .35 in turnover intent, meaning that we can explain roughly 26% of the variation in satisfaction through stress and support and .35% of the variation in turnover intent through support and satisfaction. Pretty neat right? So where do we go from here? we have a model and its fits the data very well. We want to be brave scientists though, so we should refine our model and make some bolder claims. For example, let´s make the claim that support does NOT influence turnover intent, support does not even influence satisfaction. No, support actually has a one directional relationship with stress, that is the degree of support we have impacts our degree of stress, but stress does not influence how much support we have. Let´s specify this model and fit it. model.2 &lt;- (&#39; #regressions stress ~ support satisfaction ~ stress turnover_intent ~ satisfaction &#39;) fit.2 &lt;- sem( model = model.2 , data = clean_data , estimator = &#39;MLM&#39;) summary(fit.2 , standardize = TRUE , fit.measures = TRUE , rsquare = TRUE ) ## lavaan 0.6-9 ended normally after 14 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 295 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 11.898 12.009 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.008 0.007 ## Scaling correction factor 0.991 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 260.797 241.478 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.080 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.965 0.962 ## Tucker-Lewis Index (TLI) 0.930 0.923 ## ## Robust Comparative Fit Index (CFI) 0.965 ## Robust Tucker-Lewis Index (TLI) 0.930 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1122.743 -1122.743 ## Loglikelihood unrestricted model (H1) -1116.794 -1116.794 ## ## Akaike (AIC) 2257.486 2257.486 ## Bayesian (BIC) 2279.608 2279.608 ## Sample-size adjusted Bayesian (BIC) 2260.580 2260.580 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.100 0.101 ## 90 Percent confidence interval - lower 0.045 0.046 ## 90 Percent confidence interval - upper 0.163 0.164 ## P-value RMSEA &lt;= 0.05 0.064 0.063 ## ## Robust RMSEA 0.100 ## 90 Percent confidence interval - lower 0.046 ## 90 Percent confidence interval - upper 0.163 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.058 0.058 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~ ## support -0.393 0.083 -4.756 0.000 -0.393 -0.362 ## satisfaction ~ ## stress -0.717 0.104 -6.920 0.000 -0.717 -0.489 ## turnover_intent ~ ## satisfaction -0.682 0.049 -13.783 0.000 -0.682 -0.591 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .stress 0.466 0.057 8.237 0.000 0.466 0.869 ## .satisfaction 0.875 0.067 12.995 0.000 0.875 0.761 ## .turnover_intnt 0.996 0.074 13.531 0.000 0.996 0.651 ## ## R-Square: ## Estimate ## stress 0.131 ## satisfaction 0.239 ## turnover_intnt 0.349 By looking at the fit measures we can see that this model fits very well. But notice that we now have 3 degrees of freedom, that is, by further constraining the model (not allowing correlations between stress and support and removing the regression between support and turnover intent) we have expressed a more theoretically parsimonious model - very cool. Let´s make the path diagram and interpret the coefficients. semPaths( object = fit.2 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , curvePivot = TRUE , esize = 10 , asize = 5 #arrowhead size , sizeMan2 = 3 #height of the manifest variables , nCharNodes = 0 , edge.label.cex = 1.5 , residuals = FALSE , sizeMan = 15 , rotation = 2 , theme = &#39;Borkulo&#39;) #Note. The new comments are just arguments we didn´t use in the previous plot Cool, notice how we rephrased the connection between support and stress to a one-sided arrow. This can now be interpreted as a regression weight, that is, for every unit increase in support we se a .36 unit decrease in stress. We also see a bigger coefficient for the relationship between stress and satisfaction. This is likely due to the fact that we presume that support does not impact satisfaction but influences stress and thus, some of the variation we can explain in stress by support also influences satisfaction - i.e., stress mediates the relationship between support and satisfaction. The relationship between satisfaction and turnover intent is also different, but only by .01, that is, super small and an unimportant difference. Now that we have two overidentified models we can compare the fit of them. This is surprisingly easy; we only use the anova function and our to model fits. Note that this is not a GLM anova but an LRT test, that is, a likelihood ratio test. anova(fit.1, fit.2) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.1 1 2858.5 2891.7 3.0141 ## fit.2 3 2257.5 2279.6 11.8977 9.2498 2 0.009805 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So what does this tell us? It tells us that the baseline model(fit.1) fits the data significantly better then the other model(fit.2). However, one should not that the AIC and BIC is lower for the fit.2 model, low scores on AIC and BIC is indicative of good fit, and models with low values are preferable to those with higher BIC and AIC. I do not feel particularly strongly about any of these models, but if I had to choose one I would choose the second one since I like parsimony, but the first model technically fits better (though its almost identified so do with that what you will). For more on the LRT you can see: http://econ.upf.edu/~satorra/dades/BryantSatorraPaperInPressSEM.pdf One last thing we can do before we move on to other things is the check the modification indices. This is one cool thing that you can do with a path analysis that normal regression can´t really do. We will not use them for anything, but it is interesting to look at them. modificationindices(fit.2) %&gt;% arrange(-mi) %&gt;% head(20) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 1 support ~ satisfaction 8.440 0.131 0.131 0.208 0.208 ## 2 satisfaction ~ support 8.440 0.252 0.252 0.158 0.235 ## 3 stress ~~ satisfaction 8.440 0.299 0.299 0.468 0.468 ## 4 stress ~ satisfaction 8.440 0.341 0.341 0.500 0.500 ## 5 stress ~ turnover_intent 6.336 -0.109 -0.109 -0.184 -0.184 ## 6 support ~ turnover_intent 3.887 -0.067 -0.067 -0.124 -0.124 ## 7 stress ~~ turnover_intent 3.408 -0.082 -0.082 -0.121 -0.121 ## 8 satisfaction ~ turnover_intent 2.351 -0.171 -0.171 -0.197 -0.197 ## 9 turnover_intent ~ stress 2.351 -0.139 -0.139 -0.083 -0.083 ## 10 satisfaction ~~ turnover_intent 2.351 -0.170 -0.170 -0.182 -0.182 ## 11 turnover_intent ~ support 0.298 -0.048 -0.048 -0.026 -0.039 #Note. the %&gt;% is a pipe function, they make code simpler to write so look into them! See the provided programming resources in the beginning of the book. The modification indices (mi column) shows how much the chi2 of the model would drop if the parameter was included. Thus, indices over 3.84 will significantly increase the fit of the model. We have a few modifications we can make to our model, some of which improves the fit quite a bit. Lastly, let´s do a sensitivity check by running the second model through the raw data. That is, the data with the outliers included. sensitivity.fit &lt;- sem( model = model.2 , data = data , estimator = &#39;MLM&#39;) summary(sensitivity.fit, standardize = TRUE, fit.measures = TRUE , rsquare = TRUE) ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 322 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 10.159 10.615 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.017 0.014 ## Scaling correction factor 0.957 ## Satorra-Bentler correction ## ## Model Test Baseline Model: ## ## Test statistic 267.940 254.344 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 1.053 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.973 0.969 ## Tucker-Lewis Index (TLI) 0.945 0.939 ## ## Robust Comparative Fit Index (CFI) 0.972 ## Robust Tucker-Lewis Index (TLI) 0.944 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1234.025 -1234.025 ## Loglikelihood unrestricted model (H1) -1228.945 -1228.945 ## ## Akaike (AIC) 2480.049 2480.049 ## Bayesian (BIC) 2502.697 2502.697 ## Sample-size adjusted Bayesian (BIC) 2483.665 2483.665 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.086 0.089 ## 90 Percent confidence interval - lower 0.032 0.034 ## 90 Percent confidence interval - upper 0.147 0.151 ## P-value RMSEA &lt;= 0.05 0.119 0.108 ## ## Robust RMSEA 0.087 ## 90 Percent confidence interval - lower 0.034 ## 90 Percent confidence interval - upper 0.146 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.054 0.054 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## stress ~ ## support -0.346 0.073 -4.730 0.000 -0.346 -0.330 ## satisfaction ~ ## stress -0.694 0.099 -7.007 0.000 -0.694 -0.482 ## turnover_intent ~ ## satisfaction -0.672 0.048 -14.120 0.000 -0.672 -0.586 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .stress 0.493 0.060 8.155 0.000 0.493 0.891 ## .satisfaction 0.878 0.065 13.536 0.000 0.878 0.767 ## .turnover_intnt 0.989 0.074 13.405 0.000 0.989 0.657 ## ## R-Square: ## Estimate ## stress 0.109 ## satisfaction 0.233 ## turnover_intnt 0.343 anova(sensitivity.fit, fit.2) ## Warning in lavTestLRT(object = object, ..., model.names = NAMES): lavaan WARNING: some ## models have the same degrees of freedom ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## sensitivity.fit 3 2480.1 2502.7 10.159 ## fit.2 3 2257.5 2279.6 11.898 1.7384 0 #Note. We are comparing two models with the same degrees of freedom, this is not very appropriate since they are not nested (and on different data) but its nice to see the measures anyways. So, the model based on the raw data is not that different from we get when we have cleaned the data. This becomes even more apparent if we were to check the parameter estimates for the two models like so: parameterestimates(fit.2) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 stress ~ support -0.393 0.083 -4.756 0 -0.555 -0.231 ## 2 satisfaction ~ stress -0.717 0.104 -6.920 0 -0.920 -0.514 ## 3 turnover_intent ~ satisfaction -0.682 0.049 -13.783 0 -0.779 -0.585 ## 4 stress ~~ stress 0.466 0.057 8.237 0 0.355 0.577 ## 5 satisfaction ~~ satisfaction 0.875 0.067 12.995 0 0.743 1.007 ## 6 turnover_intent ~~ turnover_intent 0.996 0.074 13.531 0 0.851 1.140 ## 7 support ~~ support 0.454 0.000 NA NA 0.454 0.454 parameterestimates(sensitivity.fit) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 stress ~ support -0.346 0.073 -4.730 0 -0.489 -0.202 ## 2 satisfaction ~ stress -0.694 0.099 -7.007 0 -0.888 -0.500 ## 3 turnover_intent ~ satisfaction -0.672 0.048 -14.120 0 -0.765 -0.579 ## 4 stress ~~ stress 0.493 0.060 8.155 0 0.374 0.611 ## 5 satisfaction ~~ satisfaction 0.878 0.065 13.536 0 0.751 1.005 ## 6 turnover_intent ~~ turnover_intent 0.989 0.074 13.405 0 0.844 1.133 ## 7 support ~~ support 0.504 0.000 NA NA 0.504 0.504 They are pretty much the same parameter estimates, certainly not significantly different. So, what more can we do for this fun little analysis? we can do some bootstrapping. Lavaan has a really nice bootstrapping function that gives a lot of freedom to choose what to do with the estimates. Let´s bootstrap all estimates from the second model(fit.2) and take a percentile intervall for them. Note that this can take a few seconds. boot_sample &lt;- bootstrapLavaan( object = fit.2 #the fit we want to sample , R = 1000 #the number of iterations we want ) Notice that boot_sample is a matrix of many estimates. Let´s put the regression coefficients in a dataframe and make our intervals. boot_sample &lt;- data.frame(boot_sample) boot_coef &lt;- boot_sample[c(1,2,3)] quantile(boot_coef$stress.support #the estimate we want to take intervals on , probs = c(.025, .5, .975)) #the percentile points we want to see ## 2.5% 50% 97.5% ## -0.5489199 -0.3825642 -0.2386760 #We see that the regression weight ranges between -.55 and -.24 quantile(boot_coef$satisfaction.stress , probs = c(.025, .5, .975)) ## 2.5% 50% 97.5% ## -0.9758261 -0.7247085 -0.5454487 #We see that the regression weight ranges between -.98 and -.55 quantile(boot_coef$turnover_intent.satisfaction , probs = c(.025, .5, .975)) ## 2.5% 50% 97.5% ## -0.7719642 -0.6811282 -0.5928512 #We see that the regression weight ranges between -.77 and -.59 This is not super interesting, but it is pretty cool and also kind of illustrates what you can do with bootstrapping in a more manual way. There are of course much more you can do than to take percentile intervals of the regression weights but that lies beyond the specific realm of path analysis. 10.3 Multiple regression(again) By now you should feel good, both with lavaan semPlot and the general procedure for path analysis. So, lets take a step back and rework the good ´ol Field example for multiple regression, but with a path analysis. And don´t worry, this will be short and sweet :) 10.4 Mediation analysis In the Field book he gives an example of mediation with the data set from Lambert et al.,(2012). Since we are very comfortable with working with paths, mediation becomes a simple thing. In the regression example in the previous chapter we only estimated direct effects, that is, effects of a on c (or x on y if you prefer those letters). Mediation can estimate both direct and indirect effects. The indirect effect is the multiplication of path a and path b. Let´s load the data. While the process of a moderation analysis is quite complicated in terms of functions and packages - as we saw in the regression part of the book, with mediation we can rely on good ´ol lavaan. #Packages we need library(lavaan) library(semPlot) #Loading the data data &lt;- read_sav(&quot;data sets/Lambert et al. (2012).sav&quot;) names(data) &lt;- c(&#39;consumption&#39;, &#39;ln_porn&#39;, &#39;commit&#39;, &#39;infi&#39;, &#39;hook_ups&#39;) Let´s specify the model from the Field book. med_model.1 &lt;- (&#39; # Direct effect infi ~ c*consumption # Mediator commit ~ a*consumption infi ~ b*commit # Indirect effect (a*b) ab := a*b # Total effect total := c + (a*b) &#39;) #Note. The defined variables a, b and c are needed for the mediation analysis to work. These are the paths of our model, without them we cannot calculate our main and indirect effects! the lavaan symbol &#39;:=&#39; describes a new parameter dependent on our specified model. In this case it´s the regression weights (I believe) Let´s fit and summarise the result of this model. med_fit.1 &lt;- sem(med_model.1, data) #Summarising a lavaan can take a lot of commands, I will note what they do here as I have done before, but note that there are many other alternatives you can use! summary(med_fit.1 #the model we want to use(med_fit.1) , standardize = T #do we want a standardized estimate? T mean true/yes , fit.measures = T #do we want fit measures? , rsquare = T) #do we want r2 ## lavaan 0.6-9 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 35.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -501.418 ## Loglikelihood unrestricted model (H1) -501.418 ## ## Akaike (AIC) 1012.836 ## Bayesian (BIC) 1030.218 ## Sample-size adjusted Bayesian (BIC) 1014.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## consumptin (c) 0.107 0.038 2.797 0.005 0.107 0.171 ## commit ~ ## consumptin (a) -0.092 0.042 -2.175 0.030 -0.092 -0.139 ## infi ~ ## commit (b) -0.268 0.058 -4.612 0.000 -0.268 -0.282 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.429 0.039 10.932 0.000 0.429 0.878 ## .commit 0.531 0.049 10.932 0.000 0.531 0.981 ## ## R-Square: ## Estimate ## infi 0.122 ## commit 0.019 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.025 0.013 1.967 0.049 0.025 0.039 ## total 0.132 0.040 3.329 0.001 0.132 0.211 Nice, this is exactly what we want. Notice also that our model is saturated/identified, that is, it has 0 degrees of freedom. This means that we can´t really assess the fit of this model. Let´s make a quick interpretation. Infidelity increases with .107 units for every unit increase in consumption. Commitment decreases by .092 units for every increase in consumption and infidelity decreases with .268 for every unit increase in commitment. ´ab´ is our indirect effect, that is, .025. Which can be interpreted as a kind of r2. semPaths(med_fit.1 , what = &#39;est, std&#39; , style = &#39;lisrel&#39; , layout = &#39;tree&#39; , nCharNodes = 0 , edge.label.cex = 1.5 , sizeMan = 20 , residuals = F , rotation = 4 , theme = &#39;Borkulo&#39;) #Note that this is NOT the same output as in the field book. He uses the log transformed variable for consumption. So let´s recreate his model. med_model.2 &lt;- (&#39; # direct effect infi ~ c*ln_porn # mediator commit ~ a*ln_porn infi ~ b*commit # indirect effect (a*b) ab := a*b # total effect total := c + (a*b) &#39;) med_fit.2 &lt;- sem(med_model.2, data) summary(med_fit.2, standardize = T, fit.measures = T, rsquare = T) ## lavaan 0.6-9 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 33.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -502.418 ## Loglikelihood unrestricted model (H1) -502.418 ## ## Akaike (AIC) 1014.835 ## Bayesian (BIC) 1032.218 ## Sample-size adjusted Bayesian (BIC) 1016.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## ln_porn (c) 0.457 0.193 2.365 0.018 0.457 0.145 ## commit ~ ## ln_porn (a) -0.470 0.212 -2.215 0.027 -0.470 -0.142 ## infi ~ ## commit (b) -0.271 0.058 -4.642 0.000 -0.271 -0.285 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.432 0.040 10.932 0.000 0.432 0.886 ## .commit 0.531 0.049 10.932 0.000 0.531 0.980 ## ## R-Square: ## Estimate ## infi 0.114 ## commit 0.020 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.127 0.064 1.999 0.046 0.127 0.040 ## total 0.585 0.200 2.925 0.003 0.585 0.186 semPaths(med_fit.2 , what = &#39;est&#39; , style = &#39;lisrel&#39; , layout = &#39;tree&#39; , nCharNodes = 0 , edge.label.cex = 1.5 , sizeMan = 20 , residuals = F , theme = &#39;Borkulo&#39; , rotation = 4) #Very nice, we have recreated the findings from Field. These are the same models, but with the log transformation. The reason why I didn´t use that in the first model is because it´s harder to interpret. I have a hard time understanding what this indirect effect of .127 means since its a combination of the influence of a log variable, through a variable that is not logged. A smarter person than I will have to describe what this means. Field also uses bootstrapped standard errors; this can be specified in the fit portion of our workflow. Let´s refit our second model but with bootstrapped SEs. #it´s going to take a few seconds so do not fret if you dont get an output set.seed(234) #setting seed so that we can recreate the random values med_fit.2 &lt;- sem(med_model.2 #what model we want to fit , se = &#39;bootstrap&#39; #how do we want to estimate the std.err? , data) #indicates the data we want to fit the model on. summary(med_fit.2, standardize = T, fit.measures = T, rsquare = T) ## lavaan 0.6-9 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Used Total ## Number of observations 239 240 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 33.895 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -502.418 ## Loglikelihood unrestricted model (H1) -502.418 ## ## Akaike (AIC) 1014.835 ## Bayesian (BIC) 1032.218 ## Sample-size adjusted Bayesian (BIC) 1016.369 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value RMSEA &lt;= 0.05 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## infi ~ ## ln_porn (c) 0.457 0.251 1.822 0.068 0.457 0.145 ## commit ~ ## ln_porn (a) -0.470 0.234 -2.004 0.045 -0.470 -0.142 ## infi ~ ## commit (b) -0.271 0.071 -3.824 0.000 -0.271 -0.285 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .infi 0.432 0.055 7.908 0.000 0.432 0.886 ## .commit 0.531 0.050 10.539 0.000 0.531 0.980 ## ## R-Square: ## Estimate ## infi 0.114 ## commit 0.020 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.127 0.071 1.798 0.072 0.127 0.040 ## total 0.585 0.250 2.339 0.019 0.585 0.186 parameterestimates(med_fit.2) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 infi ~ ln_porn c 0.457 0.251 1.822 0.068 -0.032 0.960 ## 2 commit ~ ln_porn a -0.470 0.234 -2.004 0.045 -0.917 0.019 ## 3 infi ~ commit b -0.271 0.071 -3.824 0.000 -0.404 -0.124 ## 4 infi ~~ infi 0.432 0.055 7.908 0.000 0.314 0.531 ## 5 commit ~~ commit 0.531 0.050 10.539 0.000 0.434 0.630 ## 6 ln_porn ~~ ln_porn 0.049 0.000 NA NA 0.049 0.049 ## 7 ab := a*b ab 0.127 0.071 1.798 0.072 -0.005 0.281 ## 8 total := c+(a*b) total 0.585 0.250 2.339 0.019 0.089 1.096 Cool, now we have a SE estimate of .071 instead of .064. Note also that the ci for the ab effect, that is, the indirect effect, ranges from -.005 to .28, we cross zero and can therefore not be certain in the existence of the mediation. The only robust effect is that of commitment on infidelity. Let´s finish of by fitting a normal linear interaction model and compare the outcomes. lm_model &lt;- lm(infi ~ consumption*commit, data) summary.lm(lm_model) ## ## Call: ## lm(formula = infi ~ consumption * commit, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.94415 -0.36188 -0.13989 0.00893 2.00893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.53260 0.42566 3.601 0.000387 *** ## consumption -0.05829 0.19836 -0.294 0.769133 ## commit -0.33807 0.10103 -3.346 0.000954 *** ## consumption:commit 0.04142 0.04864 0.852 0.395292 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6591 on 235 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.1251, Adjusted R-squared: 0.1139 ## F-statistic: 11.2 on 3 and 235 DF, p-value: 6.744e-07 This output seems very nice to me. It very much confirms what our mediation analysis found. The only reliable effect is that of commitment. "],["exploratory-factor-analysis-efa.html", "11 Exploratory factor analysis (EFA)", " 11 Exploratory factor analysis (EFA) In this section we will take a look at a very large sample from a cross-language hexaco-100 study. The aim is to see if the scale works on all languages. And to fit a theoretically good model. For the full study see: https://www.researchgate.net/publication/332130706_The_HEXACO-100_across_16_languages_A_large-scale_test_of_measurement_invariance we wont be able to use the full scale (not enough power in the computer), but we can try some cool stuff. We will be using this data for both the EFA and the CFA later on. library(tidyverse) library(readxl) library(here) hexaco_100 &lt;- read_excel(here(&quot;Data sets/hexaco.100.xlsx&quot;)) View(hexaco_100) This data set can be found at: https://osf.io/bwtnr/ or at the book repo. Below we have a brief description of the variables. Variable Description lang &lt;- Language version of HEXACO-100 2 = Chinese, 3 = Croatian, 4 =Czech, 5 = Dutch, 6 = German, 8 = Italian 9 = Japanese, 10 = Korean 11 = Polish, 13 = Serbian 14 = Spanish, 15 = Swedish, 16 = Turkish) hex1 - hex100 &lt;- Items of the HEXACO-100; 1 = strongly disagree to 5 = strongly agree(inverted items are already re-coded); for assignment of items to HEXACO factors, see www.hexaco.org) hh &lt;- Mean value of Honesty-Humility hex:6, 30, 54, 78, 12, 36, 60, 84, 18, 42, 66, 90, 24, 48, 72, 96 em &lt;- Mean value of Emotionality hex:5, 29, 53, 77, 11, 35, 59, 83, 17, 41, 65, 89, 23, 47, 71, 95 ex &lt;- Mean value of Extraversion hex:4, 28, 52, 76, 10, 34, 58, 82, 16, 40, 64, 88, 22, 46, 70, 94 ag &lt;- Mean value of Agreeableness hex:3, 27, 51, 75, 9, 33, 57, 81, 15, 39, 63, 87, 21, 45, 69, 93 co &lt;- Mean value of Conscientiousness hex:2, 26, 50, 74, 8, 32, 56, 80, 14, 38, 62, 86, 20, 44, 68, 92 op &lt;- Mean value of Openness to Experience hex:1, 25, 49, 73, 7, 31, 55, 79, 13, 37, 61, 85, 19, 43, 67, 91 hh_sinc &lt;- Mean value of sincerity facet of Honesty-Humility 6, 30, 54, 78 hh_fair &lt;- Mean value of fairness facet of Honesty-Humility 12, 36, 60, 84 hh_gree &lt;- Mean value of greed-avoidance facet of Honesty-Humility 18, 42, 66, 90 hh_mode &lt;- Mean value of modesty facet of Honesty-Humility 24, 48, 72, 96 em_fear &lt;- Mean value of fearfulness facet of Emotionality 5, 29, 53, 77 em_anxi &lt;- Mean value of anxiety facet of Emotionality 11, 35, 59, 83 em_depe &lt;- Mean value of dependence facet of Emotionality 17, 41, 65, 89 em_sent &lt;- Mean value of sentimentality facet of Emotionality 23, 47, 71, 95 ex_sses &lt;- Mean value of social self-esteem facet of Extraversion 4, 28, 52, 76 ex_socb &lt;- Mean value of social boldness facet of Extraversion 10, 34, 58, 82 ex_soci &lt;- Mean value of sociability facet of Extraversion 16, 40, 64, 88 ex_live &lt;- Mean value of liveliness facet of Extraversion 22, 46, 70, 94 ag_forg &lt;- Mean value of forgiveness facet of Agreeableness 3, 27, 51, 7 ag_gent &lt;- Mean value of gentleness facet of Agreeableness 9, 33, 57, 81 ag_flex &lt;- Mean value of flexibility facet of Agreeableness 15, 39, 63, 87 ag_pati &lt;- Mean value of patience facet of Agreeableness 21, 45, 69, 93 co_orga &lt;- Mean value of organization facet of Conscientiousness 2, 26, 50, 74 co_dili &lt;- Mean value of diligence facet of Conscientiousness 8, 32, 56, 80 co_perf &lt;- Mean value of perfectionism facet of Conscientiousness 14, 38, 62, 86 co_prud &lt;- Mean value of prudence facet of Conscientiousness 20, 44, 68, 92 op_aesa &lt;- Mean value of aesthetic appreciation facet of Openness to Experience 1, 25, 49, 73 op_inqu &lt;- Mean value of inquisitiveness facet of Openness to Experience 7, 31, 55, 79 op_crea &lt;- Mean value of creativity facet of Openness to Experience 13, 37, 61, 85 op_unco &lt;- Mean value of unconventionality facet of Openness to Experience 19, 43, 67, 91 altruism &lt;- Mean value of altruism facet 97, 98, 99, 100 So, we have a lot of variables. Since we are using the data set more than once, we should split the sample so that we can use one half on the EFA and one for the CFA. Let´s do that now. #Our random grouping variable set.seed(324) grouping &lt;- rbinom(n = 25914,size = 1, prob = .5) efa_data &lt;- hexaco_100 %&gt;% filter(grouping == 1) cfa_data &lt;- hexaco_100 %&gt;% filter(grouping == 0) This dataset is still too big for us to use in an efficient manner. I will look at one factor in the data, conscientiousness, and examine how well we can capture the facets. Let´s make a data set with conscientiousness from our efa split. This data set will still be very very large, so let´s take a random sample of 1000 from the c data. c_efa_data &lt;- efa_data %&gt;% dplyr::select(hex2, hex26, hex50, hex74, hex8, hex32, hex56, hex80, hex14, hex38, hex62, hex86, hex20, hex44, hex68, hex92) c_efa_data &lt;- slice_sample(c_efa_data, n = 1000, replace = F) Now that we have our data, we should look at the descriptive statistics of the variables. My favourite way, as you might know by now, is to use the mvn function. library(MVN) normality_diagnostics &lt;- mvn(c_efa_data , mvnTest = &#39;mardia&#39; , multivariatePlot = &#39;qq&#39; , univariateTest = &#39;AD&#39; , showOutliers = TRUE , showNewData = TRUE , multivariateOutlierMethod = &#39;adj&#39;) Lets check out the descriptive statistics from our diagnostics. One of the reasons I like the mvn function is that it stores a lot of useful information that we can access at any point later on. normality_diagnostics$Descriptives ## n Mean Std.Dev Median Min Max 25th 75th Skew Kurtosis ## hex2 1000 3.050 1.1120867 3 1 5 2 4 -0.002508437 -0.82143670 ## hex26 1000 3.475 1.1647883 4 1 5 3 4 -0.416110282 -0.82440748 ## hex50 1000 3.597 1.3054866 4 1 5 2 5 -0.545236972 -0.94672707 ## hex74 1000 3.412 1.1906610 4 1 5 2 4 -0.354524887 -0.96547339 ## hex8 1000 3.798 0.9529450 4 1 5 3 4 -0.642493661 -0.04725980 ## hex32 1000 3.769 0.9426174 4 1 5 3 4 -0.644194691 0.02404429 ## hex56 1000 3.654 1.0730496 4 1 5 3 4 -0.669001869 -0.22626167 ## hex80 1000 3.708 1.0477757 4 1 5 3 4 -0.721876064 -0.12937096 ## hex14 1000 3.426 1.0666122 4 1 5 3 4 -0.365724750 -0.63591986 ## hex38 1000 3.639 1.0381506 4 1 5 3 4 -0.691687225 -0.18085416 ## hex62 1000 3.742 0.9371842 4 1 5 3 4 -0.590045002 -0.14830252 ## hex86 1000 3.075 1.1843892 3 1 5 2 4 -0.025719486 -0.99140133 ## hex20 1000 3.267 1.0434557 3 1 5 2 4 -0.184216204 -0.70204656 ## hex44 1000 3.561 1.0398848 4 1 5 3 4 -0.517272669 -0.44702630 ## hex68 1000 3.172 0.9845737 3 1 5 2 4 -0.160086005 -0.70742902 ## hex92 1000 3.038 1.0495988 3 1 5 2 4 -0.112011084 -0.74881524 Seems very reasonable, no big skew or kurtosis. The mean and median are good as well We can also take a look at the univariate normality. normality_diagnostics$univariateNormality ## Test Variable Statistic p value Normality ## 1 Anderson-Darling hex2 35.3622 &lt;0.001 NO ## 2 Anderson-Darling hex26 44.5690 &lt;0.001 NO ## 3 Anderson-Darling hex50 52.3565 &lt;0.001 NO ## 4 Anderson-Darling hex74 46.7257 &lt;0.001 NO ## 5 Anderson-Darling hex8 56.3651 &lt;0.001 NO ## 6 Anderson-Darling hex32 57.8443 &lt;0.001 NO ## 7 Anderson-Darling hex56 52.0495 &lt;0.001 NO ## 8 Anderson-Darling hex80 57.8224 &lt;0.001 NO ## 9 Anderson-Darling hex14 43.9103 &lt;0.001 NO ## 10 Anderson-Darling hex38 61.1790 &lt;0.001 NO ## 11 Anderson-Darling hex62 59.4221 &lt;0.001 NO ## 12 Anderson-Darling hex86 35.1592 &lt;0.001 NO ## 13 Anderson-Darling hex20 40.9404 &lt;0.001 NO ## 14 Anderson-Darling hex44 51.4561 &lt;0.001 NO ## 15 Anderson-Darling hex68 47.7856 &lt;0.001 NO ## 16 Anderson-Darling hex92 40.6415 &lt;0.001 NO Unsurprisingly, not normal. But remember that all these tests are sensitive. Let´s check the multivariate normality as well normality_diagnostics$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 2022.73637299326 8.68036552148957e-104 NO ## 2 Mardia Kurtosis 25.8973135283211 0 NO ## 3 MVN &lt;NA&gt; &lt;NA&gt; NO Right, does not look super good. But we should not rely too much on these tests. What we should do however, is use robust measures. ggplot(c_efa_data)+ geom_density(aes(hex2))+ geom_density(aes(hex26))+ geom_density(aes(hex50))+ geom_density(aes(hex74))+ geom_density(aes(hex8))+ geom_density(aes(hex32))+ geom_density(aes(hex56))+ geom_density(aes(hex80))+ geom_density(aes(hex14))+ geom_density(aes(hex38))+ geom_density(aes(hex62))+ geom_density(aes(hex86))+ geom_density(aes(hex20))+ geom_density(aes(hex44))+ geom_density(aes(hex68))+ geom_density(aes(hex92))+ theme_bw() #Doesnt look that bad to me For comfort, let´s use the clean data identified by the multivariate test. I don´t like doing this, but we can use the raw data for reference later. clean_efa_data &lt;- normality_diagnostics$newData Let´s use the clean data and get into the EFA. These are the packages that we will need. library(GPArotation) library(corpcor) library(paran) library(EFAtools) First thing to do is to check if our sample is adequate. We use the EFAtools package functions bartlett and KMO for that. We should also have a look at the correlation matrix. #This is our correlation matrix efa_cormat &lt;- cor(clean_efa_data) efa_cormat ## hex2 hex26 hex50 hex74 hex8 hex32 hex56 hex80 ## hex2 1.00000000 0.2871955 0.4531649 0.28415481 0.1470856 0.2912171 0.13801972 0.16787332 ## hex26 0.28719547 1.0000000 0.2872702 0.43157386 0.1664188 0.2486826 0.26028062 0.20725469 ## hex50 0.45316487 0.2872702 1.0000000 0.42370697 0.1017198 0.1655037 0.18446994 0.16585962 ## hex74 0.28415481 0.4315739 0.4237070 1.00000000 0.1962804 0.3091320 0.42641427 0.32747466 ## hex8 0.14708563 0.1664188 0.1017198 0.19628036 1.0000000 0.4278839 0.31818441 0.29322674 ## hex32 0.29121707 0.2486826 0.1655037 0.30913198 0.4278839 1.0000000 0.49059451 0.28520909 ## hex56 0.13801972 0.2602806 0.1844699 0.42641427 0.3181844 0.4905945 1.00000000 0.31097363 ## hex80 0.16787332 0.2072547 0.1658596 0.32747466 0.2932267 0.2852091 0.31097363 1.00000000 ## hex14 0.17395376 0.1734283 0.0837333 0.06477086 0.1622340 0.1969429 0.04024899 0.10252886 ## hex38 0.15160392 0.1432670 0.1555252 0.23248669 0.1359317 0.2570612 0.23892328 0.18628779 ## hex62 0.25889834 0.2107949 0.2030223 0.24033722 0.2348717 0.3391787 0.27122452 0.24648584 ## hex86 0.23399211 0.2779688 0.1679841 0.22859601 0.3567389 0.3204844 0.21048840 0.29282324 ## hex20 0.12743711 0.1945245 0.1615198 0.21468002 0.1315954 0.1715654 0.17577214 0.19895397 ## hex44 0.04661578 0.2162990 0.2146281 0.38723733 0.1149172 0.1592388 0.32448200 0.27515917 ## hex68 0.12328960 0.1832443 0.1449304 0.14923597 0.1211598 0.1802041 0.18176380 0.07287841 ## hex92 0.14063931 0.3193040 0.2334803 0.34281796 0.1268410 0.1701101 0.24885037 0.25170416 ## hex14 hex38 hex62 hex86 hex20 hex44 hex68 ## hex2 0.17395376 0.15160392 0.2588983 0.23399211 0.1274371 0.04661578 0.12328960 ## hex26 0.17342827 0.14326701 0.2107949 0.27796879 0.1945245 0.21629902 0.18324430 ## hex50 0.08373330 0.15552518 0.2030223 0.16798413 0.1615198 0.21462814 0.14493043 ## hex74 0.06477086 0.23248669 0.2403372 0.22859601 0.2146800 0.38723733 0.14923597 ## hex8 0.16223404 0.13593166 0.2348717 0.35673895 0.1315954 0.11491720 0.12115982 ## hex32 0.19694294 0.25706118 0.3391787 0.32048440 0.1715654 0.15923877 0.18020409 ## hex56 0.04024899 0.23892328 0.2712245 0.21048840 0.1757721 0.32448200 0.18176380 ## hex80 0.10252886 0.18628779 0.2464858 0.29282324 0.1989540 0.27515917 0.07287841 ## hex14 1.00000000 0.17644992 0.3725353 0.23634346 0.1993020 0.03981975 0.15628762 ## hex38 0.17644992 1.00000000 0.3972801 0.32174800 0.2459375 0.26711513 0.08387355 ## hex62 0.37253530 0.39728014 1.0000000 0.34899974 0.2077077 0.25708080 0.23778354 ## hex86 0.23634346 0.32174800 0.3489997 1.00000000 0.2026275 0.17831975 0.08758804 ## hex20 0.19930196 0.24593747 0.2077077 0.20262749 1.0000000 0.36908913 0.27023288 ## hex44 0.03981975 0.26711513 0.2570808 0.17831975 0.3690891 1.00000000 0.20568709 ## hex68 0.15628762 0.08387355 0.2377835 0.08758804 0.2702329 0.20568709 1.00000000 ## hex92 0.08303258 0.22223690 0.1647727 0.18348147 0.3569038 0.28036236 0.18646604 ## hex92 ## hex2 0.14063931 ## hex26 0.31930404 ## hex50 0.23348034 ## hex74 0.34281796 ## hex8 0.12684100 ## hex32 0.17011011 ## hex56 0.24885037 ## hex80 0.25170416 ## hex14 0.08303258 ## hex38 0.22223690 ## hex62 0.16477272 ## hex86 0.18348147 ## hex20 0.35690376 ## hex44 0.28036236 ## hex68 0.18646604 ## hex92 1.00000000 Most correlations are very small, we should not expect too much from this. Let´s check the KMO and bartlett´s test to see where we are at. BARTLETT(clean_efa_data) ## i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data. ## ## v The Bartlett&#39;s test of sphericity was significant at an alpha level of .05. ## These data are probably suitable for factor analysis. ## ## &lt;U+0001D712&gt;²(120) = 3039.2, p &lt; .001 KMO(clean_efa_data) ## i &#39;x&#39; was not a correlation matrix. Correlations are found from entered raw data. ## ## -- Kaiser-Meyer-Olkin criterion (KMO) ---------------------------------------------------- ## ## v The overall KMO value for your data is meritorious. ## These data are probably suitable for factor analysis. ## ## Overall: 0.84 ## ## For each variable: ## hex2 hex26 hex50 hex74 hex8 hex32 hex56 hex80 hex14 hex38 hex62 hex86 hex20 hex44 hex68 ## 0.775 0.875 0.793 0.857 0.837 0.836 0.839 0.908 0.766 0.851 0.845 0.871 0.817 0.826 0.833 ## hex92 ## 0.874 Nice, we should be good to go with our factor extraction, but first lets also look at the determinant. det(efa_cormat) ## [1] 0.02751169 Good, it´s not negative and its quite big. Certainly bigger than the cut-off at &lt;.00001. We are good to go, let´s check the scree plot to see where the eigen values drop off and then perform a parallel analysis to get an estimate of how many factors we should extract. SCREE(efa_cormat) ## ## Eigenvalues were found using PCA, SMC, and EFA. Most of the variance is captured by extracting one factor it seems. Let´s performe the parallel analysis to confirm this. paran(efa_cormat, iterations = 5000, centile = 0, quietly = FALSE, status = TRUE, all = TRUE, cfa = TRUE, graph = TRUE, color = TRUE, col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty = c(1, 2, 3), lwd = 1, legend = TRUE, file = &quot;&quot;, width = 640, height = 640, grdevice = &quot;png&quot;, seed = 0) ## ## Using eigendecomposition of correlation matrix. ## Computing: 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## ## ## Results of Horn&#39;s Parallel Analysis for factor retention ## 5000 iterations, using the mean estimate ## ## -------------------------------------------------- ## Factor Adjusted Unadjusted Estimated ## Eigenvalue Eigenvalue Bias ## -------------------------------------------------- ## 1 -0.838338 2.315898 3.154237 ## 2 -0.564988 1.945278 2.510266 ## 3 -0.387499 1.661586 2.049086 ## 4 -0.925372 0.746462 1.671835 ## 5 -0.815656 0.529957 1.345614 ## 6 -0.806786 0.259593 1.066380 ## 7 -0.696998 0.122074 0.819073 ## 8 -0.634500 -0.02910 0.605393 ## 9 -0.504794 -0.08692 0.417865 ## 10 -0.434328 -0.17524 0.259079 ## 11 -0.358293 -0.22913 0.129157 ## 12 -0.274465 -0.24473 0.029731 ## 13 -0.238661 -0.27265 -0.03399 ## 14 -0.241428 -0.30715 -0.06572 ## 15 -0.289076 -0.38367 -0.09460 ## 16 -0.619855 -0.84320 -0.22334 ## -------------------------------------------------- ## ## Adjusted eigenvalues &gt; 0 indicate dimensions to retain. ## (0 factors retained) Looks like it is as we feared. We don´t really have a factor structure to latch on to. But lets conduct our EFA and see what the pattern matrices look like. We start by fitting our efa model. Theoretically, we should extract 4 factors, so lets do that and call the model efa.4. efa.4&lt;- EFA( x = efa_cormat #the correlation matrix , N = 839 #number of obs , n_factors = 4 #number of factors , method = &#39;ML&#39; #method of estimation , rotation = &#39;oblimin&#39;)#method of rotation Before looking at the model output, lets check the communalities of our items resulting from this model. That is, how much variance is accounted for in each item from the 4 factor model fitted. #There are different measures of communalities, the efa function takes 2, one called h2_init and one called h2. h2 is the final communality estimate from the unrotated solution and h2_init is the initial communality estimate from a PAF. I will use h2, since I did not use PAF, but it might be useful to look into how different measures of communalities will affect the variance estimates of your model. efa.4$h2 ## hex2 hex26 hex50 hex74 hex8 hex32 hex56 hex80 hex14 ## 0.5083995 0.2907840 0.4810619 0.5527056 0.3215491 0.5681460 0.5331742 0.2442253 0.3065944 ## hex38 hex62 hex86 hex20 hex44 hex68 hex92 ## 0.2671640 0.4369054 0.3128426 0.3220605 0.4526378 0.1262208 0.2833638 In general we can account for a reasonable amount of variance in our items, with the exception of hex68. Let´s keep this in mind for when we look at the main model output. To get a look at this output simply call the fitted model like so: efa.4 ## ## EFA performed with type = &#39;EFAtools&#39;, method = &#39;ML&#39;, and rotation = &#39;oblimin&#39;. ## ## -- Rotated Loadings ---------------------------------------------------------------------- ## ## F1 F2 F3 F4 ## hex2 .041 -.206 .698 .172 ## hex26 .122 .198 .358 -.008 ## hex50 -.091 .100 .693 -.055 ## hex74 .237 .344 .410 -.251 ## hex8 .560 -.076 -.030 .120 ## hex32 .739 -.125 .068 .092 ## hex56 .669 .189 -.010 -.218 ## hex80 .322 .218 .060 .006 ## hex14 .009 .043 .052 .526 ## hex38 .132 .300 -.000 .272 ## hex62 .194 .191 .073 .453 ## hex86 .272 .091 .091 .317 ## hex20 -.062 .530 -.004 .195 ## hex44 .049 .672 -.032 -.074 ## hex68 .042 .249 .051 .141 ## hex92 .024 .451 .146 -.027 ## ## -- Factor Intercorrelations -------------------------------------------------------------- ## ## F1 F2 F3 F4 ## F1 1.000 -0.436 0.403 0.298 ## F2 -0.436 1.000 -0.386 -0.181 ## F3 0.403 -0.386 1.000 0.215 ## F4 0.298 -0.181 0.215 1.000 ## ## -- Variances Accounted for --------------------------------------------------------------- ## ## F1 F2 F3 F4 ## SS loadings 3.822 0.803 0.735 0.648 ## Prop Tot Var 0.239 0.050 0.046 0.040 ## Cum Prop Tot Var 0.239 0.289 0.335 0.375 ## Prop Comm Var 0.636 0.134 0.122 0.108 ## Cum Prop Comm Var 0.636 0.770 0.892 1.000 ## ## -- Model Fit ----------------------------------------------------------------------------- ## ## &lt;U+0001D712&gt;²(62) = 243.58, p &lt; .001 ## CFI = .97 ## RMSEA [90% CI] = .06 [.05; .07] ## AIC = 119.58 ## BIC = -173.81 ## CAF = .48 This looks pretty good in my opinion. We have a factor structure that resembles what could be theoretically expected as well as what the unadjusted eigen values of the parallel analysis suggested IF we use kaiser extraction, that is, extracting factors that have an eigen value of 1 or more(note, factor 4 does not have an eigen value of 1 but slightly lower though the point of inflection seem to be at 4 factors - see the graphs above). The factor correlation table indicates the correlation between the factors, some are quite correlated such as factor 2 and 1 with r = -.44. Looking at the explained variance table we can see that we explain roughly 37% of the variation of out items using this model. And the model fit measures are very nice with a low RMSEA and a high CFI. Note that the fit measures are only available when using maximum likelihood estimation. If you are using a non ML estimation another way of assessing the fit or if we extracted the right number of factors is to look at the difference between the model correlations (reproduced correlations) and the correlations in the raw data. That is, check the sum of the residuals. Residuals can be extracted using the factor.residuals function from the psych package. You just enter the original correlation matrix and the rotated loadings from your model (I believe) resid &lt;- psych::factor.residuals(r = efa.4$orig_R, #our original R matrix f = efa.4$rot_loadings) #our rotated loadings Now that we have our residuals, we can do more or less whatever we wish to do with them. Note that the object resid is a 16x16 matrix. They can be tricky to work with, so we can instead put the upper triangle of the matrix in a column - essentially a list of the residuals. I will only check the distribution of them with a simple histogram, but residuals are good to have for many things. We can do this like so: resid &lt;- as.matrix(resid[upper.tri(resid)]) hist(resid) #Looks very nice if you ask me, some residuals are quite big though considering we are working with correlatoins. The issues with this model are the crossloadings and the small loading on hex68 - which we expected due to the low communality. So where do we go from here? In a full analysis we should try out different models and compare them to each other. And then follow upp these models by testing their reliability. Reliability can be measured in many ways. I prefer using a split sample to see if the results are replicated. This should be done for all models of interest in order to examine whether or not they are reliable. For an introduction to this I recommend reading Osborne, J. W., &amp; Fitzpatrick, D. C. (2012). Replication analysis in exploratory factor analysis: What it is and: Why it makes your analysis better. Practical Assessment, Research and Evaluation, 17(15), 18. The most barebones way of examining this is with some sort of intercorrelation measure such as cronbachs alpha. I will show a way of doing this eventhough I personally do not think these measures are very convincing in terms of reliabuility. We start by creating dataframes with the items corresponding to each factor. factor1 &lt;- data.frame(clean_efa_data$hex8, clean_efa_data$hex32, clean_efa_data$hex56, clean_efa_data$hex80) factor2 &lt;- data.frame(clean_efa_data$hex20, clean_efa_data$hex44, clean_efa_data$hex92) factor3 &lt;- data.frame(clean_efa_data$hex2, clean_efa_data$hex26, clean_efa_data$hex50, clean_efa_data$hex74) factor4 &lt;- data.frame(clean_efa_data$hex14, clean_efa_data$hex38, clean_efa_data$hex62, clean_efa_data$hex86) These dataframes can be usefull for many things, but lets check the cronbachs alpha of the factors. psych::alpha(factor1) ## ## Reliability analysis ## Call: psych::alpha(x = factor1) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.68 0.69 0.63 0.35 2.2 0.018 3.7 0.71 0.31 ## ## lower alpha upper 95% confidence boundaries ## 0.65 0.68 0.72 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## clean_efa_data.hex8 0.63 0.63 0.55 0.36 1.7 0.022 0.01252 0.31 ## clean_efa_data.hex32 0.57 0.57 0.47 0.31 1.3 0.025 0.00016 0.31 ## clean_efa_data.hex56 0.60 0.60 0.51 0.34 1.5 0.024 0.00643 0.29 ## clean_efa_data.hex80 0.68 0.68 0.59 0.41 2.1 0.019 0.00762 0.43 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## clean_efa_data.hex8 853 0.69 0.71 0.55 0.45 3.8 0.93 ## clean_efa_data.hex32 853 0.76 0.77 0.67 0.55 3.8 0.94 ## clean_efa_data.hex56 853 0.75 0.74 0.61 0.50 3.7 1.07 ## clean_efa_data.hex80 853 0.67 0.66 0.45 0.38 3.7 1.04 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## clean_efa_data.hex8 0.01 0.09 0.20 0.46 0.23 0 ## clean_efa_data.hex32 0.02 0.09 0.21 0.47 0.21 0 ## clean_efa_data.hex56 0.04 0.13 0.18 0.43 0.22 0 ## clean_efa_data.hex80 0.03 0.12 0.17 0.46 0.22 0 psych::alpha(factor2) ## ## Reliability analysis ## Call: psych::alpha(x = factor2) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.6 0.6 0.51 0.34 1.5 0.024 3.3 0.77 0.36 ## ## lower alpha upper 95% confidence boundaries ## 0.56 0.6 0.65 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## clean_efa_data.hex20 0.44 0.44 0.28 0.28 0.78 0.038 NA 0.28 ## clean_efa_data.hex44 0.53 0.53 0.36 0.36 1.11 0.032 NA 0.36 ## clean_efa_data.hex92 0.54 0.54 0.37 0.37 1.17 0.032 NA 0.37 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## clean_efa_data.hex20 853 0.77 0.77 0.59 0.45 3.3 1 ## clean_efa_data.hex44 853 0.74 0.74 0.51 0.39 3.6 1 ## clean_efa_data.hex92 853 0.73 0.73 0.50 0.39 3.0 1 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## clean_efa_data.hex20 0.04 0.22 0.30 0.33 0.11 0 ## clean_efa_data.hex44 0.03 0.15 0.21 0.43 0.17 0 ## clean_efa_data.hex92 0.07 0.25 0.31 0.31 0.06 0 psych::alpha(factor3) ## ## Reliability analysis ## Call: psych::alpha(x = factor3) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.69 0.69 0.65 0.36 2.3 0.017 3.4 0.86 0.36 ## ## lower alpha upper 95% confidence boundaries ## 0.66 0.69 0.73 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## clean_efa_data.hex2 0.65 0.65 0.56 0.38 1.8 0.021 0.0066 0.42 ## clean_efa_data.hex26 0.66 0.65 0.57 0.39 1.9 0.020 0.0082 0.42 ## clean_efa_data.hex50 0.60 0.60 0.51 0.33 1.5 0.024 0.0071 0.29 ## clean_efa_data.hex74 0.61 0.61 0.52 0.34 1.6 0.023 0.0092 0.29 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## clean_efa_data.hex2 853 0.69 0.70 0.54 0.45 3.0 1.1 ## clean_efa_data.hex26 853 0.69 0.69 0.53 0.43 3.5 1.2 ## clean_efa_data.hex50 853 0.77 0.75 0.63 0.52 3.6 1.3 ## clean_efa_data.hex74 853 0.74 0.74 0.61 0.51 3.4 1.2 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## clean_efa_data.hex2 0.08 0.26 0.30 0.26 0.10 0 ## clean_efa_data.hex26 0.05 0.19 0.18 0.37 0.21 0 ## clean_efa_data.hex50 0.07 0.18 0.13 0.28 0.33 0 ## clean_efa_data.hex74 0.05 0.22 0.16 0.36 0.20 0 psych::alpha(factor4) ## ## Reliability analysis ## Call: psych::alpha(x = factor4) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.63 0.64 0.59 0.31 1.8 0.021 3.5 0.73 0.34 ## ## lower alpha upper 95% confidence boundaries ## 0.59 0.63 0.67 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## clean_efa_data.hex14 0.62 0.62 0.53 0.36 1.66 0.023 0.0015 0.35 ## clean_efa_data.hex38 0.57 0.58 0.49 0.32 1.41 0.025 0.0053 0.35 ## clean_efa_data.hex62 0.49 0.49 0.40 0.24 0.97 0.030 0.0053 0.24 ## clean_efa_data.hex86 0.57 0.58 0.50 0.32 1.38 0.026 0.0146 0.37 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## clean_efa_data.hex14 853 0.64 0.64 0.44 0.34 3.4 1.06 ## clean_efa_data.hex38 853 0.68 0.68 0.51 0.40 3.7 1.03 ## clean_efa_data.hex62 853 0.74 0.76 0.66 0.53 3.8 0.93 ## clean_efa_data.hex86 853 0.72 0.69 0.51 0.41 3.1 1.17 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## clean_efa_data.hex14 0.04 0.18 0.25 0.39 0.15 0 ## clean_efa_data.hex38 0.03 0.14 0.17 0.48 0.19 0 ## clean_efa_data.hex62 0.01 0.11 0.20 0.48 0.20 0 ## clean_efa_data.hex86 0.08 0.27 0.24 0.28 0.13 0 Nice, we have our alpha measures. They are a bit low, but hey, that´s the name of the facet game. This is basically all there is to the EFA in terms of procedure. The point is to reduce the items to latent variables/factors - which is something that we have done. What to do next depends on your research question - do you want to use participants scores to predict something? maybe you just want to examine the reliability. An overly long look into these techniques are superfluous at this point since we will move on the CFA and SEM later on, which are better suited for those types of questions anyways. But note that you could and should play around with the data here and fit different model solutions and compare them. One reason I like this dataset is because of how massive it is, try to remove some items and see what happens :) EFA is fun in the sense that it is very open, you don´t have any clear answers to your questions and you are just asking questions from your data. I think it´s a good approach to pilot studies or studies that are not heavy on hypothesis testing. #Confirmatory factor analysis "],["multi-level-modelling.html", "12 Multi-level modelling", " 12 Multi-level modelling "],["structural-equation-modelling.html", "13 Structural equation modelling", " 13 Structural equation modelling "],["meta-analysis.html", "14 Meta-analysis", " 14 Meta-analysis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
